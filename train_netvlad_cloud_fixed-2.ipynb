{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetVLAD Visual Localization - Cloud Training\n",
    "\n",
    "This notebook trains a NetVLAD model for visual localization in cloud environments (Google Colab, Kaggle, etc.).\n",
    "\n",
    "**Dataset**: 100 street-level images from Vilnius (61 train, 39 test)\n",
    "\n",
    "**What this does**:\n",
    "1. Installs dependencies\n",
    "2. Uploads dataset (images + CSV) OR uses existing files\n",
    "3. Trains NetVLAD model (MobileNetV3 or ResNet50)\n",
    "4. Builds descriptor database\n",
    "5. Evaluates on test set\n",
    "6. Downloads trained model and results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 11:26:05.749992: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-01-02 11:26:05.762907: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2026-01-02 11:26:05.778604: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2026-01-02 11:26:05.783202: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2026-01-02 11:26:05.795236: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-01-02 11:26:06.678756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.17.1\n",
      "GPU available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'), PhysicalDevice(name='/physical_device:GPU:1', device_type='GPU')]\n",
      "Num GPUs: 2\n"
     ]
    }
   ],
   "source": [
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {tf.config.list_physical_devices('GPU')}\")\n",
    "print(f\"Num GPUs: {len(tf.config.list_physical_devices('GPU'))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (if needed)\n",
    "!pip install -q numpy pandas pillow matplotlib scikit-learn opencv-python tqdm tf2onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Upload Dataset\n",
    "\n",
    "**Option A**: Upload from local machine (for Colab)  \n",
    "**Option B**: Upload from Google Drive  \n",
    "**Option C**: Already present in Kaggle environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local Jupyter or Kaggle\n",
      "Working directory: /home/simsku2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Check if running on Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "    print(\"Running on Google Colab\")\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Running on local Jupyter or Kaggle\")\n",
    "\n",
    "# Set working directory\n",
    "if IN_COLAB:\n",
    "    WORK_DIR = '/content/netvlad_project'\n",
    "else:\n",
    "    WORK_DIR = os.getcwd()\n",
    "\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "print(f\"Working directory: {WORK_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make sure dataset.csv and images/ folder are in the current directory\n",
      "Current directory: /home/simsku2\n"
     ]
    }
   ],
   "source": [
    "# Option A: Upload from local machine (Colab)\n",
    "if IN_COLAB:\n",
    "    print(\"Upload your dataset.csv and images folder (as ZIP)\")\n",
    "    print(\"You'll need:\")\n",
    "    print(\"  1. dataset.csv\")\n",
    "    print(\"  2. images.zip (containing all 100 JPEG files)\")\n",
    "    \n",
    "    from google.colab import files\n",
    "    \n",
    "    # Upload CSV\n",
    "    print(\"\\nUpload dataset.csv:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Upload images ZIP\n",
    "    print(\"\\nUpload images.zip:\")\n",
    "    uploaded = files.upload()\n",
    "    \n",
    "    # Extract images\n",
    "    !unzip -q images.zip\n",
    "    print(\"✓ Dataset uploaded and extracted\")\n",
    "else:\n",
    "    print(\"Make sure dataset.csv and images/ folder are in the current directory\")\n",
    "    print(f\"Current directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Dataset loaded: 1838 images\n",
      "  Train: 1757\n",
      "  Test: 81\n",
      "✓ Images found: 1850\n"
     ]
    }
   ],
   "source": [
    "# Verify dataset\n",
    "import pandas as pd\n",
    "\n",
    "assert os.path.exists('dataset.csv'), \"dataset.csv not found!\"\n",
    "assert os.path.exists('images'), \"images/ folder not found!\"\n",
    "\n",
    "df = pd.read_csv('dataset.csv')\n",
    "print(f\"✓ Dataset loaded: {len(df)} images\")\n",
    "print(f\"  Train: {len(df[df['split'] == 'train'])}\")\n",
    "print(f\"  Test: {len(df[df['split'] == 'test'])}\")\n",
    "\n",
    "num_images = len(os.listdir('images'))\n",
    "print(f\"✓ Images found: {num_images}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  Backbone: mobilenetv3small\n",
      "  Epochs: 20\n",
      "  Batch size: 4\n",
      "  NetVLAD clusters: 64\n",
      "  Binary hashing: 512 bits (mobile optimized)\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "BACKBONE = 'mobilenetv3small'  # Options: 'mobilenetv3', 'mobilenetv3small', 'resnet50'\n",
    "EPOCHS = 20  # Slightly longer for binary hashing convergence\n",
    "BATCH_SIZE = 4 # Reduce to 4 if you get OOM errors\n",
    "NUM_CLUSTERS = 64\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "# Mobile optimization settings\n",
    "USE_BINARY_HASHING = True  # Enable binary hashing for mobile deployment\n",
    "HASH_BITS = 512  # Binary hash code size (128, 256, or 512)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Backbone: {BACKBONE}\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  NetVLAD clusters: {NUM_CLUSTERS}\")\n",
    "if USE_BINARY_HASHING:\n",
    "    print(f\"  Binary hashing: {HASH_BITS} bits (mobile optimized)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Implementation\n",
    "\n",
    "All model code is embedded here for cloud deployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ NetVLAD layer defined\n"
     ]
    }
   ],
   "source": [
    "# NetVLAD Layer\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "class NetVLAD(keras.layers.Layer):\n",
    "    \"\"\"NetVLAD layer for aggregating local features.\"\"\"\n",
    "    \n",
    "    def __init__(self, num_clusters=64, **kwargs):\n",
    "        super(NetVLAD, self).__init__(**kwargs)\n",
    "        self.num_clusters = num_clusters\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.D = input_shape[-1]\n",
    "        \n",
    "        self.cluster_centers = self.add_weight(\n",
    "            name='cluster_centers',\n",
    "            shape=(self.D, self.num_clusters),\n",
    "            initializer=keras.initializers.GlorotUniform(),\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.conv_soft_assign = keras.layers.Conv2D(\n",
    "            filters=self.num_clusters,\n",
    "            kernel_size=(1, 1),\n",
    "            padding='same',\n",
    "            use_bias=True\n",
    "        )\n",
    "        \n",
    "        super(NetVLAD, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x):\n",
    "        batch_size = tf.shape(x)[0]\n",
    "        H, W = tf.shape(x)[1], tf.shape(x)[2]\n",
    "        N = H * W\n",
    "        \n",
    "        # Soft assignment\n",
    "        soft_assign = self.conv_soft_assign(x)\n",
    "        soft_assign = tf.nn.softmax(soft_assign, axis=-1)\n",
    "        \n",
    "        # Reshape\n",
    "        x_flat = tf.reshape(x, [batch_size, N, self.D])\n",
    "        soft_assign_flat = tf.reshape(soft_assign, [batch_size, N, self.num_clusters])\n",
    "        \n",
    "        # VLAD computation\n",
    "        vlad = []\n",
    "        for k in range(self.num_clusters):\n",
    "            cluster_center = self.cluster_centers[:, k]\n",
    "            residual = x_flat - cluster_center\n",
    "            weighted_residual = tf.expand_dims(soft_assign_flat[:, :, k], -1) * residual\n",
    "            cluster_vlad = tf.reduce_sum(weighted_residual, axis=1)\n",
    "            vlad.append(cluster_vlad)\n",
    "        \n",
    "        vlad = tf.stack(vlad, axis=1)\n",
    "        vlad = tf.reshape(vlad, [batch_size, self.num_clusters * self.D])\n",
    "        \n",
    "        # L2 normalization\n",
    "        vlad = tf.nn.l2_normalize(vlad, axis=1)\n",
    "        \n",
    "        return vlad\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(NetVLAD, self).get_config()\n",
    "        config.update({'num_clusters': self.num_clusters})\n",
    "        return config\n",
    "\n",
    "print(\"✓ NetVLAD layer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Binary hashing layer defined\n"
     ]
    }
   ],
   "source": [
    "# Binary Hashing Layer for Mobile Deployment\n",
    "class BinaryHashingLayer(keras.layers.Layer):\n",
    "    \"\"\"Learnable binary hashing for compact descriptor representation.\"\"\"\n",
    "    \n",
    "    def __init__(self, hash_bits=256, **kwargs):\n",
    "        super(BinaryHashingLayer, self).__init__(**kwargs)\n",
    "        self.hash_bits = hash_bits\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.input_dim = input_shape[-1]\n",
    "        \n",
    "        # Learned projection matrix\n",
    "        self.projection = self.add_weight(\n",
    "            name='projection',\n",
    "            shape=(self.input_dim, self.hash_bits),\n",
    "            initializer=keras.initializers.GlorotUniform(),\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        self.bias = self.add_weight(\n",
    "            name='bias',\n",
    "            shape=(self.hash_bits,),\n",
    "            initializer=keras.initializers.Zeros(),\n",
    "            trainable=True\n",
    "        )\n",
    "        \n",
    "        super(BinaryHashingLayer, self).build(input_shape)\n",
    "    \n",
    "    def call(self, x, training=None):\n",
    "        # Linear projection\n",
    "        projected = tf.matmul(x, self.projection) + self.bias\n",
    "        \n",
    "        if training:\n",
    "            # During training: tanh for smooth gradients\n",
    "            hash_codes = tf.tanh(projected)\n",
    "        else:\n",
    "            # During inference: hard binarization\n",
    "            hash_codes = tf.sign(projected)\n",
    "            hash_codes = tf.where(tf.equal(hash_codes, 0),\n",
    "                                  tf.ones_like(hash_codes),\n",
    "                                  hash_codes)\n",
    "        \n",
    "        return hash_codes\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(BinaryHashingLayer, self).get_config()\n",
    "        config.update({'hash_bits': self.hash_bits})\n",
    "        return config\n",
    "\n",
    "print(\"✓ Binary hashing layer defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model builder defined (with binary hashing support)\n"
     ]
    }
   ],
   "source": [
    "# Build NetVLAD Model with Optional Binary Hashing\n",
    "def build_netvlad_model_with_hashing(backbone_name='mobilenetv3', \n",
    "                                      num_clusters=64, \n",
    "                                      use_hashing=False, \n",
    "                                      hash_bits=256):\n",
    "    # Load backbone\n",
    "    if backbone_name == 'mobilenetv3':\n",
    "        base_model = keras.applications.MobileNetV3Large(\n",
    "            input_shape=(224, 224, 3),\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            pooling=None\n",
    "        )\n",
    "    elif backbone_name == 'mobilenetv3small':\n",
    "        base_model = keras.applications.MobileNetV3Small(\n",
    "            input_shape=(224, 224, 3),\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            pooling=None\n",
    "        )\n",
    "    elif backbone_name == 'resnet50':\n",
    "        base_model = keras.applications.ResNet50(\n",
    "            input_shape=(224, 224, 3),\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            pooling=None\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backbone: {backbone_name}\")\n",
    "    \n",
    "    # Freeze backbone\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Build model architecture\n",
    "    inputs = keras.Input(shape=(224, 224, 3))\n",
    "    features = base_model(inputs)\n",
    "    descriptor = NetVLAD(num_clusters=num_clusters, name='netvlad')(features)\n",
    "    \n",
    "    # Optionally add binary hashing layer\n",
    "    if use_hashing:\n",
    "        output = BinaryHashingLayer(hash_bits=hash_bits, name='binary_hashing')(descriptor)\n",
    "        model = keras.Model(inputs=inputs, outputs=output, name='NetVLAD_Binary')\n",
    "    else:\n",
    "        model = keras.Model(inputs=inputs, outputs=descriptor, name='NetVLAD_Float')\n",
    "    \n",
    "    # Print model info\n",
    "    print(f\"\\nModel: {backbone_name} + NetVLAD({num_clusters})\")\n",
    "    if use_hashing:\n",
    "        print(f\"  Binary hashing: {hash_bits} bits\")\n",
    "        print(f\"  Output shape: (batch, {hash_bits})\")\n",
    "    else:\n",
    "        print(f\"  Output dimension: {model.output_shape[1]}\")\n",
    "    print(f\"Trainable params: {sum([tf.size(w).numpy() for w in model.trainable_weights]):,}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Keep backward compatibility\n",
    "def build_netvlad_model(backbone_name='mobilenetv3', num_clusters=64):\n",
    "    \"\"\"Build NetVLAD model without hashing (backward compatible).\"\"\"\n",
    "    return build_netvlad_model_with_hashing(\n",
    "        backbone_name=backbone_name,\n",
    "        num_clusters=num_clusters,\n",
    "        use_hashing=False\n",
    "    )\n",
    "\n",
    "print(\"✓ Model builder defined (with binary hashing support)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Contrastive loss defined\n"
     ]
    }
   ],
   "source": [
    "# Contrastive Loss\n",
    "class ContrastiveLoss(keras.losses.Loss):\n",
    "    \"\"\"Contrastive loss for metric learning.\"\"\"\n",
    "    \n",
    "    def __init__(self, margin=0.5, **kwargs):\n",
    "        super(ContrastiveLoss, self).__init__(**kwargs)\n",
    "        self.margin = margin\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        if isinstance(y_pred, (list, tuple)):\n",
    "            descriptor_1, descriptor_2 = y_pred\n",
    "        else:\n",
    "            descriptor_1 = y_pred[:, 0, :]\n",
    "            descriptor_2 = y_pred[:, 1, :]\n",
    "        \n",
    "        distance = tf.sqrt(\n",
    "            tf.reduce_sum(tf.square(descriptor_1 - descriptor_2), axis=1) + 1e-8\n",
    "        )\n",
    "        \n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        \n",
    "        similar_loss = y_true * tf.square(distance)\n",
    "        dissimilar_loss = (1.0 - y_true) * tf.square(\n",
    "            tf.maximum(self.margin - distance, 0.0)\n",
    "        )\n",
    "        \n",
    "        return tf.reduce_mean(similar_loss + dissimilar_loss)\n",
    "\n",
    "print(\"✓ Contrastive loss defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images: 1757\n",
      "Test images: 81\n",
      "✓ Distance matrix computed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "train_df = df[df['split'] == 'train'].reset_index(drop=True)\n",
    "test_df = df[df['split'] == 'test'].reset_index(drop=True)\n",
    "\n",
    "print(f\"Train images: {len(train_df)}\")\n",
    "print(f\"Test images: {len(test_df)}\")\n",
    "\n",
    "# Compute distance matrix\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    return np.sqrt(dlat**2 + dlon**2)\n",
    "\n",
    "n = len(train_df)\n",
    "dist_matrix = np.zeros((n, n))\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        dist = haversine_distance(\n",
    "            train_df.iloc[i]['latitude'], train_df.iloc[i]['longitude'],\n",
    "            train_df.iloc[j]['latitude'], train_df.iloc[j]['longitude']\n",
    "        )\n",
    "        dist_matrix[i, j] = dist\n",
    "        dist_matrix[j, i] = dist\n",
    "\n",
    "print(\"✓ Distance matrix computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Generated 100 sample pairs\n",
      "  Positive: 47\n",
      "  Negative: 53\n"
     ]
    }
   ],
   "source": [
    "# Generate training pairs\n",
    "POSITIVE_THRESHOLD = 0.000225  # ~25m\n",
    "NEGATIVE_THRESHOLD = 0.0009    # ~100m\n",
    "\n",
    "def generate_pairs(num_pairs):\n",
    "    \"\"\"Generate positive/negative pairs.\"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for _ in range(num_pairs):\n",
    "        anchor_idx = np.random.randint(0, n)\n",
    "        distances = dist_matrix[anchor_idx]\n",
    "        \n",
    "        if np.random.random() < 0.5:\n",
    "            # Positive pair\n",
    "            positives = np.where((distances > 0) & (distances < POSITIVE_THRESHOLD))[0]\n",
    "            if len(positives) > 0:\n",
    "                pair_idx = np.random.choice(positives)\n",
    "                label = 1\n",
    "            else:\n",
    "                # Fallback to negative\n",
    "                negatives = np.where(distances > NEGATIVE_THRESHOLD)[0]\n",
    "                if len(negatives) == 0:\n",
    "                    continue\n",
    "                pair_idx = np.random.choice(negatives)\n",
    "                label = 0\n",
    "        else:\n",
    "            # Negative pair\n",
    "            negatives = np.where(distances > NEGATIVE_THRESHOLD)[0]\n",
    "            if len(negatives) > 0:\n",
    "                pair_idx = np.random.choice(negatives)\n",
    "                label = 0\n",
    "            else:\n",
    "                # Fallback to positive\n",
    "                positives = np.where((distances > 0) & (distances < POSITIVE_THRESHOLD))[0]\n",
    "                if len(positives) == 0:\n",
    "                    continue\n",
    "                pair_idx = np.random.choice(positives)\n",
    "                label = 1\n",
    "        \n",
    "        pairs.append((anchor_idx, pair_idx, label))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Test pair generation\n",
    "test_pairs = generate_pairs(100)\n",
    "print(f\"✓ Generated {len(test_pairs)} sample pairs\")\n",
    "print(f\"  Positive: {sum([1 for _, _, l in test_pairs if l == 1])}\")\n",
    "print(f\"  Negative: {sum([1 for _, _, l in test_pairs if l == 0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessing function defined\n"
     ]
    }
   ],
   "source": [
    "# Image preprocessing\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "\n",
    "def preprocess_image(img_path):\n",
    "    \"\"\"Load and preprocess image.\"\"\"\n",
    "    img = keras_image.load_img(img_path, target_size=(224, 224))\n",
    "    img = keras_image.img_to_array(img)\n",
    "    \n",
    "    # ImageNet normalization\n",
    "    mean = np.array([0.485, 0.456, 0.406]) * 255.0\n",
    "    std = np.array([0.229, 0.224, 0.225]) * 255.0\n",
    "    img = (img - mean) / std\n",
    "    \n",
    "    return img\n",
    "\n",
    "print(\"✓ Preprocessing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building mobilenetv3small model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-02 11:28:46.426933: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 900 MB memory:  -> device: 0, name: NVIDIA H100 NVL, pci bus id: 0000:ca:00.0, compute capability: 9.0\n",
      "2026-01-02 11:28:46.428738: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 92489 MB memory:  -> device: 1, name: NVIDIA H100 NVL, pci bus id: 0000:e1:00.0, compute capability: 9.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model: mobilenetv3small + NetVLAD(64)\n",
      "  Binary hashing: 512 bits\n",
      "  Output shape: (batch, 512)\n",
      "Trainable params: 18,948,672\n",
      "✓ Siamese model built\n"
     ]
    }
   ],
   "source": [
    "# Build model (with hashing if enabled)\n",
    "print(f\"Building {BACKBONE} model...\")\n",
    "\n",
    "if USE_BINARY_HASHING:\n",
    "    # Build model WITH binary hashing layer\n",
    "    netvlad_model = build_netvlad_model_with_hashing(\n",
    "        BACKBONE, NUM_CLUSTERS, \n",
    "        use_hashing=True, \n",
    "        hash_bits=HASH_BITS\n",
    "    )\n",
    "else:\n",
    "    # Build standard float model\n",
    "    netvlad_model = build_netvlad_model_with_hashing(\n",
    "        BACKBONE, NUM_CLUSTERS, \n",
    "        use_hashing=False\n",
    "    )\n",
    "\n",
    "# Create Siamese model with single output\n",
    "input_1 = keras.Input(shape=(224, 224, 3), name='image_1')\n",
    "input_2 = keras.Input(shape=(224, 224, 3), name='image_2')\n",
    "\n",
    "descriptor_1 = netvlad_model(input_1)\n",
    "descriptor_2 = netvlad_model(input_2)\n",
    "\n",
    "# Stack descriptors using Keras Lambda layer\n",
    "descriptors_stacked = keras.layers.Lambda(\n",
    "    lambda x: tf.stack(x, axis=1)\n",
    ")([descriptor_1, descriptor_2])\n",
    "\n",
    "siamese_model = keras.Model(\n",
    "    inputs=[input_1, input_2],\n",
    "    outputs=descriptors_stacked\n",
    ")\n",
    "\n",
    "print(\"✓ Siamese model built\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model compiled\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "siamese_model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE),\n",
    "    loss=ContrastiveLoss(margin=0.5)\n",
    ")\n",
    "\n",
    "print(\"✓ Model compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training configuration:\n",
      "  Epochs: 20\n",
      "  Steps per epoch: 878\n",
      "  Batch size: 4\n",
      "  Pairs per epoch: 3514\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "PAIRS_PER_EPOCH = len(train_df) * 2  # 2x pairs per epoch\n",
    "STEPS_PER_EPOCH = PAIRS_PER_EPOCH // BATCH_SIZE\n",
    "\n",
    "print(f\"Training configuration:\")\n",
    "print(f\"  Epochs: {EPOCHS}\")\n",
    "print(f\"  Steps per epoch: {STEPS_PER_EPOCH}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Pairs per epoch: {PAIRS_PER_EPOCH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767353348.090224   15451 service.cc:146] XLA service 0x55556784fc70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1767353348.090252   15451 service.cc:154]   StreamExecutor device (0): NVIDIA H100 NVL, Compute Capability 9.0\n",
      "I0000 00:00:1767353348.090254   15451 service.cc:154]   StreamExecutor device (1): NVIDIA H100 NVL, Compute Capability 9.0\n",
      "2026-01-02 11:29:08.550903: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2026-01-02 11:29:09.871147: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2026-01-02 11:31:17.077384: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:393] ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_10', 156 bytes spill stores, 164 bytes spill loads\n",
      "ptxas warning : Registers are spilled to local memory in function 'input_multiply_reduce_fusion_8', 156 bytes spill stores, 164 bytes spill loads\n",
      "\n",
      "I0000 00:00:1767353477.162982   15451 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Step 5/878, Loss: 0.1312\n",
      "  Step 10/878, Loss: 0.1191\n",
      "  Step 15/878, Loss: 0.1104\n",
      "  Step 20/878, Loss: 0.1049\n",
      "  Step 25/878, Loss: 0.1007\n",
      "  Step 30/878, Loss: 0.0972\n",
      "  Step 35/878, Loss: 0.0946\n",
      "  Step 40/878, Loss: 0.0928\n",
      "  Step 45/878, Loss: 0.0914\n",
      "  Step 50/878, Loss: 0.0902\n",
      "  Step 55/878, Loss: 0.0890\n",
      "  Step 60/878, Loss: 0.0877\n",
      "  Step 65/878, Loss: 0.0865\n",
      "  Step 70/878, Loss: 0.0854\n",
      "  Step 75/878, Loss: 0.0844\n",
      "  Step 80/878, Loss: 0.0835\n",
      "  Step 85/878, Loss: 0.0828\n",
      "  Step 90/878, Loss: 0.0821\n",
      "  Step 95/878, Loss: 0.0815\n",
      "  Step 100/878, Loss: 0.0809\n",
      "  Step 105/878, Loss: 0.0803\n",
      "  Step 110/878, Loss: 0.0797\n",
      "  Step 115/878, Loss: 0.0792\n",
      "  Step 120/878, Loss: 0.0787\n",
      "  Step 125/878, Loss: 0.0782\n",
      "  Step 130/878, Loss: 0.0778\n",
      "  Step 135/878, Loss: 0.0774\n",
      "  Step 140/878, Loss: 0.0770\n",
      "  Step 145/878, Loss: 0.0766\n",
      "  Step 150/878, Loss: 0.0762\n",
      "  Step 155/878, Loss: 0.0759\n",
      "  Step 160/878, Loss: 0.0756\n",
      "  Step 165/878, Loss: 0.0752\n",
      "  Step 170/878, Loss: 0.0749\n",
      "  Step 175/878, Loss: 0.0746\n",
      "  Step 180/878, Loss: 0.0743\n",
      "  Step 185/878, Loss: 0.0741\n",
      "  Step 190/878, Loss: 0.0738\n",
      "  Step 195/878, Loss: 0.0735\n",
      "  Step 200/878, Loss: 0.0733\n",
      "  Step 205/878, Loss: 0.0731\n",
      "  Step 210/878, Loss: 0.0728\n",
      "  Step 215/878, Loss: 0.0726\n",
      "  Step 220/878, Loss: 0.0724\n",
      "  Step 225/878, Loss: 0.0722\n",
      "  Step 230/878, Loss: 0.0720\n",
      "  Step 235/878, Loss: 0.0718\n",
      "  Step 240/878, Loss: 0.0717\n",
      "  Step 245/878, Loss: 0.0715\n",
      "  Step 250/878, Loss: 0.0714\n",
      "  Step 255/878, Loss: 0.0712\n",
      "  Step 260/878, Loss: 0.0710\n",
      "  Step 265/878, Loss: 0.0709\n",
      "  Step 270/878, Loss: 0.0707\n",
      "  Step 275/878, Loss: 0.0706\n",
      "  Step 280/878, Loss: 0.0705\n",
      "  Step 285/878, Loss: 0.0703\n",
      "  Step 290/878, Loss: 0.0702\n",
      "  Step 295/878, Loss: 0.0701\n",
      "  Step 300/878, Loss: 0.0699\n",
      "  Step 305/878, Loss: 0.0698\n",
      "  Step 310/878, Loss: 0.0697\n",
      "  Step 315/878, Loss: 0.0696\n",
      "  Step 320/878, Loss: 0.0694\n",
      "  Step 325/878, Loss: 0.0693\n",
      "  Step 330/878, Loss: 0.0692\n",
      "  Step 335/878, Loss: 0.0691\n",
      "  Step 340/878, Loss: 0.0690\n",
      "  Step 345/878, Loss: 0.0689\n",
      "  Step 350/878, Loss: 0.0688\n",
      "  Step 355/878, Loss: 0.0687\n",
      "  Step 360/878, Loss: 0.0686\n",
      "  Step 365/878, Loss: 0.0685\n",
      "  Step 370/878, Loss: 0.0684\n",
      "  Step 375/878, Loss: 0.0683\n",
      "  Step 380/878, Loss: 0.0682\n",
      "  Step 385/878, Loss: 0.0681\n",
      "  Step 390/878, Loss: 0.0680\n",
      "  Step 395/878, Loss: 0.0679\n",
      "  Step 400/878, Loss: 0.0678\n",
      "  Step 405/878, Loss: 0.0677\n",
      "  Step 410/878, Loss: 0.0677\n",
      "  Step 415/878, Loss: 0.0676\n",
      "  Step 420/878, Loss: 0.0675\n",
      "  Step 425/878, Loss: 0.0674\n",
      "  Step 430/878, Loss: 0.0674\n",
      "  Step 435/878, Loss: 0.0673\n",
      "  Step 440/878, Loss: 0.0672\n",
      "  Step 445/878, Loss: 0.0671\n",
      "  Step 450/878, Loss: 0.0671\n",
      "  Step 455/878, Loss: 0.0670\n",
      "  Step 460/878, Loss: 0.0669\n",
      "  Step 465/878, Loss: 0.0669\n",
      "  Step 470/878, Loss: 0.0668\n",
      "  Step 475/878, Loss: 0.0667\n",
      "  Step 480/878, Loss: 0.0667\n",
      "  Step 485/878, Loss: 0.0666\n",
      "  Step 490/878, Loss: 0.0666\n",
      "  Step 495/878, Loss: 0.0665\n",
      "  Step 500/878, Loss: 0.0665\n",
      "  Step 505/878, Loss: 0.0664\n",
      "  Step 510/878, Loss: 0.0664\n",
      "  Step 515/878, Loss: 0.0663\n",
      "  Step 520/878, Loss: 0.0663\n",
      "  Step 525/878, Loss: 0.0662\n",
      "  Step 530/878, Loss: 0.0661\n",
      "  Step 535/878, Loss: 0.0661\n",
      "  Step 540/878, Loss: 0.0660\n",
      "  Step 545/878, Loss: 0.0660\n",
      "  Step 550/878, Loss: 0.0659\n",
      "  Step 555/878, Loss: 0.0659\n",
      "  Step 560/878, Loss: 0.0658\n",
      "  Step 565/878, Loss: 0.0658\n",
      "  Step 570/878, Loss: 0.0657\n",
      "  Step 575/878, Loss: 0.0657\n",
      "  Step 580/878, Loss: 0.0656\n",
      "  Step 585/878, Loss: 0.0656\n",
      "  Step 590/878, Loss: 0.0655\n",
      "  Step 595/878, Loss: 0.0655\n",
      "  Step 600/878, Loss: 0.0654\n",
      "  Step 605/878, Loss: 0.0654\n",
      "  Step 610/878, Loss: 0.0653\n",
      "  Step 615/878, Loss: 0.0653\n",
      "  Step 620/878, Loss: 0.0652\n",
      "  Step 625/878, Loss: 0.0652\n",
      "  Step 630/878, Loss: 0.0651\n",
      "  Step 635/878, Loss: 0.0651\n",
      "  Step 640/878, Loss: 0.0650\n",
      "  Step 645/878, Loss: 0.0650\n",
      "  Step 650/878, Loss: 0.0649\n",
      "  Step 655/878, Loss: 0.0649\n",
      "  Step 660/878, Loss: 0.0648\n",
      "  Step 665/878, Loss: 0.0648\n",
      "  Step 670/878, Loss: 0.0647\n",
      "  Step 675/878, Loss: 0.0647\n",
      "  Step 680/878, Loss: 0.0647\n",
      "  Step 685/878, Loss: 0.0646\n",
      "  Step 690/878, Loss: 0.0646\n",
      "  Step 695/878, Loss: 0.0645\n",
      "  Step 700/878, Loss: 0.0645\n",
      "  Step 705/878, Loss: 0.0644\n",
      "  Step 710/878, Loss: 0.0644\n",
      "  Step 715/878, Loss: 0.0643\n",
      "  Step 720/878, Loss: 0.0643\n",
      "  Step 725/878, Loss: 0.0642\n",
      "  Step 730/878, Loss: 0.0642\n",
      "  Step 735/878, Loss: 0.0642\n",
      "  Step 740/878, Loss: 0.0641\n",
      "  Step 745/878, Loss: 0.0641\n",
      "  Step 750/878, Loss: 0.0640\n",
      "  Step 755/878, Loss: 0.0640\n",
      "  Step 760/878, Loss: 0.0640\n",
      "  Step 765/878, Loss: 0.0639\n",
      "  Step 770/878, Loss: 0.0639\n",
      "  Step 775/878, Loss: 0.0638\n",
      "  Step 780/878, Loss: 0.0638\n",
      "  Step 785/878, Loss: 0.0638\n",
      "  Step 790/878, Loss: 0.0637\n",
      "  Step 795/878, Loss: 0.0637\n",
      "  Step 800/878, Loss: 0.0636\n",
      "  Step 805/878, Loss: 0.0636\n",
      "  Step 810/878, Loss: 0.0636\n",
      "  Step 815/878, Loss: 0.0635\n",
      "  Step 820/878, Loss: 0.0635\n",
      "  Step 825/878, Loss: 0.0634\n",
      "  Step 830/878, Loss: 0.0634\n",
      "  Step 835/878, Loss: 0.0634\n",
      "  Step 840/878, Loss: 0.0633\n",
      "  Step 845/878, Loss: 0.0633\n",
      "  Step 850/878, Loss: 0.0633\n",
      "  Step 855/878, Loss: 0.0632\n",
      "  Step 860/878, Loss: 0.0632\n",
      "  Step 865/878, Loss: 0.0632\n",
      "  Step 870/878, Loss: 0.0631\n",
      "  Step 875/878, Loss: 0.0631\n",
      "  Epoch 1 Loss: 0.0631\n",
      "\n",
      "Epoch 2/20\n",
      "  Step 5/878, Loss: 0.0571\n",
      "  Step 10/878, Loss: 0.0570\n",
      "  Step 15/878, Loss: 0.0570\n",
      "  Step 20/878, Loss: 0.0570\n",
      "  Step 25/878, Loss: 0.0569\n",
      "  Step 30/878, Loss: 0.0569\n",
      "  Step 35/878, Loss: 0.0569\n",
      "  Step 40/878, Loss: 0.0569\n",
      "  Step 45/878, Loss: 0.0569\n",
      "  Step 50/878, Loss: 0.0569\n",
      "  Step 55/878, Loss: 0.0569\n",
      "  Step 60/878, Loss: 0.0569\n",
      "  Step 65/878, Loss: 0.0568\n",
      "  Step 70/878, Loss: 0.0568\n",
      "  Step 75/878, Loss: 0.0568\n",
      "  Step 80/878, Loss: 0.0568\n",
      "  Step 85/878, Loss: 0.0568\n",
      "  Step 90/878, Loss: 0.0568\n",
      "  Step 95/878, Loss: 0.0567\n",
      "  Step 100/878, Loss: 0.0567\n",
      "  Step 105/878, Loss: 0.0567\n",
      "  Step 110/878, Loss: 0.0567\n",
      "  Step 115/878, Loss: 0.0566\n",
      "  Step 120/878, Loss: 0.0566\n",
      "  Step 125/878, Loss: 0.0566\n",
      "  Step 130/878, Loss: 0.0566\n",
      "  Step 135/878, Loss: 0.0566\n",
      "  Step 140/878, Loss: 0.0565\n",
      "  Step 145/878, Loss: 0.0565\n",
      "  Step 150/878, Loss: 0.0565\n",
      "  Step 155/878, Loss: 0.0565\n",
      "  Step 160/878, Loss: 0.0565\n",
      "  Step 165/878, Loss: 0.0564\n",
      "  Step 170/878, Loss: 0.0564\n",
      "  Step 175/878, Loss: 0.0564\n",
      "  Step 180/878, Loss: 0.0564\n",
      "  Step 185/878, Loss: 0.0564\n",
      "  Step 190/878, Loss: 0.0564\n",
      "  Step 195/878, Loss: 0.0563\n",
      "  Step 200/878, Loss: 0.0563\n",
      "  Step 205/878, Loss: 0.0563\n",
      "  Step 210/878, Loss: 0.0563\n",
      "  Step 215/878, Loss: 0.0563\n",
      "  Step 220/878, Loss: 0.0563\n",
      "  Step 225/878, Loss: 0.0562\n",
      "  Step 230/878, Loss: 0.0562\n",
      "  Step 235/878, Loss: 0.0562\n",
      "  Step 240/878, Loss: 0.0562\n",
      "  Step 245/878, Loss: 0.0562\n",
      "  Step 250/878, Loss: 0.0562\n",
      "  Step 255/878, Loss: 0.0561\n",
      "  Step 260/878, Loss: 0.0561\n",
      "  Step 265/878, Loss: 0.0561\n",
      "  Step 270/878, Loss: 0.0561\n",
      "  Step 275/878, Loss: 0.0561\n",
      "  Step 280/878, Loss: 0.0561\n",
      "  Step 285/878, Loss: 0.0560\n",
      "  Step 290/878, Loss: 0.0560\n",
      "  Step 295/878, Loss: 0.0560\n",
      "  Step 300/878, Loss: 0.0560\n",
      "  Step 305/878, Loss: 0.0560\n",
      "  Step 310/878, Loss: 0.0560\n",
      "  Step 315/878, Loss: 0.0559\n",
      "  Step 320/878, Loss: 0.0559\n",
      "  Step 325/878, Loss: 0.0559\n",
      "  Step 330/878, Loss: 0.0559\n",
      "  Step 335/878, Loss: 0.0559\n",
      "  Step 340/878, Loss: 0.0559\n",
      "  Step 345/878, Loss: 0.0558\n",
      "  Step 350/878, Loss: 0.0558\n",
      "  Step 355/878, Loss: 0.0558\n",
      "  Step 360/878, Loss: 0.0558\n",
      "  Step 365/878, Loss: 0.0558\n",
      "  Step 370/878, Loss: 0.0558\n",
      "  Step 375/878, Loss: 0.0558\n",
      "  Step 380/878, Loss: 0.0557\n",
      "  Step 385/878, Loss: 0.0557\n",
      "  Step 390/878, Loss: 0.0557\n",
      "  Step 395/878, Loss: 0.0557\n",
      "  Step 400/878, Loss: 0.0557\n",
      "  Step 405/878, Loss: 0.0557\n",
      "  Step 410/878, Loss: 0.0557\n",
      "  Step 415/878, Loss: 0.0556\n",
      "  Step 420/878, Loss: 0.0556\n",
      "  Step 425/878, Loss: 0.0556\n",
      "  Step 430/878, Loss: 0.0556\n",
      "  Step 435/878, Loss: 0.0556\n",
      "  Step 440/878, Loss: 0.0556\n",
      "  Step 445/878, Loss: 0.0556\n",
      "  Step 450/878, Loss: 0.0555\n",
      "  Step 455/878, Loss: 0.0555\n",
      "  Step 460/878, Loss: 0.0555\n",
      "  Step 465/878, Loss: 0.0555\n",
      "  Step 470/878, Loss: 0.0555\n",
      "  Step 475/878, Loss: 0.0555\n",
      "  Step 480/878, Loss: 0.0555\n",
      "  Step 485/878, Loss: 0.0555\n",
      "  Step 490/878, Loss: 0.0554\n",
      "  Step 495/878, Loss: 0.0554\n",
      "  Step 500/878, Loss: 0.0554\n",
      "  Step 505/878, Loss: 0.0554\n",
      "  Step 510/878, Loss: 0.0554\n",
      "  Step 515/878, Loss: 0.0554\n",
      "  Step 520/878, Loss: 0.0554\n",
      "  Step 525/878, Loss: 0.0553\n",
      "  Step 530/878, Loss: 0.0553\n",
      "  Step 535/878, Loss: 0.0553\n",
      "  Step 540/878, Loss: 0.0553\n",
      "  Step 545/878, Loss: 0.0553\n",
      "  Step 550/878, Loss: 0.0553\n",
      "  Step 555/878, Loss: 0.0553\n",
      "  Step 560/878, Loss: 0.0552\n",
      "  Step 565/878, Loss: 0.0552\n",
      "  Step 570/878, Loss: 0.0552\n",
      "  Step 575/878, Loss: 0.0552\n",
      "  Step 580/878, Loss: 0.0552\n",
      "  Step 585/878, Loss: 0.0552\n",
      "  Step 590/878, Loss: 0.0552\n",
      "  Step 595/878, Loss: 0.0551\n",
      "  Step 600/878, Loss: 0.0551\n",
      "  Step 605/878, Loss: 0.0551\n",
      "  Step 610/878, Loss: 0.0551\n",
      "  Step 615/878, Loss: 0.0551\n",
      "  Step 620/878, Loss: 0.0551\n",
      "  Step 625/878, Loss: 0.0551\n",
      "  Step 630/878, Loss: 0.0551\n",
      "  Step 635/878, Loss: 0.0550\n",
      "  Step 640/878, Loss: 0.0550\n",
      "  Step 645/878, Loss: 0.0550\n",
      "  Step 650/878, Loss: 0.0550\n",
      "  Step 655/878, Loss: 0.0550\n",
      "  Step 660/878, Loss: 0.0550\n",
      "  Step 665/878, Loss: 0.0550\n",
      "  Step 670/878, Loss: 0.0550\n",
      "  Step 675/878, Loss: 0.0549\n",
      "  Step 680/878, Loss: 0.0549\n",
      "  Step 685/878, Loss: 0.0549\n",
      "  Step 690/878, Loss: 0.0549\n",
      "  Step 695/878, Loss: 0.0549\n",
      "  Step 700/878, Loss: 0.0549\n",
      "  Step 705/878, Loss: 0.0549\n",
      "  Step 710/878, Loss: 0.0549\n",
      "  Step 715/878, Loss: 0.0548\n",
      "  Step 720/878, Loss: 0.0548\n",
      "  Step 725/878, Loss: 0.0548\n",
      "  Step 730/878, Loss: 0.0548\n",
      "  Step 735/878, Loss: 0.0548\n",
      "  Step 740/878, Loss: 0.0548\n",
      "  Step 745/878, Loss: 0.0548\n",
      "  Step 750/878, Loss: 0.0548\n",
      "  Step 755/878, Loss: 0.0547\n",
      "  Step 760/878, Loss: 0.0547\n",
      "  Step 765/878, Loss: 0.0547\n",
      "  Step 770/878, Loss: 0.0547\n",
      "  Step 775/878, Loss: 0.0547\n",
      "  Step 780/878, Loss: 0.0547\n",
      "  Step 785/878, Loss: 0.0547\n",
      "  Step 790/878, Loss: 0.0547\n",
      "  Step 795/878, Loss: 0.0546\n",
      "  Step 800/878, Loss: 0.0546\n",
      "  Step 805/878, Loss: 0.0546\n",
      "  Step 810/878, Loss: 0.0546\n",
      "  Step 815/878, Loss: 0.0546\n",
      "  Step 820/878, Loss: 0.0546\n",
      "  Step 825/878, Loss: 0.0546\n",
      "  Step 830/878, Loss: 0.0546\n",
      "  Step 835/878, Loss: 0.0545\n",
      "  Step 840/878, Loss: 0.0545\n",
      "  Step 845/878, Loss: 0.0545\n",
      "  Step 850/878, Loss: 0.0545\n",
      "  Step 855/878, Loss: 0.0545\n",
      "  Step 860/878, Loss: 0.0545\n",
      "  Step 865/878, Loss: 0.0545\n",
      "  Step 870/878, Loss: 0.0545\n",
      "  Step 875/878, Loss: 0.0544\n",
      "  Epoch 2 Loss: 0.0544\n",
      "\n",
      "Epoch 3/20\n",
      "  Step 5/878, Loss: 0.0524\n",
      "  Step 10/878, Loss: 0.0524\n",
      "  Step 15/878, Loss: 0.0524\n",
      "  Step 20/878, Loss: 0.0524\n",
      "  Step 25/878, Loss: 0.0524\n",
      "  Step 30/878, Loss: 0.0524\n",
      "  Step 35/878, Loss: 0.0524\n",
      "  Step 40/878, Loss: 0.0524\n",
      "  Step 45/878, Loss: 0.0524\n",
      "  Step 50/878, Loss: 0.0524\n",
      "  Step 55/878, Loss: 0.0524\n",
      "  Step 60/878, Loss: 0.0524\n",
      "  Step 65/878, Loss: 0.0524\n",
      "  Step 70/878, Loss: 0.0524\n",
      "  Step 75/878, Loss: 0.0524\n",
      "  Step 80/878, Loss: 0.0524\n",
      "  Step 85/878, Loss: 0.0524\n",
      "  Step 90/878, Loss: 0.0523\n",
      "  Step 95/878, Loss: 0.0523\n",
      "  Step 100/878, Loss: 0.0523\n",
      "  Step 105/878, Loss: 0.0523\n",
      "  Step 110/878, Loss: 0.0523\n",
      "  Step 115/878, Loss: 0.0523\n",
      "  Step 120/878, Loss: 0.0523\n",
      "  Step 125/878, Loss: 0.0523\n",
      "  Step 130/878, Loss: 0.0523\n",
      "  Step 135/878, Loss: 0.0523\n",
      "  Step 140/878, Loss: 0.0523\n",
      "  Step 145/878, Loss: 0.0523\n",
      "  Step 150/878, Loss: 0.0523\n",
      "  Step 155/878, Loss: 0.0522\n",
      "  Step 160/878, Loss: 0.0522\n",
      "  Step 165/878, Loss: 0.0522\n",
      "  Step 170/878, Loss: 0.0522\n",
      "  Step 175/878, Loss: 0.0522\n",
      "  Step 180/878, Loss: 0.0522\n",
      "  Step 185/878, Loss: 0.0522\n",
      "  Step 190/878, Loss: 0.0522\n",
      "  Step 195/878, Loss: 0.0522\n",
      "  Step 200/878, Loss: 0.0522\n",
      "  Step 205/878, Loss: 0.0522\n",
      "  Step 210/878, Loss: 0.0522\n",
      "  Step 215/878, Loss: 0.0521\n",
      "  Step 220/878, Loss: 0.0521\n",
      "  Step 225/878, Loss: 0.0521\n",
      "  Step 230/878, Loss: 0.0521\n",
      "  Step 235/878, Loss: 0.0521\n",
      "  Step 240/878, Loss: 0.0521\n",
      "  Step 245/878, Loss: 0.0521\n",
      "  Step 250/878, Loss: 0.0521\n",
      "  Step 255/878, Loss: 0.0521\n",
      "  Step 260/878, Loss: 0.0521\n",
      "  Step 265/878, Loss: 0.0521\n",
      "  Step 270/878, Loss: 0.0521\n",
      "  Step 275/878, Loss: 0.0520\n",
      "  Step 280/878, Loss: 0.0520\n",
      "  Step 285/878, Loss: 0.0520\n",
      "  Step 290/878, Loss: 0.0520\n",
      "  Step 295/878, Loss: 0.0520\n",
      "  Step 300/878, Loss: 0.0520\n",
      "  Step 305/878, Loss: 0.0520\n",
      "  Step 310/878, Loss: 0.0520\n",
      "  Step 315/878, Loss: 0.0520\n",
      "  Step 320/878, Loss: 0.0520\n",
      "  Step 325/878, Loss: 0.0520\n",
      "  Step 330/878, Loss: 0.0519\n",
      "  Step 335/878, Loss: 0.0519\n",
      "  Step 340/878, Loss: 0.0519\n",
      "  Step 345/878, Loss: 0.0519\n",
      "  Step 350/878, Loss: 0.0519\n",
      "  Step 355/878, Loss: 0.0519\n",
      "  Step 360/878, Loss: 0.0519\n",
      "  Step 365/878, Loss: 0.0519\n",
      "  Step 370/878, Loss: 0.0519\n",
      "  Step 375/878, Loss: 0.0519\n",
      "  Step 380/878, Loss: 0.0518\n",
      "  Step 385/878, Loss: 0.0518\n",
      "  Step 390/878, Loss: 0.0518\n",
      "  Step 395/878, Loss: 0.0518\n",
      "  Step 400/878, Loss: 0.0518\n",
      "  Step 405/878, Loss: 0.0518\n",
      "  Step 410/878, Loss: 0.0518\n",
      "  Step 415/878, Loss: 0.0518\n",
      "  Step 420/878, Loss: 0.0518\n",
      "  Step 425/878, Loss: 0.0518\n",
      "  Step 430/878, Loss: 0.0517\n",
      "  Step 435/878, Loss: 0.0517\n",
      "  Step 440/878, Loss: 0.0517\n",
      "  Step 445/878, Loss: 0.0517\n",
      "  Step 450/878, Loss: 0.0517\n",
      "  Step 455/878, Loss: 0.0517\n",
      "  Step 460/878, Loss: 0.0517\n",
      "  Step 465/878, Loss: 0.0517\n",
      "  Step 470/878, Loss: 0.0517\n",
      "  Step 475/878, Loss: 0.0517\n",
      "  Step 480/878, Loss: 0.0516\n",
      "  Step 485/878, Loss: 0.0516\n",
      "  Step 490/878, Loss: 0.0516\n",
      "  Step 495/878, Loss: 0.0516\n",
      "  Step 500/878, Loss: 0.0516\n",
      "  Step 505/878, Loss: 0.0516\n",
      "  Step 510/878, Loss: 0.0516\n",
      "  Step 515/878, Loss: 0.0516\n",
      "  Step 520/878, Loss: 0.0516\n",
      "  Step 525/878, Loss: 0.0516\n",
      "  Step 530/878, Loss: 0.0516\n",
      "  Step 535/878, Loss: 0.0515\n",
      "  Step 540/878, Loss: 0.0515\n",
      "  Step 545/878, Loss: 0.0515\n",
      "  Step 550/878, Loss: 0.0515\n",
      "  Step 555/878, Loss: 0.0515\n",
      "  Step 560/878, Loss: 0.0515\n",
      "  Step 565/878, Loss: 0.0515\n",
      "  Step 570/878, Loss: 0.0515\n",
      "  Step 575/878, Loss: 0.0515\n",
      "  Step 580/878, Loss: 0.0515\n",
      "  Step 585/878, Loss: 0.0515\n",
      "  Step 590/878, Loss: 0.0514\n",
      "  Step 595/878, Loss: 0.0514\n",
      "  Step 600/878, Loss: 0.0514\n",
      "  Step 605/878, Loss: 0.0514\n",
      "  Step 610/878, Loss: 0.0514\n",
      "  Step 615/878, Loss: 0.0514\n",
      "  Step 620/878, Loss: 0.0514\n",
      "  Step 625/878, Loss: 0.0514\n",
      "  Step 630/878, Loss: 0.0514\n",
      "  Step 635/878, Loss: 0.0514\n",
      "  Step 640/878, Loss: 0.0514\n",
      "  Step 645/878, Loss: 0.0513\n",
      "  Step 650/878, Loss: 0.0513\n",
      "  Step 655/878, Loss: 0.0513\n",
      "  Step 660/878, Loss: 0.0513\n",
      "  Step 665/878, Loss: 0.0513\n",
      "  Step 670/878, Loss: 0.0513\n",
      "  Step 675/878, Loss: 0.0513\n",
      "  Step 680/878, Loss: 0.0513\n",
      "  Step 685/878, Loss: 0.0513\n",
      "  Step 690/878, Loss: 0.0513\n",
      "  Step 695/878, Loss: 0.0513\n",
      "  Step 700/878, Loss: 0.0513\n",
      "  Step 705/878, Loss: 0.0512\n",
      "  Step 710/878, Loss: 0.0512\n",
      "  Step 715/878, Loss: 0.0512\n",
      "  Step 720/878, Loss: 0.0512\n",
      "  Step 725/878, Loss: 0.0512\n",
      "  Step 730/878, Loss: 0.0512\n",
      "  Step 735/878, Loss: 0.0512\n",
      "  Step 740/878, Loss: 0.0512\n",
      "  Step 745/878, Loss: 0.0512\n",
      "  Step 750/878, Loss: 0.0512\n",
      "  Step 755/878, Loss: 0.0512\n",
      "  Step 760/878, Loss: 0.0512\n",
      "  Step 765/878, Loss: 0.0511\n",
      "  Step 770/878, Loss: 0.0511\n",
      "  Step 775/878, Loss: 0.0511\n",
      "  Step 780/878, Loss: 0.0511\n",
      "  Step 785/878, Loss: 0.0511\n",
      "  Step 790/878, Loss: 0.0511\n",
      "  Step 795/878, Loss: 0.0511\n",
      "  Step 800/878, Loss: 0.0511\n",
      "  Step 805/878, Loss: 0.0511\n",
      "  Step 810/878, Loss: 0.0511\n",
      "  Step 815/878, Loss: 0.0511\n",
      "  Step 820/878, Loss: 0.0511\n",
      "  Step 825/878, Loss: 0.0510\n",
      "  Step 830/878, Loss: 0.0510\n",
      "  Step 835/878, Loss: 0.0510\n",
      "  Step 840/878, Loss: 0.0510\n",
      "  Step 845/878, Loss: 0.0510\n",
      "  Step 850/878, Loss: 0.0510\n",
      "  Step 855/878, Loss: 0.0510\n",
      "  Step 860/878, Loss: 0.0510\n",
      "  Step 865/878, Loss: 0.0510\n",
      "  Step 870/878, Loss: 0.0510\n",
      "  Step 875/878, Loss: 0.0510\n",
      "  Epoch 3 Loss: 0.0510\n",
      "\n",
      "Epoch 4/20\n",
      "  Step 5/878, Loss: 0.0494\n",
      "  Step 10/878, Loss: 0.0494\n",
      "  Step 15/878, Loss: 0.0494\n",
      "  Step 20/878, Loss: 0.0494\n",
      "  Step 25/878, Loss: 0.0494\n",
      "  Step 30/878, Loss: 0.0494\n",
      "  Step 35/878, Loss: 0.0494\n",
      "  Step 40/878, Loss: 0.0494\n",
      "  Step 45/878, Loss: 0.0494\n",
      "  Step 50/878, Loss: 0.0494\n",
      "  Step 55/878, Loss: 0.0493\n",
      "  Step 60/878, Loss: 0.0493\n",
      "  Step 65/878, Loss: 0.0493\n",
      "  Step 70/878, Loss: 0.0493\n",
      "  Step 75/878, Loss: 0.0493\n",
      "  Step 80/878, Loss: 0.0493\n",
      "  Step 85/878, Loss: 0.0493\n",
      "  Step 90/878, Loss: 0.0493\n",
      "  Step 95/878, Loss: 0.0493\n",
      "  Step 100/878, Loss: 0.0493\n",
      "  Step 105/878, Loss: 0.0493\n",
      "  Step 110/878, Loss: 0.0492\n",
      "  Step 115/878, Loss: 0.0492\n",
      "  Step 120/878, Loss: 0.0492\n",
      "  Step 125/878, Loss: 0.0492\n",
      "  Step 130/878, Loss: 0.0492\n",
      "  Step 135/878, Loss: 0.0492\n",
      "  Step 140/878, Loss: 0.0492\n",
      "  Step 145/878, Loss: 0.0492\n",
      "  Step 150/878, Loss: 0.0492\n",
      "  Step 155/878, Loss: 0.0492\n",
      "  Step 160/878, Loss: 0.0491\n",
      "  Step 165/878, Loss: 0.0491\n",
      "  Step 170/878, Loss: 0.0491\n",
      "  Step 175/878, Loss: 0.0491\n",
      "  Step 180/878, Loss: 0.0491\n",
      "  Step 185/878, Loss: 0.0491\n",
      "  Step 190/878, Loss: 0.0491\n",
      "  Step 195/878, Loss: 0.0491\n",
      "  Step 200/878, Loss: 0.0491\n",
      "  Step 205/878, Loss: 0.0490\n",
      "  Step 210/878, Loss: 0.0490\n",
      "  Step 215/878, Loss: 0.0490\n",
      "  Step 220/878, Loss: 0.0490\n",
      "  Step 225/878, Loss: 0.0490\n",
      "  Step 230/878, Loss: 0.0490\n",
      "  Step 235/878, Loss: 0.0490\n",
      "  Step 240/878, Loss: 0.0490\n",
      "  Step 245/878, Loss: 0.0490\n",
      "  Step 250/878, Loss: 0.0489\n",
      "  Step 255/878, Loss: 0.0489\n",
      "  Step 260/878, Loss: 0.0489\n",
      "  Step 265/878, Loss: 0.0489\n",
      "  Step 270/878, Loss: 0.0489\n",
      "  Step 275/878, Loss: 0.0489\n",
      "  Step 280/878, Loss: 0.0489\n",
      "  Step 285/878, Loss: 0.0489\n",
      "  Step 290/878, Loss: 0.0489\n",
      "  Step 295/878, Loss: 0.0489\n",
      "  Step 300/878, Loss: 0.0488\n",
      "  Step 305/878, Loss: 0.0488\n",
      "  Step 310/878, Loss: 0.0488\n",
      "  Step 315/878, Loss: 0.0488\n",
      "  Step 320/878, Loss: 0.0488\n",
      "  Step 325/878, Loss: 0.0488\n",
      "  Step 330/878, Loss: 0.0488\n",
      "  Step 335/878, Loss: 0.0488\n",
      "  Step 340/878, Loss: 0.0488\n",
      "  Step 345/878, Loss: 0.0488\n",
      "  Step 350/878, Loss: 0.0488\n",
      "  Step 355/878, Loss: 0.0487\n",
      "  Step 360/878, Loss: 0.0487\n",
      "  Step 365/878, Loss: 0.0487\n",
      "  Step 370/878, Loss: 0.0487\n",
      "  Step 375/878, Loss: 0.0487\n",
      "  Step 380/878, Loss: 0.0487\n",
      "  Step 385/878, Loss: 0.0487\n",
      "  Step 390/878, Loss: 0.0487\n",
      "  Step 395/878, Loss: 0.0487\n",
      "  Step 400/878, Loss: 0.0487\n",
      "  Step 405/878, Loss: 0.0487\n",
      "  Step 410/878, Loss: 0.0486\n",
      "  Step 415/878, Loss: 0.0486\n",
      "  Step 420/878, Loss: 0.0486\n",
      "  Step 425/878, Loss: 0.0486\n",
      "  Step 430/878, Loss: 0.0486\n",
      "  Step 435/878, Loss: 0.0486\n",
      "  Step 440/878, Loss: 0.0486\n",
      "  Step 445/878, Loss: 0.0486\n",
      "  Step 450/878, Loss: 0.0486\n",
      "  Step 455/878, Loss: 0.0486\n",
      "  Step 460/878, Loss: 0.0486\n",
      "  Step 465/878, Loss: 0.0486\n",
      "  Step 470/878, Loss: 0.0486\n",
      "  Step 475/878, Loss: 0.0485\n",
      "  Step 480/878, Loss: 0.0485\n",
      "  Step 485/878, Loss: 0.0485\n",
      "  Step 490/878, Loss: 0.0485\n",
      "  Step 495/878, Loss: 0.0485\n",
      "  Step 500/878, Loss: 0.0485\n",
      "  Step 505/878, Loss: 0.0485\n",
      "  Step 510/878, Loss: 0.0485\n",
      "  Step 515/878, Loss: 0.0485\n",
      "  Step 520/878, Loss: 0.0485\n",
      "  Step 525/878, Loss: 0.0485\n",
      "  Step 530/878, Loss: 0.0484\n",
      "  Step 535/878, Loss: 0.0484\n",
      "  Step 540/878, Loss: 0.0484\n",
      "  Step 545/878, Loss: 0.0484\n",
      "  Step 550/878, Loss: 0.0484\n",
      "  Step 555/878, Loss: 0.0484\n",
      "  Step 560/878, Loss: 0.0484\n",
      "  Step 565/878, Loss: 0.0484\n",
      "  Step 570/878, Loss: 0.0484\n",
      "  Step 575/878, Loss: 0.0484\n",
      "  Step 580/878, Loss: 0.0484\n",
      "  Step 585/878, Loss: 0.0484\n",
      "  Step 590/878, Loss: 0.0483\n",
      "  Step 595/878, Loss: 0.0483\n",
      "  Step 600/878, Loss: 0.0483\n",
      "  Step 605/878, Loss: 0.0483\n",
      "  Step 610/878, Loss: 0.0483\n",
      "  Step 615/878, Loss: 0.0483\n",
      "  Step 620/878, Loss: 0.0483\n",
      "  Step 625/878, Loss: 0.0483\n",
      "  Step 630/878, Loss: 0.0483\n",
      "  Step 635/878, Loss: 0.0483\n",
      "  Step 640/878, Loss: 0.0483\n",
      "  Step 645/878, Loss: 0.0483\n",
      "  Step 650/878, Loss: 0.0482\n",
      "  Step 655/878, Loss: 0.0482\n",
      "  Step 660/878, Loss: 0.0482\n",
      "  Step 665/878, Loss: 0.0482\n",
      "  Step 670/878, Loss: 0.0482\n",
      "  Step 675/878, Loss: 0.0482\n",
      "  Step 680/878, Loss: 0.0482\n",
      "  Step 685/878, Loss: 0.0482\n",
      "  Step 690/878, Loss: 0.0482\n",
      "  Step 695/878, Loss: 0.0482\n",
      "  Step 700/878, Loss: 0.0482\n",
      "  Step 705/878, Loss: 0.0482\n",
      "  Step 710/878, Loss: 0.0482\n",
      "  Step 715/878, Loss: 0.0481\n",
      "  Step 720/878, Loss: 0.0481\n",
      "  Step 725/878, Loss: 0.0481\n",
      "  Step 730/878, Loss: 0.0481\n",
      "  Step 735/878, Loss: 0.0481\n",
      "  Step 740/878, Loss: 0.0481\n",
      "  Step 745/878, Loss: 0.0481\n",
      "  Step 750/878, Loss: 0.0481\n",
      "  Step 755/878, Loss: 0.0481\n",
      "  Step 760/878, Loss: 0.0481\n",
      "  Step 765/878, Loss: 0.0481\n",
      "  Step 770/878, Loss: 0.0481\n",
      "  Step 775/878, Loss: 0.0481\n",
      "  Step 780/878, Loss: 0.0480\n",
      "  Step 785/878, Loss: 0.0480\n",
      "  Step 790/878, Loss: 0.0480\n",
      "  Step 795/878, Loss: 0.0480\n",
      "  Step 800/878, Loss: 0.0480\n",
      "  Step 805/878, Loss: 0.0480\n",
      "  Step 810/878, Loss: 0.0480\n",
      "  Step 815/878, Loss: 0.0480\n",
      "  Step 820/878, Loss: 0.0480\n",
      "  Step 825/878, Loss: 0.0480\n",
      "  Step 830/878, Loss: 0.0480\n",
      "  Step 835/878, Loss: 0.0480\n",
      "  Step 840/878, Loss: 0.0480\n",
      "  Step 845/878, Loss: 0.0479\n",
      "  Step 850/878, Loss: 0.0479\n",
      "  Step 855/878, Loss: 0.0479\n",
      "  Step 860/878, Loss: 0.0479\n",
      "  Step 865/878, Loss: 0.0479\n",
      "  Step 870/878, Loss: 0.0479\n",
      "  Step 875/878, Loss: 0.0479\n",
      "  Epoch 4 Loss: 0.0479\n",
      "\n",
      "Epoch 5/20\n",
      "  Step 5/878, Loss: 0.0466\n",
      "  Step 10/878, Loss: 0.0466\n",
      "  Step 15/878, Loss: 0.0466\n",
      "  Step 20/878, Loss: 0.0466\n",
      "  Step 25/878, Loss: 0.0466\n",
      "  Step 30/878, Loss: 0.0466\n",
      "  Step 35/878, Loss: 0.0466\n",
      "  Step 40/878, Loss: 0.0466\n",
      "  Step 45/878, Loss: 0.0466\n",
      "  Step 50/878, Loss: 0.0466\n",
      "  Step 55/878, Loss: 0.0466\n",
      "  Step 60/878, Loss: 0.0465\n",
      "  Step 65/878, Loss: 0.0465\n",
      "  Step 70/878, Loss: 0.0465\n",
      "  Step 75/878, Loss: 0.0465\n",
      "  Step 80/878, Loss: 0.0465\n",
      "  Step 85/878, Loss: 0.0465\n",
      "  Step 90/878, Loss: 0.0465\n",
      "  Step 95/878, Loss: 0.0465\n",
      "  Step 100/878, Loss: 0.0465\n",
      "  Step 105/878, Loss: 0.0465\n",
      "  Step 110/878, Loss: 0.0465\n",
      "  Step 115/878, Loss: 0.0465\n",
      "  Step 120/878, Loss: 0.0465\n",
      "  Step 125/878, Loss: 0.0465\n",
      "  Step 130/878, Loss: 0.0465\n",
      "  Step 135/878, Loss: 0.0464\n",
      "  Step 140/878, Loss: 0.0464\n",
      "  Step 145/878, Loss: 0.0464\n",
      "  Step 150/878, Loss: 0.0464\n",
      "  Step 155/878, Loss: 0.0464\n",
      "  Step 160/878, Loss: 0.0464\n",
      "  Step 165/878, Loss: 0.0464\n",
      "  Step 170/878, Loss: 0.0464\n",
      "  Step 175/878, Loss: 0.0464\n",
      "  Step 180/878, Loss: 0.0464\n",
      "  Step 185/878, Loss: 0.0464\n",
      "  Step 190/878, Loss: 0.0464\n",
      "  Step 195/878, Loss: 0.0464\n",
      "  Step 200/878, Loss: 0.0464\n",
      "  Step 205/878, Loss: 0.0463\n",
      "  Step 210/878, Loss: 0.0463\n",
      "  Step 215/878, Loss: 0.0463\n",
      "  Step 220/878, Loss: 0.0463\n",
      "  Step 225/878, Loss: 0.0463\n",
      "  Step 230/878, Loss: 0.0463\n",
      "  Step 235/878, Loss: 0.0463\n",
      "  Step 240/878, Loss: 0.0463\n",
      "  Step 245/878, Loss: 0.0463\n",
      "  Step 250/878, Loss: 0.0463\n",
      "  Step 255/878, Loss: 0.0463\n",
      "  Step 260/878, Loss: 0.0463\n",
      "  Step 265/878, Loss: 0.0463\n",
      "  Step 270/878, Loss: 0.0463\n",
      "  Step 275/878, Loss: 0.0462\n",
      "  Step 280/878, Loss: 0.0462\n",
      "  Step 285/878, Loss: 0.0462\n",
      "  Step 290/878, Loss: 0.0462\n",
      "  Step 295/878, Loss: 0.0462\n",
      "  Step 300/878, Loss: 0.0462\n",
      "  Step 305/878, Loss: 0.0462\n",
      "  Step 310/878, Loss: 0.0462\n",
      "  Step 315/878, Loss: 0.0462\n",
      "  Step 320/878, Loss: 0.0462\n",
      "  Step 325/878, Loss: 0.0462\n",
      "  Step 330/878, Loss: 0.0462\n",
      "  Step 335/878, Loss: 0.0462\n",
      "  Step 340/878, Loss: 0.0462\n",
      "  Step 345/878, Loss: 0.0462\n",
      "  Step 350/878, Loss: 0.0462\n",
      "  Step 355/878, Loss: 0.0462\n",
      "  Step 360/878, Loss: 0.0461\n",
      "  Step 365/878, Loss: 0.0461\n",
      "  Step 370/878, Loss: 0.0461\n",
      "  Step 375/878, Loss: 0.0461\n",
      "  Step 380/878, Loss: 0.0461\n",
      "  Step 385/878, Loss: 0.0461\n",
      "  Step 390/878, Loss: 0.0461\n",
      "  Step 395/878, Loss: 0.0461\n",
      "  Step 400/878, Loss: 0.0461\n",
      "  Step 405/878, Loss: 0.0461\n",
      "  Step 410/878, Loss: 0.0461\n",
      "  Step 415/878, Loss: 0.0461\n",
      "  Step 420/878, Loss: 0.0461\n",
      "  Step 425/878, Loss: 0.0461\n",
      "  Step 430/878, Loss: 0.0461\n",
      "  Step 435/878, Loss: 0.0461\n",
      "  Step 440/878, Loss: 0.0460\n",
      "  Step 445/878, Loss: 0.0460\n",
      "  Step 450/878, Loss: 0.0460\n",
      "  Step 455/878, Loss: 0.0460\n",
      "  Step 460/878, Loss: 0.0460\n",
      "  Step 465/878, Loss: 0.0460\n",
      "  Step 470/878, Loss: 0.0460\n",
      "  Step 475/878, Loss: 0.0460\n",
      "  Step 480/878, Loss: 0.0460\n",
      "  Step 485/878, Loss: 0.0460\n",
      "  Step 490/878, Loss: 0.0460\n",
      "  Step 495/878, Loss: 0.0460\n",
      "  Step 500/878, Loss: 0.0460\n",
      "  Step 505/878, Loss: 0.0460\n",
      "  Step 510/878, Loss: 0.0460\n",
      "  Step 515/878, Loss: 0.0460\n",
      "  Step 520/878, Loss: 0.0459\n",
      "  Step 525/878, Loss: 0.0459\n",
      "  Step 530/878, Loss: 0.0459\n",
      "  Step 535/878, Loss: 0.0459\n",
      "  Step 540/878, Loss: 0.0459\n",
      "  Step 545/878, Loss: 0.0459\n",
      "  Step 550/878, Loss: 0.0459\n",
      "  Step 555/878, Loss: 0.0459\n",
      "  Step 560/878, Loss: 0.0459\n",
      "  Step 565/878, Loss: 0.0459\n",
      "  Step 570/878, Loss: 0.0459\n",
      "  Step 575/878, Loss: 0.0459\n",
      "  Step 580/878, Loss: 0.0459\n",
      "  Step 585/878, Loss: 0.0459\n",
      "  Step 590/878, Loss: 0.0459\n",
      "  Step 595/878, Loss: 0.0459\n",
      "  Step 600/878, Loss: 0.0458\n",
      "  Step 605/878, Loss: 0.0458\n",
      "  Step 610/878, Loss: 0.0458\n",
      "  Step 615/878, Loss: 0.0458\n",
      "  Step 620/878, Loss: 0.0458\n",
      "  Step 625/878, Loss: 0.0458\n",
      "  Step 630/878, Loss: 0.0458\n",
      "  Step 635/878, Loss: 0.0458\n",
      "  Step 640/878, Loss: 0.0458\n",
      "  Step 645/878, Loss: 0.0458\n",
      "  Step 650/878, Loss: 0.0458\n",
      "  Step 655/878, Loss: 0.0458\n",
      "  Step 660/878, Loss: 0.0458\n",
      "  Step 665/878, Loss: 0.0458\n",
      "  Step 670/878, Loss: 0.0458\n",
      "  Step 675/878, Loss: 0.0458\n",
      "  Step 680/878, Loss: 0.0458\n",
      "  Step 685/878, Loss: 0.0458\n",
      "  Step 690/878, Loss: 0.0457\n",
      "  Step 695/878, Loss: 0.0457\n",
      "  Step 700/878, Loss: 0.0457\n",
      "  Step 705/878, Loss: 0.0457\n",
      "  Step 710/878, Loss: 0.0457\n",
      "  Step 715/878, Loss: 0.0457\n",
      "  Step 720/878, Loss: 0.0457\n",
      "  Step 725/878, Loss: 0.0457\n",
      "  Step 730/878, Loss: 0.0457\n",
      "  Step 735/878, Loss: 0.0457\n",
      "  Step 740/878, Loss: 0.0457\n",
      "  Step 745/878, Loss: 0.0457\n",
      "  Step 750/878, Loss: 0.0457\n",
      "  Step 755/878, Loss: 0.0457\n",
      "  Step 760/878, Loss: 0.0457\n",
      "  Step 765/878, Loss: 0.0457\n",
      "  Step 770/878, Loss: 0.0456\n",
      "  Step 775/878, Loss: 0.0456\n",
      "  Step 780/878, Loss: 0.0456\n",
      "  Step 785/878, Loss: 0.0456\n",
      "  Step 790/878, Loss: 0.0456\n",
      "  Step 795/878, Loss: 0.0456\n",
      "  Step 800/878, Loss: 0.0456\n",
      "  Step 805/878, Loss: 0.0456\n",
      "  Step 810/878, Loss: 0.0456\n",
      "  Step 815/878, Loss: 0.0456\n",
      "  Step 820/878, Loss: 0.0456\n",
      "  Step 825/878, Loss: 0.0456\n",
      "  Step 830/878, Loss: 0.0456\n",
      "  Step 835/878, Loss: 0.0456\n",
      "  Step 840/878, Loss: 0.0456\n",
      "  Step 845/878, Loss: 0.0456\n",
      "  Step 850/878, Loss: 0.0456\n",
      "  Step 855/878, Loss: 0.0455\n",
      "  Step 860/878, Loss: 0.0455\n",
      "  Step 865/878, Loss: 0.0455\n",
      "  Step 870/878, Loss: 0.0455\n",
      "  Step 875/878, Loss: 0.0455\n",
      "  Epoch 5 Loss: 0.0455\n",
      "\n",
      "Epoch 6/20\n",
      "  Step 5/878, Loss: 0.0445\n",
      "  Step 10/878, Loss: 0.0445\n",
      "  Step 15/878, Loss: 0.0445\n",
      "  Step 20/878, Loss: 0.0445\n",
      "  Step 25/878, Loss: 0.0445\n",
      "  Step 30/878, Loss: 0.0445\n",
      "  Step 35/878, Loss: 0.0445\n",
      "  Step 40/878, Loss: 0.0445\n",
      "  Step 45/878, Loss: 0.0445\n",
      "  Step 50/878, Loss: 0.0445\n",
      "  Step 55/878, Loss: 0.0445\n",
      "  Step 60/878, Loss: 0.0445\n",
      "  Step 65/878, Loss: 0.0445\n",
      "  Step 70/878, Loss: 0.0445\n",
      "  Step 75/878, Loss: 0.0444\n",
      "  Step 80/878, Loss: 0.0444\n",
      "  Step 85/878, Loss: 0.0444\n",
      "  Step 90/878, Loss: 0.0444\n",
      "  Step 95/878, Loss: 0.0444\n",
      "  Step 100/878, Loss: 0.0444\n",
      "  Step 105/878, Loss: 0.0444\n",
      "  Step 110/878, Loss: 0.0444\n",
      "  Step 115/878, Loss: 0.0444\n",
      "  Step 120/878, Loss: 0.0444\n",
      "  Step 125/878, Loss: 0.0444\n",
      "  Step 130/878, Loss: 0.0444\n",
      "  Step 135/878, Loss: 0.0444\n",
      "  Step 140/878, Loss: 0.0443\n",
      "  Step 145/878, Loss: 0.0443\n",
      "  Step 150/878, Loss: 0.0443\n",
      "  Step 155/878, Loss: 0.0443\n",
      "  Step 160/878, Loss: 0.0443\n",
      "  Step 165/878, Loss: 0.0443\n",
      "  Step 170/878, Loss: 0.0443\n",
      "  Step 175/878, Loss: 0.0443\n",
      "  Step 180/878, Loss: 0.0443\n",
      "  Step 185/878, Loss: 0.0443\n",
      "  Step 190/878, Loss: 0.0443\n",
      "  Step 195/878, Loss: 0.0443\n",
      "  Step 200/878, Loss: 0.0443\n",
      "  Step 205/878, Loss: 0.0443\n",
      "  Step 210/878, Loss: 0.0443\n",
      "  Step 215/878, Loss: 0.0443\n",
      "  Step 220/878, Loss: 0.0443\n",
      "  Step 225/878, Loss: 0.0443\n",
      "  Step 230/878, Loss: 0.0442\n",
      "  Step 235/878, Loss: 0.0442\n",
      "  Step 240/878, Loss: 0.0442\n",
      "  Step 245/878, Loss: 0.0442\n",
      "  Step 250/878, Loss: 0.0442\n",
      "  Step 255/878, Loss: 0.0442\n",
      "  Step 260/878, Loss: 0.0442\n",
      "  Step 265/878, Loss: 0.0442\n",
      "  Step 270/878, Loss: 0.0442\n",
      "  Step 275/878, Loss: 0.0442\n",
      "  Step 280/878, Loss: 0.0442\n",
      "  Step 285/878, Loss: 0.0442\n",
      "  Step 290/878, Loss: 0.0442\n",
      "  Step 295/878, Loss: 0.0442\n",
      "  Step 300/878, Loss: 0.0442\n",
      "  Step 305/878, Loss: 0.0442\n",
      "  Step 310/878, Loss: 0.0442\n",
      "  Step 315/878, Loss: 0.0442\n",
      "  Step 320/878, Loss: 0.0441\n",
      "  Step 325/878, Loss: 0.0441\n",
      "  Step 330/878, Loss: 0.0441\n",
      "  Step 335/878, Loss: 0.0441\n",
      "  Step 340/878, Loss: 0.0441\n",
      "  Step 345/878, Loss: 0.0441\n",
      "  Step 350/878, Loss: 0.0441\n",
      "  Step 355/878, Loss: 0.0441\n",
      "  Step 360/878, Loss: 0.0441\n",
      "  Step 365/878, Loss: 0.0441\n",
      "  Step 370/878, Loss: 0.0441\n",
      "  Step 375/878, Loss: 0.0441\n",
      "  Step 380/878, Loss: 0.0441\n",
      "  Step 385/878, Loss: 0.0441\n",
      "  Step 390/878, Loss: 0.0441\n",
      "  Step 395/878, Loss: 0.0441\n",
      "  Step 400/878, Loss: 0.0441\n",
      "  Step 405/878, Loss: 0.0441\n",
      "  Step 410/878, Loss: 0.0441\n",
      "  Step 415/878, Loss: 0.0440\n",
      "  Step 420/878, Loss: 0.0440\n",
      "  Step 425/878, Loss: 0.0440\n",
      "  Step 430/878, Loss: 0.0440\n",
      "  Step 435/878, Loss: 0.0440\n",
      "  Step 440/878, Loss: 0.0440\n",
      "  Step 445/878, Loss: 0.0440\n",
      "  Step 450/878, Loss: 0.0440\n",
      "  Step 455/878, Loss: 0.0440\n",
      "  Step 460/878, Loss: 0.0440\n",
      "  Step 465/878, Loss: 0.0440\n",
      "  Step 470/878, Loss: 0.0440\n",
      "  Step 475/878, Loss: 0.0440\n",
      "  Step 480/878, Loss: 0.0440\n",
      "  Step 485/878, Loss: 0.0440\n",
      "  Step 490/878, Loss: 0.0440\n",
      "  Step 495/878, Loss: 0.0440\n",
      "  Step 500/878, Loss: 0.0440\n",
      "  Step 505/878, Loss: 0.0440\n",
      "  Step 510/878, Loss: 0.0440\n",
      "  Step 515/878, Loss: 0.0440\n",
      "  Step 520/878, Loss: 0.0439\n",
      "  Step 525/878, Loss: 0.0439\n",
      "  Step 530/878, Loss: 0.0439\n",
      "  Step 535/878, Loss: 0.0439\n",
      "  Step 540/878, Loss: 0.0439\n",
      "  Step 545/878, Loss: 0.0439\n",
      "  Step 550/878, Loss: 0.0439\n",
      "  Step 555/878, Loss: 0.0439\n",
      "  Step 560/878, Loss: 0.0439\n",
      "  Step 565/878, Loss: 0.0439\n",
      "  Step 570/878, Loss: 0.0439\n",
      "  Step 575/878, Loss: 0.0439\n",
      "  Step 580/878, Loss: 0.0439\n",
      "  Step 585/878, Loss: 0.0439\n",
      "  Step 590/878, Loss: 0.0439\n",
      "  Step 595/878, Loss: 0.0439\n",
      "  Step 600/878, Loss: 0.0439\n",
      "  Step 605/878, Loss: 0.0439\n",
      "  Step 610/878, Loss: 0.0439\n",
      "  Step 615/878, Loss: 0.0439\n",
      "  Step 620/878, Loss: 0.0438\n",
      "  Step 625/878, Loss: 0.0438\n",
      "  Step 630/878, Loss: 0.0438\n",
      "  Step 635/878, Loss: 0.0438\n",
      "  Step 640/878, Loss: 0.0438\n",
      "  Step 645/878, Loss: 0.0438\n",
      "  Step 650/878, Loss: 0.0438\n",
      "  Step 655/878, Loss: 0.0438\n",
      "  Step 660/878, Loss: 0.0438\n",
      "  Step 665/878, Loss: 0.0438\n",
      "  Step 670/878, Loss: 0.0438\n",
      "  Step 675/878, Loss: 0.0438\n",
      "  Step 680/878, Loss: 0.0438\n",
      "  Step 685/878, Loss: 0.0438\n",
      "  Step 690/878, Loss: 0.0438\n",
      "  Step 695/878, Loss: 0.0438\n",
      "  Step 700/878, Loss: 0.0438\n",
      "  Step 705/878, Loss: 0.0438\n",
      "  Step 710/878, Loss: 0.0438\n",
      "  Step 715/878, Loss: 0.0438\n",
      "  Step 720/878, Loss: 0.0438\n",
      "  Step 725/878, Loss: 0.0438\n",
      "  Step 730/878, Loss: 0.0437\n",
      "  Step 735/878, Loss: 0.0437\n",
      "  Step 740/878, Loss: 0.0437\n",
      "  Step 745/878, Loss: 0.0437\n",
      "  Step 750/878, Loss: 0.0437\n",
      "  Step 755/878, Loss: 0.0437\n",
      "  Step 760/878, Loss: 0.0437\n",
      "  Step 765/878, Loss: 0.0437\n",
      "  Step 770/878, Loss: 0.0437\n",
      "  Step 775/878, Loss: 0.0437\n",
      "  Step 780/878, Loss: 0.0437\n",
      "  Step 785/878, Loss: 0.0437\n",
      "  Step 790/878, Loss: 0.0437\n",
      "  Step 795/878, Loss: 0.0437\n",
      "  Step 800/878, Loss: 0.0437\n",
      "  Step 805/878, Loss: 0.0437\n",
      "  Step 810/878, Loss: 0.0437\n",
      "  Step 815/878, Loss: 0.0437\n",
      "  Step 820/878, Loss: 0.0437\n",
      "  Step 825/878, Loss: 0.0437\n",
      "  Step 830/878, Loss: 0.0437\n",
      "  Step 835/878, Loss: 0.0436\n",
      "  Step 840/878, Loss: 0.0436\n",
      "  Step 845/878, Loss: 0.0436\n",
      "  Step 850/878, Loss: 0.0436\n",
      "  Step 855/878, Loss: 0.0436\n",
      "  Step 860/878, Loss: 0.0436\n",
      "  Step 865/878, Loss: 0.0436\n",
      "  Step 870/878, Loss: 0.0436\n",
      "  Step 875/878, Loss: 0.0436\n",
      "  Epoch 6 Loss: 0.0436\n",
      "\n",
      "Epoch 7/20\n",
      "  Step 5/878, Loss: 0.0427\n",
      "  Step 10/878, Loss: 0.0427\n",
      "  Step 15/878, Loss: 0.0427\n",
      "  Step 20/878, Loss: 0.0427\n",
      "  Step 25/878, Loss: 0.0427\n",
      "  Step 30/878, Loss: 0.0427\n",
      "  Step 35/878, Loss: 0.0427\n",
      "  Step 40/878, Loss: 0.0427\n",
      "  Step 45/878, Loss: 0.0427\n",
      "  Step 50/878, Loss: 0.0427\n",
      "  Step 55/878, Loss: 0.0427\n",
      "  Step 60/878, Loss: 0.0427\n",
      "  Step 65/878, Loss: 0.0427\n",
      "  Step 70/878, Loss: 0.0427\n",
      "  Step 75/878, Loss: 0.0427\n",
      "  Step 80/878, Loss: 0.0427\n",
      "  Step 85/878, Loss: 0.0426\n",
      "  Step 90/878, Loss: 0.0426\n",
      "  Step 95/878, Loss: 0.0426\n",
      "  Step 100/878, Loss: 0.0426\n",
      "  Step 105/878, Loss: 0.0426\n",
      "  Step 110/878, Loss: 0.0426\n",
      "  Step 115/878, Loss: 0.0426\n",
      "  Step 120/878, Loss: 0.0426\n",
      "  Step 125/878, Loss: 0.0426\n",
      "  Step 130/878, Loss: 0.0426\n",
      "  Step 135/878, Loss: 0.0426\n",
      "  Step 140/878, Loss: 0.0426\n",
      "  Step 145/878, Loss: 0.0426\n",
      "  Step 150/878, Loss: 0.0426\n",
      "  Step 155/878, Loss: 0.0426\n",
      "  Step 160/878, Loss: 0.0426\n",
      "  Step 165/878, Loss: 0.0426\n",
      "  Step 170/878, Loss: 0.0426\n",
      "  Step 175/878, Loss: 0.0426\n",
      "  Step 180/878, Loss: 0.0426\n",
      "  Step 185/878, Loss: 0.0425\n",
      "  Step 190/878, Loss: 0.0425\n",
      "  Step 195/878, Loss: 0.0425\n",
      "  Step 200/878, Loss: 0.0425\n",
      "  Step 205/878, Loss: 0.0425\n",
      "  Step 210/878, Loss: 0.0425\n",
      "  Step 215/878, Loss: 0.0425\n",
      "  Step 220/878, Loss: 0.0425\n",
      "  Step 225/878, Loss: 0.0425\n",
      "  Step 230/878, Loss: 0.0425\n",
      "  Step 235/878, Loss: 0.0425\n",
      "  Step 240/878, Loss: 0.0425\n",
      "  Step 245/878, Loss: 0.0425\n",
      "  Step 250/878, Loss: 0.0425\n",
      "  Step 255/878, Loss: 0.0425\n",
      "  Step 260/878, Loss: 0.0425\n",
      "  Step 265/878, Loss: 0.0425\n",
      "  Step 270/878, Loss: 0.0425\n",
      "  Step 275/878, Loss: 0.0425\n",
      "  Step 280/878, Loss: 0.0425\n",
      "  Step 285/878, Loss: 0.0425\n",
      "  Step 290/878, Loss: 0.0425\n",
      "  Step 295/878, Loss: 0.0425\n",
      "  Step 300/878, Loss: 0.0424\n",
      "  Step 305/878, Loss: 0.0424\n",
      "  Step 310/878, Loss: 0.0424\n",
      "  Step 315/878, Loss: 0.0424\n",
      "  Step 320/878, Loss: 0.0424\n",
      "  Step 325/878, Loss: 0.0424\n",
      "  Step 330/878, Loss: 0.0424\n",
      "  Step 335/878, Loss: 0.0424\n",
      "  Step 340/878, Loss: 0.0424\n",
      "  Step 345/878, Loss: 0.0424\n",
      "  Step 350/878, Loss: 0.0424\n",
      "  Step 355/878, Loss: 0.0424\n",
      "  Step 360/878, Loss: 0.0424\n",
      "  Step 365/878, Loss: 0.0424\n",
      "  Step 370/878, Loss: 0.0424\n",
      "  Step 375/878, Loss: 0.0424\n",
      "  Step 380/878, Loss: 0.0424\n",
      "  Step 385/878, Loss: 0.0424\n",
      "  Step 390/878, Loss: 0.0424\n",
      "  Step 395/878, Loss: 0.0424\n",
      "  Step 400/878, Loss: 0.0424\n",
      "  Step 405/878, Loss: 0.0423\n",
      "  Step 410/878, Loss: 0.0423\n",
      "  Step 415/878, Loss: 0.0423\n",
      "  Step 420/878, Loss: 0.0423\n",
      "  Step 425/878, Loss: 0.0423\n",
      "  Step 430/878, Loss: 0.0423\n",
      "  Step 435/878, Loss: 0.0423\n",
      "  Step 440/878, Loss: 0.0423\n",
      "  Step 445/878, Loss: 0.0423\n",
      "  Step 450/878, Loss: 0.0423\n",
      "  Step 455/878, Loss: 0.0423\n",
      "  Step 460/878, Loss: 0.0423\n",
      "  Step 465/878, Loss: 0.0423\n",
      "  Step 470/878, Loss: 0.0423\n",
      "  Step 475/878, Loss: 0.0423\n",
      "  Step 480/878, Loss: 0.0423\n",
      "  Step 485/878, Loss: 0.0423\n",
      "  Step 490/878, Loss: 0.0423\n",
      "  Step 495/878, Loss: 0.0423\n",
      "  Step 500/878, Loss: 0.0423\n",
      "  Step 505/878, Loss: 0.0423\n",
      "  Step 510/878, Loss: 0.0422\n",
      "  Step 515/878, Loss: 0.0422\n",
      "  Step 520/878, Loss: 0.0422\n",
      "  Step 525/878, Loss: 0.0422\n",
      "  Step 530/878, Loss: 0.0422\n",
      "  Step 535/878, Loss: 0.0422\n",
      "  Step 540/878, Loss: 0.0422\n",
      "  Step 545/878, Loss: 0.0422\n",
      "  Step 550/878, Loss: 0.0422\n",
      "  Step 555/878, Loss: 0.0422\n",
      "  Step 560/878, Loss: 0.0422\n",
      "  Step 565/878, Loss: 0.0422\n",
      "  Step 570/878, Loss: 0.0422\n",
      "  Step 575/878, Loss: 0.0422\n",
      "  Step 580/878, Loss: 0.0422\n",
      "  Step 585/878, Loss: 0.0422\n",
      "  Step 590/878, Loss: 0.0422\n",
      "  Step 595/878, Loss: 0.0422\n",
      "  Step 600/878, Loss: 0.0422\n",
      "  Step 605/878, Loss: 0.0422\n",
      "  Step 610/878, Loss: 0.0421\n",
      "  Step 615/878, Loss: 0.0421\n",
      "  Step 620/878, Loss: 0.0421\n",
      "  Step 625/878, Loss: 0.0421\n",
      "  Step 630/878, Loss: 0.0421\n",
      "  Step 635/878, Loss: 0.0421\n",
      "  Step 640/878, Loss: 0.0421\n",
      "  Step 645/878, Loss: 0.0421\n",
      "  Step 650/878, Loss: 0.0421\n",
      "  Step 655/878, Loss: 0.0421\n",
      "  Step 660/878, Loss: 0.0421\n",
      "  Step 665/878, Loss: 0.0421\n",
      "  Step 670/878, Loss: 0.0421\n",
      "  Step 675/878, Loss: 0.0421\n",
      "  Step 680/878, Loss: 0.0421\n",
      "  Step 685/878, Loss: 0.0421\n",
      "  Step 690/878, Loss: 0.0421\n",
      "  Step 695/878, Loss: 0.0421\n",
      "  Step 700/878, Loss: 0.0421\n",
      "  Step 705/878, Loss: 0.0421\n",
      "  Step 710/878, Loss: 0.0421\n",
      "  Step 715/878, Loss: 0.0421\n",
      "  Step 720/878, Loss: 0.0421\n",
      "  Step 725/878, Loss: 0.0421\n",
      "  Step 730/878, Loss: 0.0420\n",
      "  Step 735/878, Loss: 0.0420\n",
      "  Step 740/878, Loss: 0.0420\n",
      "  Step 745/878, Loss: 0.0420\n",
      "  Step 750/878, Loss: 0.0420\n",
      "  Step 755/878, Loss: 0.0420\n",
      "  Step 760/878, Loss: 0.0420\n",
      "  Step 765/878, Loss: 0.0420\n",
      "  Step 770/878, Loss: 0.0420\n",
      "  Step 775/878, Loss: 0.0420\n",
      "  Step 780/878, Loss: 0.0420\n",
      "  Step 785/878, Loss: 0.0420\n",
      "  Step 790/878, Loss: 0.0420\n",
      "  Step 795/878, Loss: 0.0420\n",
      "  Step 800/878, Loss: 0.0420\n",
      "  Step 805/878, Loss: 0.0420\n",
      "  Step 810/878, Loss: 0.0420\n",
      "  Step 815/878, Loss: 0.0420\n",
      "  Step 820/878, Loss: 0.0420\n",
      "  Step 825/878, Loss: 0.0420\n",
      "  Step 830/878, Loss: 0.0420\n",
      "  Step 835/878, Loss: 0.0419\n",
      "  Step 840/878, Loss: 0.0419\n",
      "  Step 845/878, Loss: 0.0419\n",
      "  Step 850/878, Loss: 0.0419\n",
      "  Step 855/878, Loss: 0.0419\n",
      "  Step 860/878, Loss: 0.0419\n",
      "  Step 865/878, Loss: 0.0419\n",
      "  Step 870/878, Loss: 0.0419\n",
      "  Step 875/878, Loss: 0.0419\n",
      "  Epoch 7 Loss: 0.0419\n",
      "\n",
      "Epoch 8/20\n",
      "  Step 5/878, Loss: 0.0411\n",
      "  Step 10/878, Loss: 0.0411\n",
      "  Step 15/878, Loss: 0.0411\n",
      "  Step 20/878, Loss: 0.0411\n",
      "  Step 25/878, Loss: 0.0411\n",
      "  Step 30/878, Loss: 0.0411\n",
      "  Step 35/878, Loss: 0.0410\n",
      "  Step 40/878, Loss: 0.0410\n",
      "  Step 45/878, Loss: 0.0410\n",
      "  Step 50/878, Loss: 0.0410\n",
      "  Step 55/878, Loss: 0.0410\n",
      "  Step 60/878, Loss: 0.0410\n",
      "  Step 65/878, Loss: 0.0410\n",
      "  Step 70/878, Loss: 0.0410\n",
      "  Step 75/878, Loss: 0.0410\n",
      "  Step 80/878, Loss: 0.0410\n",
      "  Step 85/878, Loss: 0.0410\n",
      "  Step 90/878, Loss: 0.0410\n",
      "  Step 95/878, Loss: 0.0410\n",
      "  Step 100/878, Loss: 0.0410\n",
      "  Step 105/878, Loss: 0.0410\n",
      "  Step 110/878, Loss: 0.0410\n",
      "  Step 115/878, Loss: 0.0410\n",
      "  Step 120/878, Loss: 0.0410\n",
      "  Step 125/878, Loss: 0.0410\n",
      "  Step 130/878, Loss: 0.0410\n",
      "  Step 135/878, Loss: 0.0410\n",
      "  Step 140/878, Loss: 0.0410\n",
      "  Step 145/878, Loss: 0.0410\n",
      "  Step 150/878, Loss: 0.0410\n",
      "  Step 155/878, Loss: 0.0409\n",
      "  Step 160/878, Loss: 0.0409\n",
      "  Step 165/878, Loss: 0.0409\n",
      "  Step 170/878, Loss: 0.0409\n",
      "  Step 175/878, Loss: 0.0409\n",
      "  Step 180/878, Loss: 0.0409\n",
      "  Step 185/878, Loss: 0.0409\n",
      "  Step 190/878, Loss: 0.0409\n",
      "  Step 195/878, Loss: 0.0409\n",
      "  Step 200/878, Loss: 0.0409\n",
      "  Step 205/878, Loss: 0.0409\n",
      "  Step 210/878, Loss: 0.0409\n",
      "  Step 215/878, Loss: 0.0409\n",
      "  Step 220/878, Loss: 0.0409\n",
      "  Step 225/878, Loss: 0.0409\n",
      "  Step 230/878, Loss: 0.0409\n",
      "  Step 235/878, Loss: 0.0409\n",
      "  Step 240/878, Loss: 0.0409\n",
      "  Step 245/878, Loss: 0.0409\n",
      "  Step 250/878, Loss: 0.0409\n",
      "  Step 255/878, Loss: 0.0409\n",
      "  Step 260/878, Loss: 0.0408\n",
      "  Step 265/878, Loss: 0.0408\n",
      "  Step 270/878, Loss: 0.0408\n",
      "  Step 275/878, Loss: 0.0408\n",
      "  Step 280/878, Loss: 0.0408\n",
      "  Step 285/878, Loss: 0.0408\n",
      "  Step 290/878, Loss: 0.0408\n",
      "  Step 295/878, Loss: 0.0408\n",
      "  Step 300/878, Loss: 0.0408\n",
      "  Step 305/878, Loss: 0.0408\n",
      "  Step 310/878, Loss: 0.0408\n",
      "  Step 315/878, Loss: 0.0408\n",
      "  Step 320/878, Loss: 0.0408\n",
      "  Step 325/878, Loss: 0.0408\n",
      "  Step 330/878, Loss: 0.0408\n",
      "  Step 335/878, Loss: 0.0408\n",
      "  Step 340/878, Loss: 0.0408\n",
      "  Step 345/878, Loss: 0.0408\n",
      "  Step 350/878, Loss: 0.0408\n",
      "  Step 355/878, Loss: 0.0408\n",
      "  Step 360/878, Loss: 0.0408\n",
      "  Step 365/878, Loss: 0.0408\n",
      "  Step 370/878, Loss: 0.0407\n",
      "  Step 375/878, Loss: 0.0407\n",
      "  Step 380/878, Loss: 0.0407\n",
      "  Step 385/878, Loss: 0.0407\n",
      "  Step 390/878, Loss: 0.0407\n",
      "  Step 395/878, Loss: 0.0407\n",
      "  Step 400/878, Loss: 0.0407\n",
      "  Step 405/878, Loss: 0.0407\n",
      "  Step 410/878, Loss: 0.0407\n",
      "  Step 415/878, Loss: 0.0407\n",
      "  Step 420/878, Loss: 0.0407\n",
      "  Step 425/878, Loss: 0.0407\n",
      "  Step 430/878, Loss: 0.0407\n",
      "  Step 435/878, Loss: 0.0407\n",
      "  Step 440/878, Loss: 0.0407\n",
      "  Step 445/878, Loss: 0.0407\n",
      "  Step 450/878, Loss: 0.0407\n",
      "  Step 455/878, Loss: 0.0407\n",
      "  Step 460/878, Loss: 0.0407\n",
      "  Step 465/878, Loss: 0.0407\n",
      "  Step 470/878, Loss: 0.0406\n",
      "  Step 475/878, Loss: 0.0406\n",
      "  Step 480/878, Loss: 0.0406\n",
      "  Step 485/878, Loss: 0.0406\n",
      "  Step 490/878, Loss: 0.0406\n",
      "  Step 495/878, Loss: 0.0406\n",
      "  Step 500/878, Loss: 0.0406\n",
      "  Step 505/878, Loss: 0.0406\n",
      "  Step 510/878, Loss: 0.0406\n",
      "  Step 515/878, Loss: 0.0406\n",
      "  Step 520/878, Loss: 0.0406\n",
      "  Step 525/878, Loss: 0.0406\n",
      "  Step 530/878, Loss: 0.0406\n",
      "  Step 535/878, Loss: 0.0406\n",
      "  Step 540/878, Loss: 0.0406\n",
      "  Step 545/878, Loss: 0.0406\n",
      "  Step 550/878, Loss: 0.0406\n",
      "  Step 555/878, Loss: 0.0406\n",
      "  Step 560/878, Loss: 0.0406\n",
      "  Step 565/878, Loss: 0.0406\n",
      "  Step 570/878, Loss: 0.0406\n",
      "  Step 575/878, Loss: 0.0406\n",
      "  Step 580/878, Loss: 0.0405\n",
      "  Step 585/878, Loss: 0.0405\n",
      "  Step 590/878, Loss: 0.0405\n",
      "  Step 595/878, Loss: 0.0405\n",
      "  Step 600/878, Loss: 0.0405\n",
      "  Step 605/878, Loss: 0.0405\n",
      "  Step 610/878, Loss: 0.0405\n",
      "  Step 615/878, Loss: 0.0405\n",
      "  Step 620/878, Loss: 0.0405\n",
      "  Step 625/878, Loss: 0.0405\n",
      "  Step 630/878, Loss: 0.0405\n",
      "  Step 635/878, Loss: 0.0405\n",
      "  Step 640/878, Loss: 0.0405\n",
      "  Step 645/878, Loss: 0.0405\n",
      "  Step 650/878, Loss: 0.0405\n",
      "  Step 655/878, Loss: 0.0405\n",
      "  Step 660/878, Loss: 0.0405\n",
      "  Step 665/878, Loss: 0.0405\n",
      "  Step 670/878, Loss: 0.0405\n",
      "  Step 675/878, Loss: 0.0405\n",
      "  Step 680/878, Loss: 0.0405\n",
      "  Step 685/878, Loss: 0.0404\n",
      "  Step 690/878, Loss: 0.0404\n",
      "  Step 695/878, Loss: 0.0404\n",
      "  Step 700/878, Loss: 0.0404\n",
      "  Step 705/878, Loss: 0.0404\n",
      "  Step 710/878, Loss: 0.0404\n",
      "  Step 715/878, Loss: 0.0404\n",
      "  Step 720/878, Loss: 0.0404\n",
      "  Step 725/878, Loss: 0.0404\n",
      "  Step 730/878, Loss: 0.0404\n",
      "  Step 735/878, Loss: 0.0404\n",
      "  Step 740/878, Loss: 0.0404\n",
      "  Step 745/878, Loss: 0.0404\n",
      "  Step 750/878, Loss: 0.0404\n",
      "  Step 755/878, Loss: 0.0404\n",
      "  Step 760/878, Loss: 0.0404\n",
      "  Step 765/878, Loss: 0.0404\n",
      "  Step 770/878, Loss: 0.0404\n",
      "  Step 775/878, Loss: 0.0404\n",
      "  Step 780/878, Loss: 0.0404\n",
      "  Step 785/878, Loss: 0.0404\n",
      "  Step 790/878, Loss: 0.0404\n",
      "  Step 795/878, Loss: 0.0403\n",
      "  Step 800/878, Loss: 0.0403\n",
      "  Step 805/878, Loss: 0.0403\n",
      "  Step 810/878, Loss: 0.0403\n",
      "  Step 815/878, Loss: 0.0403\n",
      "  Step 820/878, Loss: 0.0403\n",
      "  Step 825/878, Loss: 0.0403\n",
      "  Step 830/878, Loss: 0.0403\n",
      "  Step 835/878, Loss: 0.0403\n",
      "  Step 840/878, Loss: 0.0403\n",
      "  Step 845/878, Loss: 0.0403\n",
      "  Step 850/878, Loss: 0.0403\n",
      "  Step 855/878, Loss: 0.0403\n",
      "  Step 860/878, Loss: 0.0403\n",
      "  Step 865/878, Loss: 0.0403\n",
      "  Step 870/878, Loss: 0.0403\n",
      "  Step 875/878, Loss: 0.0403\n",
      "  Epoch 8 Loss: 0.0403\n",
      "\n",
      "Epoch 9/20\n",
      "  Step 5/878, Loss: 0.0395\n",
      "  Step 10/878, Loss: 0.0395\n",
      "  Step 15/878, Loss: 0.0395\n",
      "  Step 20/878, Loss: 0.0395\n",
      "  Step 25/878, Loss: 0.0395\n",
      "  Step 30/878, Loss: 0.0395\n",
      "  Step 35/878, Loss: 0.0395\n",
      "  Step 40/878, Loss: 0.0395\n",
      "  Step 45/878, Loss: 0.0395\n",
      "  Step 50/878, Loss: 0.0395\n",
      "  Step 55/878, Loss: 0.0395\n",
      "  Step 60/878, Loss: 0.0394\n",
      "  Step 65/878, Loss: 0.0394\n",
      "  Step 70/878, Loss: 0.0394\n",
      "  Step 75/878, Loss: 0.0394\n",
      "  Step 80/878, Loss: 0.0394\n",
      "  Step 85/878, Loss: 0.0394\n",
      "  Step 90/878, Loss: 0.0394\n",
      "  Step 95/878, Loss: 0.0394\n",
      "  Step 100/878, Loss: 0.0394\n",
      "  Step 105/878, Loss: 0.0394\n",
      "  Step 110/878, Loss: 0.0394\n",
      "  Step 115/878, Loss: 0.0394\n",
      "  Step 120/878, Loss: 0.0394\n",
      "  Step 125/878, Loss: 0.0394\n",
      "  Step 130/878, Loss: 0.0394\n",
      "  Step 135/878, Loss: 0.0394\n",
      "  Step 140/878, Loss: 0.0394\n",
      "  Step 145/878, Loss: 0.0394\n",
      "  Step 150/878, Loss: 0.0394\n",
      "  Step 155/878, Loss: 0.0394\n",
      "  Step 160/878, Loss: 0.0394\n",
      "  Step 165/878, Loss: 0.0394\n",
      "  Step 170/878, Loss: 0.0393\n",
      "  Step 175/878, Loss: 0.0393\n",
      "  Step 180/878, Loss: 0.0393\n",
      "  Step 185/878, Loss: 0.0393\n",
      "  Step 190/878, Loss: 0.0393\n",
      "  Step 195/878, Loss: 0.0393\n",
      "  Step 200/878, Loss: 0.0393\n",
      "  Step 205/878, Loss: 0.0393\n",
      "  Step 210/878, Loss: 0.0393\n",
      "  Step 215/878, Loss: 0.0393\n",
      "  Step 220/878, Loss: 0.0393\n",
      "  Step 225/878, Loss: 0.0393\n",
      "  Step 230/878, Loss: 0.0393\n",
      "  Step 235/878, Loss: 0.0393\n",
      "  Step 240/878, Loss: 0.0393\n",
      "  Step 245/878, Loss: 0.0393\n",
      "  Step 250/878, Loss: 0.0393\n",
      "  Step 255/878, Loss: 0.0393\n",
      "  Step 260/878, Loss: 0.0393\n",
      "  Step 265/878, Loss: 0.0393\n",
      "  Step 270/878, Loss: 0.0393\n",
      "  Step 275/878, Loss: 0.0393\n",
      "  Step 280/878, Loss: 0.0393\n",
      "  Step 285/878, Loss: 0.0393\n",
      "  Step 290/878, Loss: 0.0393\n",
      "  Step 295/878, Loss: 0.0392\n",
      "  Step 300/878, Loss: 0.0392\n",
      "  Step 305/878, Loss: 0.0392\n",
      "  Step 310/878, Loss: 0.0392\n",
      "  Step 315/878, Loss: 0.0392\n",
      "  Step 320/878, Loss: 0.0392\n",
      "  Step 325/878, Loss: 0.0392\n",
      "  Step 330/878, Loss: 0.0392\n",
      "  Step 335/878, Loss: 0.0392\n",
      "  Step 340/878, Loss: 0.0392\n",
      "  Step 345/878, Loss: 0.0392\n",
      "  Step 350/878, Loss: 0.0392\n",
      "  Step 355/878, Loss: 0.0392\n",
      "  Step 360/878, Loss: 0.0392\n",
      "  Step 365/878, Loss: 0.0392\n",
      "  Step 370/878, Loss: 0.0392\n",
      "  Step 375/878, Loss: 0.0392\n",
      "  Step 380/878, Loss: 0.0392\n",
      "  Step 385/878, Loss: 0.0392\n",
      "  Step 390/878, Loss: 0.0392\n",
      "  Step 395/878, Loss: 0.0392\n",
      "  Step 400/878, Loss: 0.0392\n",
      "  Step 405/878, Loss: 0.0391\n",
      "  Step 410/878, Loss: 0.0391\n",
      "  Step 415/878, Loss: 0.0391\n",
      "  Step 420/878, Loss: 0.0391\n",
      "  Step 425/878, Loss: 0.0391\n",
      "  Step 430/878, Loss: 0.0391\n",
      "  Step 435/878, Loss: 0.0391\n",
      "  Step 440/878, Loss: 0.0391\n",
      "  Step 445/878, Loss: 0.0391\n",
      "  Step 450/878, Loss: 0.0391\n",
      "  Step 455/878, Loss: 0.0391\n",
      "  Step 460/878, Loss: 0.0391\n",
      "  Step 465/878, Loss: 0.0391\n",
      "  Step 470/878, Loss: 0.0391\n",
      "  Step 475/878, Loss: 0.0391\n",
      "  Step 480/878, Loss: 0.0391\n",
      "  Step 485/878, Loss: 0.0391\n",
      "  Step 490/878, Loss: 0.0391\n",
      "  Step 495/878, Loss: 0.0391\n",
      "  Step 500/878, Loss: 0.0391\n",
      "  Step 505/878, Loss: 0.0391\n",
      "  Step 510/878, Loss: 0.0391\n",
      "  Step 515/878, Loss: 0.0390\n",
      "  Step 520/878, Loss: 0.0390\n",
      "  Step 525/878, Loss: 0.0390\n",
      "  Step 530/878, Loss: 0.0390\n",
      "  Step 535/878, Loss: 0.0390\n",
      "  Step 540/878, Loss: 0.0390\n",
      "  Step 545/878, Loss: 0.0390\n",
      "  Step 550/878, Loss: 0.0390\n",
      "  Step 555/878, Loss: 0.0390\n",
      "  Step 560/878, Loss: 0.0390\n",
      "  Step 565/878, Loss: 0.0390\n",
      "  Step 570/878, Loss: 0.0390\n",
      "  Step 575/878, Loss: 0.0390\n",
      "  Step 580/878, Loss: 0.0390\n",
      "  Step 585/878, Loss: 0.0390\n",
      "  Step 590/878, Loss: 0.0390\n",
      "  Step 595/878, Loss: 0.0390\n",
      "  Step 600/878, Loss: 0.0390\n",
      "  Step 605/878, Loss: 0.0390\n",
      "  Step 610/878, Loss: 0.0390\n",
      "  Step 615/878, Loss: 0.0390\n",
      "  Step 620/878, Loss: 0.0390\n",
      "  Step 625/878, Loss: 0.0390\n",
      "  Step 630/878, Loss: 0.0390\n",
      "  Step 635/878, Loss: 0.0389\n",
      "  Step 640/878, Loss: 0.0389\n",
      "  Step 645/878, Loss: 0.0389\n",
      "  Step 650/878, Loss: 0.0389\n",
      "  Step 655/878, Loss: 0.0389\n",
      "  Step 660/878, Loss: 0.0389\n",
      "  Step 665/878, Loss: 0.0389\n",
      "  Step 670/878, Loss: 0.0389\n",
      "  Step 675/878, Loss: 0.0389\n",
      "  Step 680/878, Loss: 0.0389\n",
      "  Step 685/878, Loss: 0.0389\n",
      "  Step 690/878, Loss: 0.0389\n",
      "  Step 695/878, Loss: 0.0389\n",
      "  Step 700/878, Loss: 0.0389\n",
      "  Step 705/878, Loss: 0.0389\n",
      "  Step 710/878, Loss: 0.0389\n",
      "  Step 715/878, Loss: 0.0389\n",
      "  Step 720/878, Loss: 0.0389\n",
      "  Step 725/878, Loss: 0.0389\n",
      "  Step 730/878, Loss: 0.0389\n",
      "  Step 735/878, Loss: 0.0389\n",
      "  Step 740/878, Loss: 0.0389\n",
      "  Step 745/878, Loss: 0.0389\n",
      "  Step 750/878, Loss: 0.0389\n",
      "  Step 755/878, Loss: 0.0389\n",
      "  Step 760/878, Loss: 0.0388\n",
      "  Step 765/878, Loss: 0.0388\n",
      "  Step 770/878, Loss: 0.0388\n",
      "  Step 775/878, Loss: 0.0388\n",
      "  Step 780/878, Loss: 0.0388\n",
      "  Step 785/878, Loss: 0.0388\n",
      "  Step 790/878, Loss: 0.0388\n",
      "  Step 795/878, Loss: 0.0388\n",
      "  Step 800/878, Loss: 0.0388\n",
      "  Step 805/878, Loss: 0.0388\n",
      "  Step 810/878, Loss: 0.0388\n",
      "  Step 815/878, Loss: 0.0388\n",
      "  Step 820/878, Loss: 0.0388\n",
      "  Step 825/878, Loss: 0.0388\n",
      "  Step 830/878, Loss: 0.0388\n",
      "  Step 835/878, Loss: 0.0388\n",
      "  Step 840/878, Loss: 0.0388\n",
      "  Step 845/878, Loss: 0.0388\n",
      "  Step 850/878, Loss: 0.0388\n",
      "  Step 855/878, Loss: 0.0388\n",
      "  Step 860/878, Loss: 0.0388\n",
      "  Step 865/878, Loss: 0.0388\n",
      "  Step 870/878, Loss: 0.0388\n",
      "  Step 875/878, Loss: 0.0388\n",
      "  Epoch 9 Loss: 0.0388\n",
      "\n",
      "Epoch 10/20\n",
      "  Step 5/878, Loss: 0.0381\n",
      "  Step 10/878, Loss: 0.0381\n",
      "  Step 15/878, Loss: 0.0381\n",
      "  Step 20/878, Loss: 0.0381\n",
      "  Step 25/878, Loss: 0.0381\n",
      "  Step 30/878, Loss: 0.0381\n",
      "  Step 35/878, Loss: 0.0381\n",
      "  Step 40/878, Loss: 0.0381\n",
      "  Step 45/878, Loss: 0.0381\n",
      "  Step 50/878, Loss: 0.0380\n",
      "  Step 55/878, Loss: 0.0380\n",
      "  Step 60/878, Loss: 0.0380\n",
      "  Step 65/878, Loss: 0.0380\n",
      "  Step 70/878, Loss: 0.0380\n",
      "  Step 75/878, Loss: 0.0380\n",
      "  Step 80/878, Loss: 0.0380\n",
      "  Step 85/878, Loss: 0.0380\n",
      "  Step 90/878, Loss: 0.0380\n",
      "  Step 95/878, Loss: 0.0380\n",
      "  Step 100/878, Loss: 0.0380\n",
      "  Step 105/878, Loss: 0.0380\n",
      "  Step 110/878, Loss: 0.0380\n",
      "  Step 115/878, Loss: 0.0380\n",
      "  Step 120/878, Loss: 0.0380\n",
      "  Step 125/878, Loss: 0.0380\n",
      "  Step 130/878, Loss: 0.0380\n",
      "  Step 135/878, Loss: 0.0380\n",
      "  Step 140/878, Loss: 0.0380\n",
      "  Step 145/878, Loss: 0.0380\n",
      "  Step 150/878, Loss: 0.0380\n",
      "  Step 155/878, Loss: 0.0380\n",
      "  Step 160/878, Loss: 0.0380\n",
      "  Step 165/878, Loss: 0.0380\n",
      "  Step 170/878, Loss: 0.0380\n",
      "  Step 175/878, Loss: 0.0380\n",
      "  Step 180/878, Loss: 0.0380\n",
      "  Step 185/878, Loss: 0.0380\n",
      "  Step 190/878, Loss: 0.0379\n",
      "  Step 195/878, Loss: 0.0379\n",
      "  Step 200/878, Loss: 0.0379\n",
      "  Step 205/878, Loss: 0.0379\n",
      "  Step 210/878, Loss: 0.0379\n",
      "  Step 215/878, Loss: 0.0379\n",
      "  Step 220/878, Loss: 0.0379\n",
      "  Step 225/878, Loss: 0.0379\n",
      "  Step 230/878, Loss: 0.0379\n",
      "  Step 235/878, Loss: 0.0379\n",
      "  Step 240/878, Loss: 0.0379\n",
      "  Step 245/878, Loss: 0.0379\n",
      "  Step 250/878, Loss: 0.0379\n",
      "  Step 255/878, Loss: 0.0379\n",
      "  Step 260/878, Loss: 0.0379\n",
      "  Step 265/878, Loss: 0.0379\n",
      "  Step 270/878, Loss: 0.0379\n",
      "  Step 275/878, Loss: 0.0379\n",
      "  Step 280/878, Loss: 0.0379\n",
      "  Step 285/878, Loss: 0.0379\n",
      "  Step 290/878, Loss: 0.0379\n",
      "  Step 295/878, Loss: 0.0379\n",
      "  Step 300/878, Loss: 0.0379\n",
      "  Step 305/878, Loss: 0.0379\n",
      "  Step 310/878, Loss: 0.0379\n",
      "  Step 315/878, Loss: 0.0379\n",
      "  Step 320/878, Loss: 0.0379\n",
      "  Step 325/878, Loss: 0.0379\n",
      "  Step 330/878, Loss: 0.0378\n",
      "  Step 335/878, Loss: 0.0378\n",
      "  Step 340/878, Loss: 0.0378\n",
      "  Step 345/878, Loss: 0.0378\n",
      "  Step 350/878, Loss: 0.0378\n",
      "  Step 355/878, Loss: 0.0378\n",
      "  Step 360/878, Loss: 0.0378\n",
      "  Step 365/878, Loss: 0.0378\n",
      "  Step 370/878, Loss: 0.0378\n",
      "  Step 375/878, Loss: 0.0378\n",
      "  Step 380/878, Loss: 0.0378\n",
      "  Step 385/878, Loss: 0.0378\n",
      "  Step 390/878, Loss: 0.0378\n",
      "  Step 395/878, Loss: 0.0378\n",
      "  Step 400/878, Loss: 0.0378\n",
      "  Step 405/878, Loss: 0.0378\n",
      "  Step 410/878, Loss: 0.0378\n",
      "  Step 415/878, Loss: 0.0378\n",
      "  Step 420/878, Loss: 0.0378\n",
      "  Step 425/878, Loss: 0.0378\n",
      "  Step 430/878, Loss: 0.0378\n",
      "  Step 435/878, Loss: 0.0378\n",
      "  Step 440/878, Loss: 0.0378\n",
      "  Step 445/878, Loss: 0.0378\n",
      "  Step 450/878, Loss: 0.0378\n",
      "  Step 455/878, Loss: 0.0378\n",
      "  Step 460/878, Loss: 0.0378\n",
      "  Step 465/878, Loss: 0.0377\n",
      "  Step 470/878, Loss: 0.0377\n",
      "  Step 475/878, Loss: 0.0377\n",
      "  Step 480/878, Loss: 0.0377\n",
      "  Step 485/878, Loss: 0.0377\n",
      "  Step 490/878, Loss: 0.0377\n",
      "  Step 495/878, Loss: 0.0377\n",
      "  Step 500/878, Loss: 0.0377\n",
      "  Step 505/878, Loss: 0.0377\n",
      "  Step 510/878, Loss: 0.0377\n",
      "  Step 515/878, Loss: 0.0377\n",
      "  Step 520/878, Loss: 0.0377\n",
      "  Step 525/878, Loss: 0.0377\n",
      "  Step 530/878, Loss: 0.0377\n",
      "  Step 535/878, Loss: 0.0377\n",
      "  Step 540/878, Loss: 0.0377\n",
      "  Step 545/878, Loss: 0.0377\n",
      "  Step 550/878, Loss: 0.0377\n",
      "  Step 555/878, Loss: 0.0377\n",
      "  Step 560/878, Loss: 0.0377\n",
      "  Step 565/878, Loss: 0.0377\n",
      "  Step 570/878, Loss: 0.0377\n",
      "  Step 575/878, Loss: 0.0377\n",
      "  Step 580/878, Loss: 0.0377\n",
      "  Step 585/878, Loss: 0.0377\n",
      "  Step 590/878, Loss: 0.0377\n",
      "  Step 595/878, Loss: 0.0377\n",
      "  Step 600/878, Loss: 0.0377\n",
      "  Step 605/878, Loss: 0.0376\n",
      "  Step 610/878, Loss: 0.0376\n",
      "  Step 615/878, Loss: 0.0376\n",
      "  Step 620/878, Loss: 0.0376\n",
      "  Step 625/878, Loss: 0.0376\n",
      "  Step 630/878, Loss: 0.0376\n",
      "  Step 635/878, Loss: 0.0376\n",
      "  Step 640/878, Loss: 0.0376\n",
      "  Step 645/878, Loss: 0.0376\n",
      "  Step 650/878, Loss: 0.0376\n",
      "  Step 655/878, Loss: 0.0376\n",
      "  Step 660/878, Loss: 0.0376\n",
      "  Step 665/878, Loss: 0.0376\n",
      "  Step 670/878, Loss: 0.0376\n",
      "  Step 675/878, Loss: 0.0376\n",
      "  Step 680/878, Loss: 0.0376\n",
      "  Step 685/878, Loss: 0.0376\n",
      "  Step 690/878, Loss: 0.0376\n",
      "  Step 695/878, Loss: 0.0376\n",
      "  Step 700/878, Loss: 0.0376\n",
      "  Step 705/878, Loss: 0.0376\n",
      "  Step 710/878, Loss: 0.0376\n",
      "  Step 715/878, Loss: 0.0376\n",
      "  Step 720/878, Loss: 0.0376\n",
      "  Step 725/878, Loss: 0.0376\n",
      "  Step 730/878, Loss: 0.0376\n",
      "  Step 735/878, Loss: 0.0376\n",
      "  Step 740/878, Loss: 0.0375\n",
      "  Step 745/878, Loss: 0.0375\n",
      "  Step 750/878, Loss: 0.0375\n",
      "  Step 755/878, Loss: 0.0375\n",
      "  Step 760/878, Loss: 0.0375\n",
      "  Step 765/878, Loss: 0.0375\n",
      "  Step 770/878, Loss: 0.0375\n",
      "  Step 775/878, Loss: 0.0375\n",
      "  Step 780/878, Loss: 0.0375\n",
      "  Step 785/878, Loss: 0.0375\n",
      "  Step 790/878, Loss: 0.0375\n",
      "  Step 795/878, Loss: 0.0375\n",
      "  Step 800/878, Loss: 0.0375\n",
      "  Step 805/878, Loss: 0.0375\n",
      "  Step 810/878, Loss: 0.0375\n",
      "  Step 815/878, Loss: 0.0375\n",
      "  Step 820/878, Loss: 0.0375\n",
      "  Step 825/878, Loss: 0.0375\n",
      "  Step 830/878, Loss: 0.0375\n",
      "  Step 835/878, Loss: 0.0375\n",
      "  Step 840/878, Loss: 0.0375\n",
      "  Step 845/878, Loss: 0.0375\n",
      "  Step 850/878, Loss: 0.0375\n",
      "  Step 855/878, Loss: 0.0375\n",
      "  Step 860/878, Loss: 0.0375\n",
      "  Step 865/878, Loss: 0.0375\n",
      "  Step 870/878, Loss: 0.0375\n",
      "  Step 875/878, Loss: 0.0375\n",
      "  Epoch 10 Loss: 0.0374\n",
      "\n",
      "Epoch 11/20\n",
      "  Step 5/878, Loss: 0.0368\n",
      "  Step 10/878, Loss: 0.0368\n",
      "  Step 15/878, Loss: 0.0368\n",
      "  Step 20/878, Loss: 0.0368\n",
      "  Step 25/878, Loss: 0.0368\n",
      "  Step 30/878, Loss: 0.0368\n",
      "  Step 35/878, Loss: 0.0368\n",
      "  Step 40/878, Loss: 0.0368\n",
      "  Step 45/878, Loss: 0.0368\n",
      "  Step 50/878, Loss: 0.0368\n",
      "  Step 55/878, Loss: 0.0368\n",
      "  Step 60/878, Loss: 0.0368\n",
      "  Step 65/878, Loss: 0.0368\n",
      "  Step 70/878, Loss: 0.0368\n",
      "  Step 75/878, Loss: 0.0367\n",
      "  Step 80/878, Loss: 0.0367\n",
      "  Step 85/878, Loss: 0.0367\n",
      "  Step 90/878, Loss: 0.0367\n",
      "  Step 95/878, Loss: 0.0367\n",
      "  Step 100/878, Loss: 0.0367\n",
      "  Step 105/878, Loss: 0.0367\n",
      "  Step 110/878, Loss: 0.0367\n",
      "  Step 115/878, Loss: 0.0367\n",
      "  Step 120/878, Loss: 0.0367\n",
      "  Step 125/878, Loss: 0.0367\n",
      "  Step 130/878, Loss: 0.0367\n",
      "  Step 135/878, Loss: 0.0367\n",
      "  Step 140/878, Loss: 0.0367\n",
      "  Step 145/878, Loss: 0.0367\n",
      "  Step 150/878, Loss: 0.0367\n",
      "  Step 155/878, Loss: 0.0367\n",
      "  Step 160/878, Loss: 0.0367\n",
      "  Step 165/878, Loss: 0.0367\n",
      "  Step 170/878, Loss: 0.0367\n",
      "  Step 175/878, Loss: 0.0367\n",
      "  Step 180/878, Loss: 0.0367\n",
      "  Step 185/878, Loss: 0.0367\n",
      "  Step 190/878, Loss: 0.0367\n",
      "  Step 195/878, Loss: 0.0367\n",
      "  Step 200/878, Loss: 0.0367\n",
      "  Step 205/878, Loss: 0.0367\n",
      "  Step 210/878, Loss: 0.0367\n",
      "  Step 215/878, Loss: 0.0367\n",
      "  Step 220/878, Loss: 0.0366\n",
      "  Step 225/878, Loss: 0.0366\n",
      "  Step 230/878, Loss: 0.0366\n",
      "  Step 235/878, Loss: 0.0366\n",
      "  Step 240/878, Loss: 0.0366\n",
      "  Step 245/878, Loss: 0.0366\n",
      "  Step 250/878, Loss: 0.0366\n",
      "  Step 255/878, Loss: 0.0366\n",
      "  Step 260/878, Loss: 0.0366\n",
      "  Step 265/878, Loss: 0.0366\n",
      "  Step 270/878, Loss: 0.0366\n",
      "  Step 275/878, Loss: 0.0366\n",
      "  Step 280/878, Loss: 0.0366\n",
      "  Step 285/878, Loss: 0.0366\n",
      "  Step 290/878, Loss: 0.0366\n",
      "  Step 295/878, Loss: 0.0366\n",
      "  Step 300/878, Loss: 0.0366\n",
      "  Step 305/878, Loss: 0.0366\n",
      "  Step 310/878, Loss: 0.0366\n",
      "  Step 315/878, Loss: 0.0366\n",
      "  Step 320/878, Loss: 0.0366\n",
      "  Step 325/878, Loss: 0.0366\n",
      "  Step 330/878, Loss: 0.0366\n",
      "  Step 335/878, Loss: 0.0366\n",
      "  Step 340/878, Loss: 0.0366\n",
      "  Step 345/878, Loss: 0.0366\n",
      "  Step 350/878, Loss: 0.0366\n",
      "  Step 355/878, Loss: 0.0366\n",
      "  Step 360/878, Loss: 0.0366\n",
      "  Step 365/878, Loss: 0.0366\n",
      "  Step 370/878, Loss: 0.0366\n",
      "  Step 375/878, Loss: 0.0365\n",
      "  Step 380/878, Loss: 0.0365\n",
      "  Step 385/878, Loss: 0.0365\n",
      "  Step 390/878, Loss: 0.0365\n",
      "  Step 395/878, Loss: 0.0365\n",
      "  Step 400/878, Loss: 0.0365\n",
      "  Step 405/878, Loss: 0.0365\n",
      "  Step 410/878, Loss: 0.0365\n",
      "  Step 415/878, Loss: 0.0365\n",
      "  Step 420/878, Loss: 0.0365\n",
      "  Step 425/878, Loss: 0.0365\n",
      "  Step 430/878, Loss: 0.0365\n",
      "  Step 435/878, Loss: 0.0365\n",
      "  Step 440/878, Loss: 0.0365\n",
      "  Step 445/878, Loss: 0.0365\n",
      "  Step 450/878, Loss: 0.0365\n",
      "  Step 455/878, Loss: 0.0365\n",
      "  Step 460/878, Loss: 0.0365\n",
      "  Step 465/878, Loss: 0.0365\n",
      "  Step 470/878, Loss: 0.0365\n",
      "  Step 475/878, Loss: 0.0365\n",
      "  Step 480/878, Loss: 0.0365\n",
      "  Step 485/878, Loss: 0.0365\n",
      "  Step 490/878, Loss: 0.0365\n",
      "  Step 495/878, Loss: 0.0365\n",
      "  Step 500/878, Loss: 0.0365\n",
      "  Step 505/878, Loss: 0.0365\n",
      "  Step 510/878, Loss: 0.0365\n",
      "  Step 515/878, Loss: 0.0365\n",
      "  Step 520/878, Loss: 0.0365\n",
      "  Step 525/878, Loss: 0.0365\n",
      "  Step 530/878, Loss: 0.0364\n",
      "  Step 535/878, Loss: 0.0364\n",
      "  Step 540/878, Loss: 0.0364\n",
      "  Step 545/878, Loss: 0.0364\n",
      "  Step 550/878, Loss: 0.0364\n",
      "  Step 555/878, Loss: 0.0364\n",
      "  Step 560/878, Loss: 0.0364\n",
      "  Step 565/878, Loss: 0.0364\n",
      "  Step 570/878, Loss: 0.0364\n",
      "  Step 575/878, Loss: 0.0364\n",
      "  Step 580/878, Loss: 0.0364\n",
      "  Step 585/878, Loss: 0.0364\n",
      "  Step 590/878, Loss: 0.0364\n",
      "  Step 595/878, Loss: 0.0364\n",
      "  Step 600/878, Loss: 0.0364\n",
      "  Step 605/878, Loss: 0.0364\n",
      "  Step 610/878, Loss: 0.0364\n",
      "  Step 615/878, Loss: 0.0364\n",
      "  Step 620/878, Loss: 0.0364\n",
      "  Step 625/878, Loss: 0.0364\n",
      "  Step 630/878, Loss: 0.0364\n",
      "  Step 635/878, Loss: 0.0364\n",
      "  Step 640/878, Loss: 0.0364\n",
      "  Step 645/878, Loss: 0.0364\n",
      "  Step 650/878, Loss: 0.0364\n",
      "  Step 655/878, Loss: 0.0364\n",
      "  Step 660/878, Loss: 0.0364\n",
      "  Step 665/878, Loss: 0.0364\n",
      "  Step 670/878, Loss: 0.0364\n",
      "  Step 675/878, Loss: 0.0364\n",
      "  Step 680/878, Loss: 0.0363\n",
      "  Step 685/878, Loss: 0.0363\n",
      "  Step 690/878, Loss: 0.0363\n",
      "  Step 695/878, Loss: 0.0363\n",
      "  Step 700/878, Loss: 0.0363\n",
      "  Step 705/878, Loss: 0.0363\n",
      "  Step 710/878, Loss: 0.0363\n",
      "  Step 715/878, Loss: 0.0363\n",
      "  Step 720/878, Loss: 0.0363\n",
      "  Step 725/878, Loss: 0.0363\n",
      "  Step 730/878, Loss: 0.0363\n",
      "  Step 735/878, Loss: 0.0363\n",
      "  Step 740/878, Loss: 0.0363\n",
      "  Step 745/878, Loss: 0.0363\n",
      "  Step 750/878, Loss: 0.0363\n",
      "  Step 755/878, Loss: 0.0363\n",
      "  Step 760/878, Loss: 0.0363\n",
      "  Step 765/878, Loss: 0.0363\n",
      "  Step 770/878, Loss: 0.0363\n",
      "  Step 775/878, Loss: 0.0363\n",
      "  Step 780/878, Loss: 0.0363\n",
      "  Step 785/878, Loss: 0.0363\n",
      "  Step 790/878, Loss: 0.0363\n",
      "  Step 795/878, Loss: 0.0363\n",
      "  Step 800/878, Loss: 0.0363\n",
      "  Step 805/878, Loss: 0.0363\n",
      "  Step 810/878, Loss: 0.0363\n",
      "  Step 815/878, Loss: 0.0363\n",
      "  Step 820/878, Loss: 0.0363\n",
      "  Step 825/878, Loss: 0.0363\n",
      "  Step 830/878, Loss: 0.0362\n",
      "  Step 835/878, Loss: 0.0362\n",
      "  Step 840/878, Loss: 0.0362\n",
      "  Step 845/878, Loss: 0.0362\n",
      "  Step 850/878, Loss: 0.0362\n",
      "  Step 855/878, Loss: 0.0362\n",
      "  Step 860/878, Loss: 0.0362\n",
      "  Step 865/878, Loss: 0.0362\n",
      "  Step 870/878, Loss: 0.0362\n",
      "  Step 875/878, Loss: 0.0362\n",
      "  Epoch 11 Loss: 0.0362\n",
      "\n",
      "Epoch 12/20\n",
      "  Step 5/878, Loss: 0.0357\n",
      "  Step 10/878, Loss: 0.0356\n",
      "  Step 15/878, Loss: 0.0356\n",
      "  Step 20/878, Loss: 0.0356\n",
      "  Step 25/878, Loss: 0.0356\n",
      "  Step 30/878, Loss: 0.0356\n",
      "  Step 35/878, Loss: 0.0356\n",
      "  Step 40/878, Loss: 0.0356\n",
      "  Step 45/878, Loss: 0.0356\n",
      "  Step 50/878, Loss: 0.0356\n",
      "  Step 55/878, Loss: 0.0356\n",
      "  Step 60/878, Loss: 0.0356\n",
      "  Step 65/878, Loss: 0.0356\n",
      "  Step 70/878, Loss: 0.0356\n",
      "  Step 75/878, Loss: 0.0356\n",
      "  Step 80/878, Loss: 0.0356\n",
      "  Step 85/878, Loss: 0.0356\n",
      "  Step 90/878, Loss: 0.0356\n",
      "  Step 95/878, Loss: 0.0356\n",
      "  Step 100/878, Loss: 0.0356\n",
      "  Step 105/878, Loss: 0.0356\n",
      "  Step 110/878, Loss: 0.0356\n",
      "  Step 115/878, Loss: 0.0356\n",
      "  Step 120/878, Loss: 0.0356\n",
      "  Step 125/878, Loss: 0.0356\n",
      "  Step 130/878, Loss: 0.0356\n",
      "  Step 135/878, Loss: 0.0356\n",
      "  Step 140/878, Loss: 0.0356\n",
      "  Step 145/878, Loss: 0.0356\n",
      "  Step 150/878, Loss: 0.0356\n",
      "  Step 155/878, Loss: 0.0356\n",
      "  Step 160/878, Loss: 0.0356\n",
      "  Step 165/878, Loss: 0.0356\n",
      "  Step 170/878, Loss: 0.0355\n",
      "  Step 175/878, Loss: 0.0355\n",
      "  Step 180/878, Loss: 0.0355\n",
      "  Step 185/878, Loss: 0.0355\n",
      "  Step 190/878, Loss: 0.0355\n",
      "  Step 195/878, Loss: 0.0355\n",
      "  Step 200/878, Loss: 0.0355\n",
      "  Step 205/878, Loss: 0.0355\n",
      "  Step 210/878, Loss: 0.0355\n",
      "  Step 215/878, Loss: 0.0355\n",
      "  Step 220/878, Loss: 0.0355\n",
      "  Step 225/878, Loss: 0.0355\n",
      "  Step 230/878, Loss: 0.0355\n",
      "  Step 235/878, Loss: 0.0355\n",
      "  Step 240/878, Loss: 0.0355\n",
      "  Step 245/878, Loss: 0.0355\n",
      "  Step 250/878, Loss: 0.0355\n",
      "  Step 255/878, Loss: 0.0355\n",
      "  Step 260/878, Loss: 0.0355\n",
      "  Step 265/878, Loss: 0.0355\n",
      "  Step 270/878, Loss: 0.0355\n",
      "  Step 275/878, Loss: 0.0355\n",
      "  Step 280/878, Loss: 0.0355\n",
      "  Step 285/878, Loss: 0.0355\n",
      "  Step 290/878, Loss: 0.0355\n",
      "  Step 295/878, Loss: 0.0355\n",
      "  Step 300/878, Loss: 0.0355\n",
      "  Step 305/878, Loss: 0.0355\n",
      "  Step 310/878, Loss: 0.0355\n",
      "  Step 315/878, Loss: 0.0355\n",
      "  Step 320/878, Loss: 0.0355\n",
      "  Step 325/878, Loss: 0.0355\n",
      "  Step 330/878, Loss: 0.0354\n",
      "  Step 335/878, Loss: 0.0354\n",
      "  Step 340/878, Loss: 0.0354\n",
      "  Step 345/878, Loss: 0.0354\n",
      "  Step 350/878, Loss: 0.0354\n",
      "  Step 355/878, Loss: 0.0354\n",
      "  Step 360/878, Loss: 0.0354\n",
      "  Step 365/878, Loss: 0.0354\n",
      "  Step 370/878, Loss: 0.0354\n",
      "  Step 375/878, Loss: 0.0354\n",
      "  Step 380/878, Loss: 0.0354\n",
      "  Step 385/878, Loss: 0.0354\n",
      "  Step 390/878, Loss: 0.0354\n",
      "  Step 395/878, Loss: 0.0354\n",
      "  Step 400/878, Loss: 0.0354\n",
      "  Step 405/878, Loss: 0.0354\n",
      "  Step 410/878, Loss: 0.0354\n",
      "  Step 415/878, Loss: 0.0354\n",
      "  Step 420/878, Loss: 0.0354\n",
      "  Step 425/878, Loss: 0.0354\n",
      "  Step 430/878, Loss: 0.0354\n",
      "  Step 435/878, Loss: 0.0354\n",
      "  Step 440/878, Loss: 0.0354\n",
      "  Step 445/878, Loss: 0.0354\n",
      "  Step 450/878, Loss: 0.0354\n",
      "  Step 455/878, Loss: 0.0354\n",
      "  Step 460/878, Loss: 0.0354\n",
      "  Step 465/878, Loss: 0.0354\n",
      "  Step 470/878, Loss: 0.0354\n",
      "  Step 475/878, Loss: 0.0354\n",
      "  Step 480/878, Loss: 0.0354\n",
      "  Step 485/878, Loss: 0.0353\n",
      "  Step 490/878, Loss: 0.0353\n",
      "  Step 495/878, Loss: 0.0353\n",
      "  Step 500/878, Loss: 0.0353\n",
      "  Step 505/878, Loss: 0.0353\n",
      "  Step 510/878, Loss: 0.0353\n",
      "  Step 515/878, Loss: 0.0353\n",
      "  Step 520/878, Loss: 0.0353\n",
      "  Step 525/878, Loss: 0.0353\n",
      "  Step 530/878, Loss: 0.0353\n",
      "  Step 535/878, Loss: 0.0353\n",
      "  Step 540/878, Loss: 0.0353\n",
      "  Step 545/878, Loss: 0.0353\n",
      "  Step 550/878, Loss: 0.0353\n",
      "  Step 555/878, Loss: 0.0353\n",
      "  Step 560/878, Loss: 0.0353\n",
      "  Step 565/878, Loss: 0.0353\n",
      "  Step 570/878, Loss: 0.0353\n",
      "  Step 575/878, Loss: 0.0353\n",
      "  Step 580/878, Loss: 0.0353\n",
      "  Step 585/878, Loss: 0.0353\n",
      "  Step 590/878, Loss: 0.0353\n",
      "  Step 595/878, Loss: 0.0353\n",
      "  Step 600/878, Loss: 0.0353\n",
      "  Step 605/878, Loss: 0.0353\n",
      "  Step 610/878, Loss: 0.0353\n",
      "  Step 615/878, Loss: 0.0353\n",
      "  Step 620/878, Loss: 0.0353\n",
      "  Step 625/878, Loss: 0.0353\n",
      "  Step 630/878, Loss: 0.0353\n",
      "  Step 635/878, Loss: 0.0353\n",
      "  Step 640/878, Loss: 0.0353\n",
      "  Step 645/878, Loss: 0.0352\n",
      "  Step 650/878, Loss: 0.0352\n",
      "  Step 655/878, Loss: 0.0352\n",
      "  Step 660/878, Loss: 0.0352\n",
      "  Step 665/878, Loss: 0.0352\n",
      "  Step 670/878, Loss: 0.0352\n",
      "  Step 675/878, Loss: 0.0352\n",
      "  Step 680/878, Loss: 0.0352\n",
      "  Step 685/878, Loss: 0.0352\n",
      "  Step 690/878, Loss: 0.0352\n",
      "  Step 695/878, Loss: 0.0352\n",
      "  Step 700/878, Loss: 0.0352\n",
      "  Step 705/878, Loss: 0.0352\n",
      "  Step 710/878, Loss: 0.0352\n",
      "  Step 715/878, Loss: 0.0352\n",
      "  Step 720/878, Loss: 0.0352\n",
      "  Step 725/878, Loss: 0.0352\n",
      "  Step 730/878, Loss: 0.0352\n",
      "  Step 735/878, Loss: 0.0352\n",
      "  Step 740/878, Loss: 0.0352\n",
      "  Step 745/878, Loss: 0.0352\n",
      "  Step 750/878, Loss: 0.0352\n",
      "  Step 755/878, Loss: 0.0352\n",
      "  Step 760/878, Loss: 0.0352\n",
      "  Step 765/878, Loss: 0.0352\n",
      "  Step 770/878, Loss: 0.0352\n",
      "  Step 775/878, Loss: 0.0352\n",
      "  Step 780/878, Loss: 0.0352\n",
      "  Step 785/878, Loss: 0.0352\n",
      "  Step 790/878, Loss: 0.0352\n",
      "  Step 795/878, Loss: 0.0352\n",
      "  Step 800/878, Loss: 0.0352\n",
      "  Step 805/878, Loss: 0.0352\n",
      "  Step 810/878, Loss: 0.0352\n",
      "  Step 815/878, Loss: 0.0351\n",
      "  Step 820/878, Loss: 0.0351\n",
      "  Step 825/878, Loss: 0.0351\n",
      "  Step 830/878, Loss: 0.0351\n",
      "  Step 835/878, Loss: 0.0351\n",
      "  Step 840/878, Loss: 0.0351\n",
      "  Step 845/878, Loss: 0.0351\n",
      "  Step 850/878, Loss: 0.0351\n",
      "  Step 855/878, Loss: 0.0351\n",
      "  Step 860/878, Loss: 0.0351\n",
      "  Step 865/878, Loss: 0.0351\n",
      "  Step 870/878, Loss: 0.0351\n",
      "  Step 875/878, Loss: 0.0351\n",
      "  Epoch 12 Loss: 0.0351\n",
      "\n",
      "Epoch 13/20\n",
      "  Step 5/878, Loss: 0.0346\n",
      "  Step 10/878, Loss: 0.0346\n",
      "  Step 15/878, Loss: 0.0346\n",
      "  Step 20/878, Loss: 0.0346\n",
      "  Step 25/878, Loss: 0.0346\n",
      "  Step 30/878, Loss: 0.0346\n",
      "  Step 35/878, Loss: 0.0346\n",
      "  Step 40/878, Loss: 0.0346\n",
      "  Step 45/878, Loss: 0.0346\n",
      "  Step 50/878, Loss: 0.0346\n",
      "  Step 55/878, Loss: 0.0346\n",
      "  Step 60/878, Loss: 0.0346\n",
      "  Step 65/878, Loss: 0.0346\n",
      "  Step 70/878, Loss: 0.0346\n",
      "  Step 75/878, Loss: 0.0346\n",
      "  Step 80/878, Loss: 0.0346\n",
      "  Step 85/878, Loss: 0.0345\n",
      "  Step 90/878, Loss: 0.0345\n",
      "  Step 95/878, Loss: 0.0345\n",
      "  Step 100/878, Loss: 0.0345\n",
      "  Step 105/878, Loss: 0.0345\n",
      "  Step 110/878, Loss: 0.0345\n",
      "  Step 115/878, Loss: 0.0345\n",
      "  Step 120/878, Loss: 0.0345\n",
      "  Step 125/878, Loss: 0.0345\n",
      "  Step 130/878, Loss: 0.0345\n",
      "  Step 135/878, Loss: 0.0345\n",
      "  Step 140/878, Loss: 0.0345\n",
      "  Step 145/878, Loss: 0.0345\n",
      "  Step 150/878, Loss: 0.0345\n",
      "  Step 155/878, Loss: 0.0345\n",
      "  Step 160/878, Loss: 0.0345\n",
      "  Step 165/878, Loss: 0.0345\n",
      "  Step 170/878, Loss: 0.0345\n",
      "  Step 175/878, Loss: 0.0345\n",
      "  Step 180/878, Loss: 0.0345\n",
      "  Step 185/878, Loss: 0.0345\n",
      "  Step 190/878, Loss: 0.0345\n",
      "  Step 195/878, Loss: 0.0345\n",
      "  Step 200/878, Loss: 0.0345\n",
      "  Step 205/878, Loss: 0.0345\n",
      "  Step 210/878, Loss: 0.0345\n",
      "  Step 215/878, Loss: 0.0345\n",
      "  Step 220/878, Loss: 0.0345\n",
      "  Step 225/878, Loss: 0.0345\n",
      "  Step 230/878, Loss: 0.0345\n",
      "  Step 235/878, Loss: 0.0345\n",
      "  Step 240/878, Loss: 0.0344\n",
      "  Step 245/878, Loss: 0.0344\n",
      "  Step 250/878, Loss: 0.0344\n",
      "  Step 255/878, Loss: 0.0344\n",
      "  Step 260/878, Loss: 0.0344\n",
      "  Step 265/878, Loss: 0.0344\n",
      "  Step 270/878, Loss: 0.0344\n",
      "  Step 275/878, Loss: 0.0344\n",
      "  Step 280/878, Loss: 0.0344\n",
      "  Step 285/878, Loss: 0.0344\n",
      "  Step 290/878, Loss: 0.0344\n",
      "  Step 295/878, Loss: 0.0344\n",
      "  Step 300/878, Loss: 0.0344\n",
      "  Step 305/878, Loss: 0.0344\n",
      "  Step 310/878, Loss: 0.0344\n",
      "  Step 315/878, Loss: 0.0344\n",
      "  Step 320/878, Loss: 0.0344\n",
      "  Step 325/878, Loss: 0.0344\n",
      "  Step 330/878, Loss: 0.0344\n",
      "  Step 335/878, Loss: 0.0344\n",
      "  Step 340/878, Loss: 0.0344\n",
      "  Step 345/878, Loss: 0.0344\n",
      "  Step 350/878, Loss: 0.0344\n",
      "  Step 355/878, Loss: 0.0344\n",
      "  Step 360/878, Loss: 0.0344\n",
      "  Step 365/878, Loss: 0.0344\n",
      "  Step 370/878, Loss: 0.0344\n",
      "  Step 375/878, Loss: 0.0344\n",
      "  Step 380/878, Loss: 0.0344\n",
      "  Step 385/878, Loss: 0.0344\n",
      "  Step 390/878, Loss: 0.0344\n",
      "  Step 395/878, Loss: 0.0344\n",
      "  Step 400/878, Loss: 0.0344\n",
      "  Step 405/878, Loss: 0.0344\n",
      "  Step 410/878, Loss: 0.0344\n",
      "  Step 415/878, Loss: 0.0343\n",
      "  Step 420/878, Loss: 0.0343\n",
      "  Step 425/878, Loss: 0.0343\n",
      "  Step 430/878, Loss: 0.0343\n",
      "  Step 435/878, Loss: 0.0343\n",
      "  Step 440/878, Loss: 0.0343\n",
      "  Step 445/878, Loss: 0.0343\n",
      "  Step 450/878, Loss: 0.0343\n",
      "  Step 455/878, Loss: 0.0343\n",
      "  Step 460/878, Loss: 0.0343\n",
      "  Step 465/878, Loss: 0.0343\n",
      "  Step 470/878, Loss: 0.0343\n",
      "  Step 475/878, Loss: 0.0343\n",
      "  Step 480/878, Loss: 0.0343\n",
      "  Step 485/878, Loss: 0.0343\n",
      "  Step 490/878, Loss: 0.0343\n",
      "  Step 495/878, Loss: 0.0343\n",
      "  Step 500/878, Loss: 0.0343\n",
      "  Step 505/878, Loss: 0.0343\n",
      "  Step 510/878, Loss: 0.0343\n",
      "  Step 515/878, Loss: 0.0343\n",
      "  Step 520/878, Loss: 0.0343\n",
      "  Step 525/878, Loss: 0.0343\n",
      "  Step 530/878, Loss: 0.0343\n",
      "  Step 535/878, Loss: 0.0343\n",
      "  Step 540/878, Loss: 0.0343\n",
      "  Step 545/878, Loss: 0.0343\n",
      "  Step 550/878, Loss: 0.0343\n",
      "  Step 555/878, Loss: 0.0343\n",
      "  Step 560/878, Loss: 0.0343\n",
      "  Step 565/878, Loss: 0.0343\n",
      "  Step 570/878, Loss: 0.0343\n",
      "  Step 575/878, Loss: 0.0343\n",
      "  Step 580/878, Loss: 0.0343\n",
      "  Step 585/878, Loss: 0.0342\n",
      "  Step 590/878, Loss: 0.0342\n",
      "  Step 595/878, Loss: 0.0342\n",
      "  Step 600/878, Loss: 0.0342\n",
      "  Step 605/878, Loss: 0.0342\n",
      "  Step 610/878, Loss: 0.0342\n",
      "  Step 615/878, Loss: 0.0342\n",
      "  Step 620/878, Loss: 0.0342\n",
      "  Step 625/878, Loss: 0.0342\n",
      "  Step 630/878, Loss: 0.0342\n",
      "  Step 635/878, Loss: 0.0342\n",
      "  Step 640/878, Loss: 0.0342\n",
      "  Step 645/878, Loss: 0.0342\n",
      "  Step 650/878, Loss: 0.0342\n",
      "  Step 655/878, Loss: 0.0342\n",
      "  Step 660/878, Loss: 0.0342\n",
      "  Step 665/878, Loss: 0.0342\n",
      "  Step 670/878, Loss: 0.0342\n",
      "  Step 675/878, Loss: 0.0342\n",
      "  Step 680/878, Loss: 0.0342\n",
      "  Step 685/878, Loss: 0.0342\n",
      "  Step 690/878, Loss: 0.0342\n",
      "  Step 695/878, Loss: 0.0342\n",
      "  Step 700/878, Loss: 0.0342\n",
      "  Step 705/878, Loss: 0.0342\n",
      "  Step 710/878, Loss: 0.0342\n",
      "  Step 715/878, Loss: 0.0342\n",
      "  Step 720/878, Loss: 0.0342\n",
      "  Step 725/878, Loss: 0.0342\n",
      "  Step 730/878, Loss: 0.0342\n",
      "  Step 735/878, Loss: 0.0342\n",
      "  Step 740/878, Loss: 0.0342\n",
      "  Step 745/878, Loss: 0.0342\n",
      "  Step 750/878, Loss: 0.0342\n",
      "  Step 755/878, Loss: 0.0342\n",
      "  Step 760/878, Loss: 0.0341\n",
      "  Step 765/878, Loss: 0.0341\n",
      "  Step 770/878, Loss: 0.0341\n",
      "  Step 775/878, Loss: 0.0341\n",
      "  Step 780/878, Loss: 0.0341\n",
      "  Step 785/878, Loss: 0.0341\n",
      "  Step 790/878, Loss: 0.0341\n",
      "  Step 795/878, Loss: 0.0341\n",
      "  Step 800/878, Loss: 0.0341\n",
      "  Step 805/878, Loss: 0.0341\n",
      "  Step 810/878, Loss: 0.0341\n",
      "  Step 815/878, Loss: 0.0341\n",
      "  Step 820/878, Loss: 0.0341\n",
      "  Step 825/878, Loss: 0.0341\n",
      "  Step 830/878, Loss: 0.0341\n",
      "  Step 835/878, Loss: 0.0341\n",
      "  Step 840/878, Loss: 0.0341\n",
      "  Step 845/878, Loss: 0.0341\n",
      "  Step 850/878, Loss: 0.0341\n",
      "  Step 855/878, Loss: 0.0341\n",
      "  Step 860/878, Loss: 0.0341\n",
      "  Step 865/878, Loss: 0.0341\n",
      "  Step 870/878, Loss: 0.0341\n",
      "  Step 875/878, Loss: 0.0341\n",
      "  Epoch 13 Loss: 0.0341\n",
      "\n",
      "Epoch 14/20\n",
      "  Step 5/878, Loss: 0.0336\n",
      "  Step 10/878, Loss: 0.0336\n",
      "  Step 15/878, Loss: 0.0336\n",
      "  Step 20/878, Loss: 0.0336\n",
      "  Step 25/878, Loss: 0.0336\n",
      "  Step 30/878, Loss: 0.0336\n",
      "  Step 35/878, Loss: 0.0336\n",
      "  Step 40/878, Loss: 0.0336\n",
      "  Step 45/878, Loss: 0.0336\n",
      "  Step 50/878, Loss: 0.0336\n",
      "  Step 55/878, Loss: 0.0336\n",
      "  Step 60/878, Loss: 0.0336\n",
      "  Step 65/878, Loss: 0.0336\n",
      "  Step 70/878, Loss: 0.0336\n",
      "  Step 75/878, Loss: 0.0336\n",
      "  Step 80/878, Loss: 0.0335\n",
      "  Step 85/878, Loss: 0.0335\n",
      "  Step 90/878, Loss: 0.0335\n",
      "  Step 95/878, Loss: 0.0335\n",
      "  Step 100/878, Loss: 0.0335\n",
      "  Step 105/878, Loss: 0.0335\n",
      "  Step 110/878, Loss: 0.0335\n",
      "  Step 115/878, Loss: 0.0335\n",
      "  Step 120/878, Loss: 0.0335\n",
      "  Step 125/878, Loss: 0.0335\n",
      "  Step 130/878, Loss: 0.0335\n",
      "  Step 135/878, Loss: 0.0335\n",
      "  Step 140/878, Loss: 0.0335\n",
      "  Step 145/878, Loss: 0.0335\n",
      "  Step 150/878, Loss: 0.0335\n",
      "  Step 155/878, Loss: 0.0335\n",
      "  Step 160/878, Loss: 0.0335\n",
      "  Step 165/878, Loss: 0.0335\n",
      "  Step 170/878, Loss: 0.0335\n",
      "  Step 175/878, Loss: 0.0335\n",
      "  Step 180/878, Loss: 0.0335\n",
      "  Step 185/878, Loss: 0.0335\n",
      "  Step 190/878, Loss: 0.0335\n",
      "  Step 195/878, Loss: 0.0335\n",
      "  Step 200/878, Loss: 0.0335\n",
      "  Step 205/878, Loss: 0.0335\n",
      "  Step 210/878, Loss: 0.0335\n",
      "  Step 215/878, Loss: 0.0335\n",
      "  Step 220/878, Loss: 0.0335\n",
      "  Step 225/878, Loss: 0.0335\n",
      "  Step 230/878, Loss: 0.0335\n",
      "  Step 235/878, Loss: 0.0335\n",
      "  Step 240/878, Loss: 0.0335\n",
      "  Step 245/878, Loss: 0.0335\n",
      "  Step 250/878, Loss: 0.0335\n",
      "  Step 255/878, Loss: 0.0335\n",
      "  Step 260/878, Loss: 0.0334\n",
      "  Step 265/878, Loss: 0.0334\n",
      "  Step 270/878, Loss: 0.0334\n",
      "  Step 275/878, Loss: 0.0334\n",
      "  Step 280/878, Loss: 0.0334\n",
      "  Step 285/878, Loss: 0.0334\n",
      "  Step 290/878, Loss: 0.0334\n",
      "  Step 295/878, Loss: 0.0334\n",
      "  Step 300/878, Loss: 0.0334\n",
      "  Step 305/878, Loss: 0.0334\n",
      "  Step 310/878, Loss: 0.0334\n",
      "  Step 315/878, Loss: 0.0334\n",
      "  Step 320/878, Loss: 0.0334\n",
      "  Step 325/878, Loss: 0.0334\n",
      "  Step 330/878, Loss: 0.0334\n",
      "  Step 335/878, Loss: 0.0334\n",
      "  Step 340/878, Loss: 0.0334\n",
      "  Step 345/878, Loss: 0.0334\n",
      "  Step 350/878, Loss: 0.0334\n",
      "  Step 355/878, Loss: 0.0334\n",
      "  Step 360/878, Loss: 0.0334\n",
      "  Step 365/878, Loss: 0.0334\n",
      "  Step 370/878, Loss: 0.0334\n",
      "  Step 375/878, Loss: 0.0334\n",
      "  Step 380/878, Loss: 0.0334\n",
      "  Step 385/878, Loss: 0.0334\n",
      "  Step 390/878, Loss: 0.0334\n",
      "  Step 395/878, Loss: 0.0334\n",
      "  Step 400/878, Loss: 0.0334\n",
      "  Step 405/878, Loss: 0.0334\n",
      "  Step 410/878, Loss: 0.0334\n",
      "  Step 415/878, Loss: 0.0334\n",
      "  Step 420/878, Loss: 0.0334\n",
      "  Step 425/878, Loss: 0.0334\n",
      "  Step 430/878, Loss: 0.0334\n",
      "  Step 435/878, Loss: 0.0334\n",
      "  Step 440/878, Loss: 0.0334\n",
      "  Step 445/878, Loss: 0.0333\n",
      "  Step 450/878, Loss: 0.0333\n",
      "  Step 455/878, Loss: 0.0333\n",
      "  Step 460/878, Loss: 0.0333\n",
      "  Step 465/878, Loss: 0.0333\n",
      "  Step 470/878, Loss: 0.0333\n",
      "  Step 475/878, Loss: 0.0333\n",
      "  Step 480/878, Loss: 0.0333\n",
      "  Step 485/878, Loss: 0.0333\n",
      "  Step 490/878, Loss: 0.0333\n",
      "  Step 495/878, Loss: 0.0333\n",
      "  Step 500/878, Loss: 0.0333\n",
      "  Step 505/878, Loss: 0.0333\n",
      "  Step 510/878, Loss: 0.0333\n",
      "  Step 515/878, Loss: 0.0333\n",
      "  Step 520/878, Loss: 0.0333\n",
      "  Step 525/878, Loss: 0.0333\n",
      "  Step 530/878, Loss: 0.0333\n",
      "  Step 535/878, Loss: 0.0333\n",
      "  Step 540/878, Loss: 0.0333\n",
      "  Step 545/878, Loss: 0.0333\n",
      "  Step 550/878, Loss: 0.0333\n",
      "  Step 555/878, Loss: 0.0333\n",
      "  Step 560/878, Loss: 0.0333\n",
      "  Step 565/878, Loss: 0.0333\n",
      "  Step 570/878, Loss: 0.0333\n",
      "  Step 575/878, Loss: 0.0333\n",
      "  Step 580/878, Loss: 0.0333\n",
      "  Step 585/878, Loss: 0.0333\n",
      "  Step 590/878, Loss: 0.0333\n",
      "  Step 595/878, Loss: 0.0333\n",
      "  Step 600/878, Loss: 0.0333\n",
      "  Step 605/878, Loss: 0.0333\n",
      "  Step 610/878, Loss: 0.0333\n",
      "  Step 615/878, Loss: 0.0333\n",
      "  Step 620/878, Loss: 0.0333\n",
      "  Step 625/878, Loss: 0.0333\n",
      "  Step 630/878, Loss: 0.0332\n",
      "  Step 635/878, Loss: 0.0332\n",
      "  Step 640/878, Loss: 0.0332\n",
      "  Step 645/878, Loss: 0.0332\n",
      "  Step 650/878, Loss: 0.0332\n",
      "  Step 655/878, Loss: 0.0332\n",
      "  Step 660/878, Loss: 0.0332\n",
      "  Step 665/878, Loss: 0.0332\n",
      "  Step 670/878, Loss: 0.0332\n",
      "  Step 675/878, Loss: 0.0332\n",
      "  Step 680/878, Loss: 0.0332\n",
      "  Step 685/878, Loss: 0.0332\n",
      "  Step 690/878, Loss: 0.0332\n",
      "  Step 695/878, Loss: 0.0332\n",
      "  Step 700/878, Loss: 0.0332\n",
      "  Step 705/878, Loss: 0.0332\n",
      "  Step 710/878, Loss: 0.0332\n",
      "  Step 715/878, Loss: 0.0332\n",
      "  Step 720/878, Loss: 0.0332\n",
      "  Step 725/878, Loss: 0.0332\n",
      "  Step 730/878, Loss: 0.0332\n",
      "  Step 735/878, Loss: 0.0332\n",
      "  Step 740/878, Loss: 0.0332\n",
      "  Step 745/878, Loss: 0.0332\n",
      "  Step 750/878, Loss: 0.0332\n",
      "  Step 755/878, Loss: 0.0332\n",
      "  Step 760/878, Loss: 0.0332\n",
      "  Step 765/878, Loss: 0.0332\n",
      "  Step 770/878, Loss: 0.0332\n",
      "  Step 775/878, Loss: 0.0332\n",
      "  Step 780/878, Loss: 0.0332\n",
      "  Step 785/878, Loss: 0.0332\n",
      "  Step 790/878, Loss: 0.0332\n",
      "  Step 795/878, Loss: 0.0332\n",
      "  Step 800/878, Loss: 0.0332\n",
      "  Step 805/878, Loss: 0.0332\n",
      "  Step 810/878, Loss: 0.0332\n",
      "  Step 815/878, Loss: 0.0332\n",
      "  Step 820/878, Loss: 0.0332\n",
      "  Step 825/878, Loss: 0.0331\n",
      "  Step 830/878, Loss: 0.0331\n",
      "  Step 835/878, Loss: 0.0331\n",
      "  Step 840/878, Loss: 0.0331\n",
      "  Step 845/878, Loss: 0.0331\n",
      "  Step 850/878, Loss: 0.0331\n",
      "  Step 855/878, Loss: 0.0331\n",
      "  Step 860/878, Loss: 0.0331\n",
      "  Step 865/878, Loss: 0.0331\n",
      "  Step 870/878, Loss: 0.0331\n",
      "  Step 875/878, Loss: 0.0331\n",
      "  Epoch 14 Loss: 0.0331\n",
      "\n",
      "Epoch 15/20\n",
      "  Step 5/878, Loss: 0.0327\n",
      "  Step 10/878, Loss: 0.0327\n",
      "  Step 15/878, Loss: 0.0327\n",
      "  Step 20/878, Loss: 0.0327\n",
      "  Step 25/878, Loss: 0.0327\n",
      "  Step 30/878, Loss: 0.0327\n",
      "  Step 35/878, Loss: 0.0327\n",
      "  Step 40/878, Loss: 0.0327\n",
      "  Step 45/878, Loss: 0.0327\n",
      "  Step 50/878, Loss: 0.0327\n",
      "  Step 55/878, Loss: 0.0326\n",
      "  Step 60/878, Loss: 0.0326\n",
      "  Step 65/878, Loss: 0.0326\n",
      "  Step 70/878, Loss: 0.0326\n",
      "  Step 75/878, Loss: 0.0326\n",
      "  Step 80/878, Loss: 0.0326\n",
      "  Step 85/878, Loss: 0.0326\n",
      "  Step 90/878, Loss: 0.0326\n",
      "  Step 95/878, Loss: 0.0326\n",
      "  Step 100/878, Loss: 0.0326\n",
      "  Step 105/878, Loss: 0.0326\n",
      "  Step 110/878, Loss: 0.0326\n",
      "  Step 115/878, Loss: 0.0326\n",
      "  Step 120/878, Loss: 0.0326\n",
      "  Step 125/878, Loss: 0.0326\n",
      "  Step 130/878, Loss: 0.0326\n",
      "  Step 135/878, Loss: 0.0326\n",
      "  Step 140/878, Loss: 0.0326\n",
      "  Step 145/878, Loss: 0.0326\n",
      "  Step 150/878, Loss: 0.0326\n",
      "  Step 155/878, Loss: 0.0326\n",
      "  Step 160/878, Loss: 0.0326\n",
      "  Step 165/878, Loss: 0.0326\n",
      "  Step 170/878, Loss: 0.0326\n",
      "  Step 175/878, Loss: 0.0326\n",
      "  Step 180/878, Loss: 0.0326\n",
      "  Step 185/878, Loss: 0.0326\n",
      "  Step 190/878, Loss: 0.0326\n",
      "  Step 195/878, Loss: 0.0326\n",
      "  Step 200/878, Loss: 0.0326\n",
      "  Step 205/878, Loss: 0.0326\n",
      "  Step 210/878, Loss: 0.0326\n",
      "  Step 215/878, Loss: 0.0326\n",
      "  Step 220/878, Loss: 0.0326\n",
      "  Step 225/878, Loss: 0.0326\n",
      "  Step 230/878, Loss: 0.0326\n",
      "  Step 235/878, Loss: 0.0326\n",
      "  Step 240/878, Loss: 0.0325\n",
      "  Step 245/878, Loss: 0.0325\n",
      "  Step 250/878, Loss: 0.0325\n",
      "  Step 255/878, Loss: 0.0325\n",
      "  Step 260/878, Loss: 0.0325\n",
      "  Step 265/878, Loss: 0.0325\n",
      "  Step 270/878, Loss: 0.0325\n",
      "  Step 275/878, Loss: 0.0325\n",
      "  Step 280/878, Loss: 0.0325\n",
      "  Step 285/878, Loss: 0.0325\n",
      "  Step 290/878, Loss: 0.0325\n",
      "  Step 295/878, Loss: 0.0325\n",
      "  Step 300/878, Loss: 0.0325\n",
      "  Step 305/878, Loss: 0.0325\n",
      "  Step 310/878, Loss: 0.0325\n",
      "  Step 315/878, Loss: 0.0325\n",
      "  Step 320/878, Loss: 0.0325\n",
      "  Step 325/878, Loss: 0.0325\n",
      "  Step 330/878, Loss: 0.0325\n",
      "  Step 335/878, Loss: 0.0325\n",
      "  Step 340/878, Loss: 0.0325\n",
      "  Step 345/878, Loss: 0.0325\n",
      "  Step 350/878, Loss: 0.0325\n",
      "  Step 355/878, Loss: 0.0325\n",
      "  Step 360/878, Loss: 0.0325\n",
      "  Step 365/878, Loss: 0.0325\n",
      "  Step 370/878, Loss: 0.0325\n",
      "  Step 375/878, Loss: 0.0325\n",
      "  Step 380/878, Loss: 0.0325\n",
      "  Step 385/878, Loss: 0.0325\n",
      "  Step 390/878, Loss: 0.0325\n",
      "  Step 395/878, Loss: 0.0325\n",
      "  Step 400/878, Loss: 0.0325\n",
      "  Step 405/878, Loss: 0.0325\n",
      "  Step 410/878, Loss: 0.0325\n",
      "  Step 415/878, Loss: 0.0325\n",
      "  Step 420/878, Loss: 0.0325\n",
      "  Step 425/878, Loss: 0.0325\n",
      "  Step 430/878, Loss: 0.0325\n",
      "  Step 435/878, Loss: 0.0325\n",
      "  Step 440/878, Loss: 0.0325\n",
      "  Step 445/878, Loss: 0.0324\n",
      "  Step 450/878, Loss: 0.0324\n",
      "  Step 455/878, Loss: 0.0324\n",
      "  Step 460/878, Loss: 0.0324\n",
      "  Step 465/878, Loss: 0.0324\n",
      "  Step 470/878, Loss: 0.0324\n",
      "  Step 475/878, Loss: 0.0324\n",
      "  Step 480/878, Loss: 0.0324\n",
      "  Step 485/878, Loss: 0.0324\n",
      "  Step 490/878, Loss: 0.0324\n",
      "  Step 495/878, Loss: 0.0324\n",
      "  Step 500/878, Loss: 0.0324\n",
      "  Step 505/878, Loss: 0.0324\n",
      "  Step 510/878, Loss: 0.0324\n",
      "  Step 515/878, Loss: 0.0324\n",
      "  Step 520/878, Loss: 0.0324\n",
      "  Step 525/878, Loss: 0.0324\n",
      "  Step 530/878, Loss: 0.0324\n",
      "  Step 535/878, Loss: 0.0324\n",
      "  Step 540/878, Loss: 0.0324\n",
      "  Step 545/878, Loss: 0.0324\n",
      "  Step 550/878, Loss: 0.0324\n",
      "  Step 555/878, Loss: 0.0324\n",
      "  Step 560/878, Loss: 0.0324\n",
      "  Step 565/878, Loss: 0.0324\n",
      "  Step 570/878, Loss: 0.0324\n",
      "  Step 575/878, Loss: 0.0324\n",
      "  Step 580/878, Loss: 0.0324\n",
      "  Step 585/878, Loss: 0.0324\n",
      "  Step 590/878, Loss: 0.0324\n",
      "  Step 595/878, Loss: 0.0324\n",
      "  Step 600/878, Loss: 0.0324\n",
      "  Step 605/878, Loss: 0.0324\n",
      "  Step 610/878, Loss: 0.0324\n",
      "  Step 615/878, Loss: 0.0324\n",
      "  Step 620/878, Loss: 0.0324\n",
      "  Step 625/878, Loss: 0.0324\n",
      "  Step 630/878, Loss: 0.0324\n",
      "  Step 635/878, Loss: 0.0323\n",
      "  Step 640/878, Loss: 0.0323\n",
      "  Step 645/878, Loss: 0.0323\n",
      "  Step 650/878, Loss: 0.0323\n",
      "  Step 655/878, Loss: 0.0323\n",
      "  Step 660/878, Loss: 0.0323\n",
      "  Step 665/878, Loss: 0.0323\n",
      "  Step 670/878, Loss: 0.0323\n",
      "  Step 675/878, Loss: 0.0323\n",
      "  Step 680/878, Loss: 0.0323\n",
      "  Step 685/878, Loss: 0.0323\n",
      "  Step 690/878, Loss: 0.0323\n",
      "  Step 695/878, Loss: 0.0323\n",
      "  Step 700/878, Loss: 0.0323\n",
      "  Step 705/878, Loss: 0.0323\n",
      "  Step 710/878, Loss: 0.0323\n",
      "  Step 715/878, Loss: 0.0323\n",
      "  Step 720/878, Loss: 0.0323\n",
      "  Step 725/878, Loss: 0.0323\n",
      "  Step 730/878, Loss: 0.0323\n",
      "  Step 735/878, Loss: 0.0323\n",
      "  Step 740/878, Loss: 0.0323\n",
      "  Step 745/878, Loss: 0.0323\n",
      "  Step 750/878, Loss: 0.0323\n",
      "  Step 755/878, Loss: 0.0323\n",
      "  Step 760/878, Loss: 0.0323\n",
      "  Step 765/878, Loss: 0.0323\n",
      "  Step 770/878, Loss: 0.0323\n",
      "  Step 775/878, Loss: 0.0323\n",
      "  Step 780/878, Loss: 0.0323\n",
      "  Step 785/878, Loss: 0.0323\n",
      "  Step 790/878, Loss: 0.0323\n",
      "  Step 795/878, Loss: 0.0323\n",
      "  Step 800/878, Loss: 0.0323\n",
      "  Step 805/878, Loss: 0.0323\n",
      "  Step 810/878, Loss: 0.0323\n",
      "  Step 815/878, Loss: 0.0323\n",
      "  Step 820/878, Loss: 0.0322\n",
      "  Step 825/878, Loss: 0.0322\n",
      "  Step 830/878, Loss: 0.0322\n",
      "  Step 835/878, Loss: 0.0322\n",
      "  Step 840/878, Loss: 0.0322\n",
      "  Step 845/878, Loss: 0.0322\n",
      "  Step 850/878, Loss: 0.0322\n",
      "  Step 855/878, Loss: 0.0322\n",
      "  Step 860/878, Loss: 0.0322\n",
      "  Step 865/878, Loss: 0.0322\n",
      "  Step 870/878, Loss: 0.0322\n",
      "  Step 875/878, Loss: 0.0322\n",
      "  Epoch 15 Loss: 0.0322\n",
      "\n",
      "Epoch 16/20\n",
      "  Step 5/878, Loss: 0.0318\n",
      "  Step 10/878, Loss: 0.0318\n",
      "  Step 15/878, Loss: 0.0318\n",
      "  Step 20/878, Loss: 0.0318\n",
      "  Step 25/878, Loss: 0.0318\n",
      "  Step 30/878, Loss: 0.0317\n",
      "  Step 35/878, Loss: 0.0317\n",
      "  Step 40/878, Loss: 0.0317\n",
      "  Step 45/878, Loss: 0.0317\n",
      "  Step 50/878, Loss: 0.0317\n",
      "  Step 55/878, Loss: 0.0317\n",
      "  Step 60/878, Loss: 0.0317\n",
      "  Step 65/878, Loss: 0.0317\n",
      "  Step 70/878, Loss: 0.0317\n",
      "  Step 75/878, Loss: 0.0317\n",
      "  Step 80/878, Loss: 0.0317\n",
      "  Step 85/878, Loss: 0.0317\n",
      "  Step 90/878, Loss: 0.0317\n",
      "  Step 95/878, Loss: 0.0317\n",
      "  Step 100/878, Loss: 0.0317\n",
      "  Step 105/878, Loss: 0.0317\n",
      "  Step 110/878, Loss: 0.0317\n",
      "  Step 115/878, Loss: 0.0317\n",
      "  Step 120/878, Loss: 0.0317\n",
      "  Step 125/878, Loss: 0.0317\n",
      "  Step 130/878, Loss: 0.0317\n",
      "  Step 135/878, Loss: 0.0317\n",
      "  Step 140/878, Loss: 0.0317\n",
      "  Step 145/878, Loss: 0.0317\n",
      "  Step 150/878, Loss: 0.0317\n",
      "  Step 155/878, Loss: 0.0317\n",
      "  Step 160/878, Loss: 0.0317\n",
      "  Step 165/878, Loss: 0.0317\n",
      "  Step 170/878, Loss: 0.0317\n",
      "  Step 175/878, Loss: 0.0317\n",
      "  Step 180/878, Loss: 0.0317\n",
      "  Step 185/878, Loss: 0.0317\n",
      "  Step 190/878, Loss: 0.0317\n",
      "  Step 195/878, Loss: 0.0317\n",
      "  Step 200/878, Loss: 0.0317\n",
      "  Step 205/878, Loss: 0.0317\n",
      "  Step 210/878, Loss: 0.0317\n",
      "  Step 215/878, Loss: 0.0317\n",
      "  Step 220/878, Loss: 0.0317\n",
      "  Step 225/878, Loss: 0.0317\n",
      "  Step 230/878, Loss: 0.0317\n",
      "  Step 235/878, Loss: 0.0317\n",
      "  Step 240/878, Loss: 0.0317\n",
      "  Step 245/878, Loss: 0.0317\n",
      "  Step 250/878, Loss: 0.0317\n",
      "  Step 255/878, Loss: 0.0317\n",
      "  Step 260/878, Loss: 0.0316\n",
      "  Step 265/878, Loss: 0.0316\n",
      "  Step 270/878, Loss: 0.0316\n",
      "  Step 275/878, Loss: 0.0316\n",
      "  Step 280/878, Loss: 0.0316\n",
      "  Step 285/878, Loss: 0.0316\n",
      "  Step 290/878, Loss: 0.0316\n",
      "  Step 295/878, Loss: 0.0316\n",
      "  Step 300/878, Loss: 0.0316\n",
      "  Step 305/878, Loss: 0.0316\n",
      "  Step 310/878, Loss: 0.0316\n",
      "  Step 315/878, Loss: 0.0316\n",
      "  Step 320/878, Loss: 0.0316\n",
      "  Step 325/878, Loss: 0.0316\n",
      "  Step 330/878, Loss: 0.0316\n",
      "  Step 335/878, Loss: 0.0316\n",
      "  Step 340/878, Loss: 0.0316\n",
      "  Step 345/878, Loss: 0.0316\n",
      "  Step 350/878, Loss: 0.0316\n",
      "  Step 355/878, Loss: 0.0316\n",
      "  Step 360/878, Loss: 0.0316\n",
      "  Step 365/878, Loss: 0.0316\n",
      "  Step 370/878, Loss: 0.0316\n",
      "  Step 375/878, Loss: 0.0316\n",
      "  Step 380/878, Loss: 0.0316\n",
      "  Step 385/878, Loss: 0.0316\n",
      "  Step 390/878, Loss: 0.0316\n",
      "  Step 395/878, Loss: 0.0316\n",
      "  Step 400/878, Loss: 0.0316\n",
      "  Step 405/878, Loss: 0.0316\n",
      "  Step 410/878, Loss: 0.0316\n",
      "  Step 415/878, Loss: 0.0316\n",
      "  Step 420/878, Loss: 0.0316\n",
      "  Step 425/878, Loss: 0.0316\n",
      "  Step 430/878, Loss: 0.0316\n",
      "  Step 435/878, Loss: 0.0316\n",
      "  Step 440/878, Loss: 0.0316\n",
      "  Step 445/878, Loss: 0.0316\n",
      "  Step 450/878, Loss: 0.0316\n",
      "  Step 455/878, Loss: 0.0316\n",
      "  Step 460/878, Loss: 0.0316\n",
      "  Step 465/878, Loss: 0.0316\n",
      "  Step 470/878, Loss: 0.0316\n",
      "  Step 475/878, Loss: 0.0315\n",
      "  Step 480/878, Loss: 0.0315\n",
      "  Step 485/878, Loss: 0.0315\n",
      "  Step 490/878, Loss: 0.0315\n",
      "  Step 495/878, Loss: 0.0315\n",
      "  Step 500/878, Loss: 0.0315\n",
      "  Step 505/878, Loss: 0.0315\n",
      "  Step 510/878, Loss: 0.0315\n",
      "  Step 515/878, Loss: 0.0315\n",
      "  Step 520/878, Loss: 0.0315\n",
      "  Step 525/878, Loss: 0.0315\n",
      "  Step 530/878, Loss: 0.0315\n",
      "  Step 535/878, Loss: 0.0315\n",
      "  Step 540/878, Loss: 0.0315\n",
      "  Step 545/878, Loss: 0.0315\n",
      "  Step 550/878, Loss: 0.0315\n",
      "  Step 555/878, Loss: 0.0315\n",
      "  Step 560/878, Loss: 0.0315\n",
      "  Step 565/878, Loss: 0.0315\n",
      "  Step 570/878, Loss: 0.0315\n",
      "  Step 575/878, Loss: 0.0315\n",
      "  Step 580/878, Loss: 0.0315\n",
      "  Step 585/878, Loss: 0.0315\n",
      "  Step 590/878, Loss: 0.0315\n",
      "  Step 595/878, Loss: 0.0315\n",
      "  Step 600/878, Loss: 0.0315\n",
      "  Step 605/878, Loss: 0.0315\n",
      "  Step 610/878, Loss: 0.0315\n",
      "  Step 615/878, Loss: 0.0315\n",
      "  Step 620/878, Loss: 0.0315\n",
      "  Step 625/878, Loss: 0.0315\n",
      "  Step 630/878, Loss: 0.0315\n",
      "  Step 635/878, Loss: 0.0315\n",
      "  Step 640/878, Loss: 0.0315\n",
      "  Step 645/878, Loss: 0.0315\n",
      "  Step 650/878, Loss: 0.0315\n",
      "  Step 655/878, Loss: 0.0315\n",
      "  Step 660/878, Loss: 0.0315\n",
      "  Step 665/878, Loss: 0.0315\n",
      "  Step 670/878, Loss: 0.0315\n",
      "  Step 675/878, Loss: 0.0315\n",
      "  Step 680/878, Loss: 0.0315\n",
      "  Step 685/878, Loss: 0.0315\n",
      "  Step 690/878, Loss: 0.0315\n",
      "  Step 695/878, Loss: 0.0314\n",
      "  Step 700/878, Loss: 0.0314\n",
      "  Step 705/878, Loss: 0.0314\n",
      "  Step 710/878, Loss: 0.0314\n",
      "  Step 715/878, Loss: 0.0314\n",
      "  Step 720/878, Loss: 0.0314\n",
      "  Step 725/878, Loss: 0.0314\n",
      "  Step 730/878, Loss: 0.0314\n",
      "  Step 735/878, Loss: 0.0314\n",
      "  Step 740/878, Loss: 0.0314\n",
      "  Step 745/878, Loss: 0.0314\n",
      "  Step 750/878, Loss: 0.0314\n",
      "  Step 755/878, Loss: 0.0314\n",
      "  Step 760/878, Loss: 0.0314\n",
      "  Step 765/878, Loss: 0.0314\n",
      "  Step 770/878, Loss: 0.0314\n",
      "  Step 775/878, Loss: 0.0314\n",
      "  Step 780/878, Loss: 0.0314\n",
      "  Step 785/878, Loss: 0.0314\n",
      "  Step 790/878, Loss: 0.0314\n",
      "  Step 795/878, Loss: 0.0314\n",
      "  Step 800/878, Loss: 0.0314\n",
      "  Step 805/878, Loss: 0.0314\n",
      "  Step 810/878, Loss: 0.0314\n",
      "  Step 815/878, Loss: 0.0314\n",
      "  Step 820/878, Loss: 0.0314\n",
      "  Step 825/878, Loss: 0.0314\n",
      "  Step 830/878, Loss: 0.0314\n",
      "  Step 835/878, Loss: 0.0314\n",
      "  Step 840/878, Loss: 0.0314\n",
      "  Step 845/878, Loss: 0.0314\n",
      "  Step 850/878, Loss: 0.0314\n",
      "  Step 855/878, Loss: 0.0314\n",
      "  Step 860/878, Loss: 0.0314\n",
      "  Step 865/878, Loss: 0.0314\n",
      "  Step 870/878, Loss: 0.0314\n",
      "  Step 875/878, Loss: 0.0314\n",
      "  Epoch 16 Loss: 0.0314\n",
      "\n",
      "Epoch 17/20\n",
      "  Step 5/878, Loss: 0.0310\n",
      "  Step 10/878, Loss: 0.0310\n",
      "  Step 15/878, Loss: 0.0310\n",
      "  Step 20/878, Loss: 0.0310\n",
      "  Step 25/878, Loss: 0.0310\n",
      "  Step 30/878, Loss: 0.0310\n",
      "  Step 35/878, Loss: 0.0310\n",
      "  Step 40/878, Loss: 0.0310\n",
      "  Step 45/878, Loss: 0.0310\n",
      "  Step 50/878, Loss: 0.0310\n",
      "  Step 55/878, Loss: 0.0309\n",
      "  Step 60/878, Loss: 0.0309\n",
      "  Step 65/878, Loss: 0.0309\n",
      "  Step 70/878, Loss: 0.0309\n",
      "  Step 75/878, Loss: 0.0309\n",
      "  Step 80/878, Loss: 0.0309\n",
      "  Step 85/878, Loss: 0.0309\n",
      "  Step 90/878, Loss: 0.0309\n",
      "  Step 95/878, Loss: 0.0309\n",
      "  Step 100/878, Loss: 0.0309\n",
      "  Step 105/878, Loss: 0.0309\n",
      "  Step 110/878, Loss: 0.0309\n",
      "  Step 115/878, Loss: 0.0309\n",
      "  Step 120/878, Loss: 0.0309\n",
      "  Step 125/878, Loss: 0.0309\n",
      "  Step 130/878, Loss: 0.0309\n",
      "  Step 135/878, Loss: 0.0309\n",
      "  Step 140/878, Loss: 0.0309\n",
      "  Step 145/878, Loss: 0.0309\n",
      "  Step 150/878, Loss: 0.0309\n",
      "  Step 155/878, Loss: 0.0309\n",
      "  Step 160/878, Loss: 0.0309\n",
      "  Step 165/878, Loss: 0.0309\n",
      "  Step 170/878, Loss: 0.0309\n",
      "  Step 175/878, Loss: 0.0309\n",
      "  Step 180/878, Loss: 0.0309\n",
      "  Step 185/878, Loss: 0.0309\n",
      "  Step 190/878, Loss: 0.0309\n",
      "  Step 195/878, Loss: 0.0309\n",
      "  Step 200/878, Loss: 0.0309\n",
      "  Step 205/878, Loss: 0.0309\n",
      "  Step 210/878, Loss: 0.0309\n",
      "  Step 215/878, Loss: 0.0309\n",
      "  Step 220/878, Loss: 0.0309\n",
      "  Step 225/878, Loss: 0.0309\n",
      "  Step 230/878, Loss: 0.0309\n",
      "  Step 235/878, Loss: 0.0309\n",
      "  Step 240/878, Loss: 0.0309\n",
      "  Step 245/878, Loss: 0.0309\n",
      "  Step 250/878, Loss: 0.0309\n",
      "  Step 255/878, Loss: 0.0309\n",
      "  Step 260/878, Loss: 0.0309\n",
      "  Step 265/878, Loss: 0.0309\n",
      "  Step 270/878, Loss: 0.0308\n",
      "  Step 275/878, Loss: 0.0308\n",
      "  Step 280/878, Loss: 0.0308\n",
      "  Step 285/878, Loss: 0.0308\n",
      "  Step 290/878, Loss: 0.0308\n",
      "  Step 295/878, Loss: 0.0308\n",
      "  Step 300/878, Loss: 0.0308\n",
      "  Step 305/878, Loss: 0.0308\n",
      "  Step 310/878, Loss: 0.0308\n",
      "  Step 315/878, Loss: 0.0308\n",
      "  Step 320/878, Loss: 0.0308\n",
      "  Step 325/878, Loss: 0.0308\n",
      "  Step 330/878, Loss: 0.0308\n",
      "  Step 335/878, Loss: 0.0308\n",
      "  Step 340/878, Loss: 0.0308\n",
      "  Step 345/878, Loss: 0.0308\n",
      "  Step 350/878, Loss: 0.0308\n",
      "  Step 355/878, Loss: 0.0308\n",
      "  Step 360/878, Loss: 0.0308\n",
      "  Step 365/878, Loss: 0.0308\n",
      "  Step 370/878, Loss: 0.0308\n",
      "  Step 375/878, Loss: 0.0308\n",
      "  Step 380/878, Loss: 0.0308\n",
      "  Step 385/878, Loss: 0.0308\n",
      "  Step 390/878, Loss: 0.0308\n",
      "  Step 395/878, Loss: 0.0308\n",
      "  Step 400/878, Loss: 0.0308\n",
      "  Step 405/878, Loss: 0.0308\n",
      "  Step 410/878, Loss: 0.0308\n",
      "  Step 415/878, Loss: 0.0308\n",
      "  Step 420/878, Loss: 0.0308\n",
      "  Step 425/878, Loss: 0.0308\n",
      "  Step 430/878, Loss: 0.0308\n",
      "  Step 435/878, Loss: 0.0308\n",
      "  Step 440/878, Loss: 0.0308\n",
      "  Step 445/878, Loss: 0.0308\n",
      "  Step 450/878, Loss: 0.0308\n",
      "  Step 455/878, Loss: 0.0308\n",
      "  Step 460/878, Loss: 0.0308\n",
      "  Step 465/878, Loss: 0.0308\n",
      "  Step 470/878, Loss: 0.0308\n",
      "  Step 475/878, Loss: 0.0308\n",
      "  Step 480/878, Loss: 0.0308\n",
      "  Step 485/878, Loss: 0.0308\n",
      "  Step 490/878, Loss: 0.0307\n",
      "  Step 495/878, Loss: 0.0307\n",
      "  Step 500/878, Loss: 0.0307\n",
      "  Step 505/878, Loss: 0.0307\n",
      "  Step 510/878, Loss: 0.0307\n",
      "  Step 515/878, Loss: 0.0307\n",
      "  Step 520/878, Loss: 0.0307\n",
      "  Step 525/878, Loss: 0.0307\n",
      "  Step 530/878, Loss: 0.0307\n",
      "  Step 535/878, Loss: 0.0307\n",
      "  Step 540/878, Loss: 0.0307\n",
      "  Step 545/878, Loss: 0.0307\n",
      "  Step 550/878, Loss: 0.0307\n",
      "  Step 555/878, Loss: 0.0307\n",
      "  Step 560/878, Loss: 0.0307\n",
      "  Step 565/878, Loss: 0.0307\n",
      "  Step 570/878, Loss: 0.0307\n",
      "  Step 575/878, Loss: 0.0307\n",
      "  Step 580/878, Loss: 0.0307\n",
      "  Step 585/878, Loss: 0.0307\n",
      "  Step 590/878, Loss: 0.0307\n",
      "  Step 595/878, Loss: 0.0307\n",
      "  Step 600/878, Loss: 0.0307\n",
      "  Step 605/878, Loss: 0.0307\n",
      "  Step 610/878, Loss: 0.0307\n",
      "  Step 615/878, Loss: 0.0307\n",
      "  Step 620/878, Loss: 0.0307\n",
      "  Step 625/878, Loss: 0.0307\n",
      "  Step 630/878, Loss: 0.0307\n",
      "  Step 635/878, Loss: 0.0307\n",
      "  Step 640/878, Loss: 0.0307\n",
      "  Step 645/878, Loss: 0.0307\n",
      "  Step 650/878, Loss: 0.0307\n",
      "  Step 655/878, Loss: 0.0307\n",
      "  Step 660/878, Loss: 0.0307\n",
      "  Step 665/878, Loss: 0.0307\n",
      "  Step 670/878, Loss: 0.0307\n",
      "  Step 675/878, Loss: 0.0307\n",
      "  Step 680/878, Loss: 0.0307\n",
      "  Step 685/878, Loss: 0.0307\n",
      "  Step 690/878, Loss: 0.0307\n",
      "  Step 695/878, Loss: 0.0307\n",
      "  Step 700/878, Loss: 0.0307\n",
      "  Step 705/878, Loss: 0.0306\n",
      "  Step 710/878, Loss: 0.0306\n",
      "  Step 715/878, Loss: 0.0306\n",
      "  Step 720/878, Loss: 0.0306\n",
      "  Step 725/878, Loss: 0.0306\n",
      "  Step 730/878, Loss: 0.0306\n",
      "  Step 735/878, Loss: 0.0306\n",
      "  Step 740/878, Loss: 0.0306\n",
      "  Step 745/878, Loss: 0.0306\n",
      "  Step 750/878, Loss: 0.0306\n",
      "  Step 755/878, Loss: 0.0306\n",
      "  Step 760/878, Loss: 0.0306\n",
      "  Step 765/878, Loss: 0.0306\n",
      "  Step 770/878, Loss: 0.0306\n",
      "  Step 775/878, Loss: 0.0306\n",
      "  Step 780/878, Loss: 0.0306\n",
      "  Step 785/878, Loss: 0.0306\n",
      "  Step 790/878, Loss: 0.0306\n",
      "  Step 795/878, Loss: 0.0306\n",
      "  Step 800/878, Loss: 0.0306\n",
      "  Step 805/878, Loss: 0.0306\n",
      "  Step 810/878, Loss: 0.0306\n",
      "  Step 815/878, Loss: 0.0306\n",
      "  Step 820/878, Loss: 0.0306\n",
      "  Step 825/878, Loss: 0.0306\n",
      "  Step 830/878, Loss: 0.0306\n",
      "  Step 835/878, Loss: 0.0306\n",
      "  Step 840/878, Loss: 0.0306\n",
      "  Step 845/878, Loss: 0.0306\n",
      "  Step 850/878, Loss: 0.0306\n",
      "  Step 855/878, Loss: 0.0306\n",
      "  Step 860/878, Loss: 0.0306\n",
      "  Step 865/878, Loss: 0.0306\n",
      "  Step 870/878, Loss: 0.0306\n",
      "  Step 875/878, Loss: 0.0306\n",
      "  Epoch 17 Loss: 0.0306\n",
      "\n",
      "Epoch 18/20\n",
      "  Step 5/878, Loss: 0.0302\n",
      "  Step 10/878, Loss: 0.0302\n",
      "  Step 15/878, Loss: 0.0302\n",
      "  Step 20/878, Loss: 0.0302\n",
      "  Step 25/878, Loss: 0.0301\n",
      "  Step 30/878, Loss: 0.0301\n",
      "  Step 35/878, Loss: 0.0301\n",
      "  Step 40/878, Loss: 0.0301\n",
      "  Step 45/878, Loss: 0.0301\n",
      "  Step 50/878, Loss: 0.0301\n",
      "  Step 55/878, Loss: 0.0301\n",
      "  Step 60/878, Loss: 0.0301\n",
      "  Step 65/878, Loss: 0.0301\n",
      "  Step 70/878, Loss: 0.0301\n",
      "  Step 75/878, Loss: 0.0301\n",
      "  Step 80/878, Loss: 0.0301\n",
      "  Step 85/878, Loss: 0.0301\n",
      "  Step 90/878, Loss: 0.0301\n",
      "  Step 95/878, Loss: 0.0301\n",
      "  Step 100/878, Loss: 0.0301\n",
      "  Step 105/878, Loss: 0.0301\n",
      "  Step 110/878, Loss: 0.0301\n",
      "  Step 115/878, Loss: 0.0301\n",
      "  Step 120/878, Loss: 0.0301\n",
      "  Step 125/878, Loss: 0.0301\n",
      "  Step 130/878, Loss: 0.0301\n",
      "  Step 135/878, Loss: 0.0301\n",
      "  Step 140/878, Loss: 0.0301\n",
      "  Step 145/878, Loss: 0.0301\n",
      "  Step 150/878, Loss: 0.0301\n",
      "  Step 155/878, Loss: 0.0301\n",
      "  Step 160/878, Loss: 0.0301\n",
      "  Step 165/878, Loss: 0.0301\n",
      "  Step 170/878, Loss: 0.0301\n",
      "  Step 175/878, Loss: 0.0301\n",
      "  Step 180/878, Loss: 0.0301\n",
      "  Step 185/878, Loss: 0.0301\n",
      "  Step 190/878, Loss: 0.0301\n",
      "  Step 195/878, Loss: 0.0301\n",
      "  Step 200/878, Loss: 0.0301\n",
      "  Step 205/878, Loss: 0.0301\n",
      "  Step 210/878, Loss: 0.0301\n",
      "  Step 215/878, Loss: 0.0301\n",
      "  Step 220/878, Loss: 0.0301\n",
      "  Step 225/878, Loss: 0.0301\n",
      "  Step 230/878, Loss: 0.0301\n",
      "  Step 235/878, Loss: 0.0301\n",
      "  Step 240/878, Loss: 0.0301\n",
      "  Step 245/878, Loss: 0.0300\n",
      "  Step 250/878, Loss: 0.0300\n",
      "  Step 255/878, Loss: 0.0300\n",
      "  Step 260/878, Loss: 0.0300\n",
      "  Step 265/878, Loss: 0.0300\n",
      "  Step 270/878, Loss: 0.0300\n",
      "  Step 275/878, Loss: 0.0300\n",
      "  Step 280/878, Loss: 0.0300\n",
      "  Step 285/878, Loss: 0.0300\n",
      "  Step 290/878, Loss: 0.0300\n",
      "  Step 295/878, Loss: 0.0300\n",
      "  Step 300/878, Loss: 0.0300\n",
      "  Step 305/878, Loss: 0.0300\n",
      "  Step 310/878, Loss: 0.0300\n",
      "  Step 315/878, Loss: 0.0300\n",
      "  Step 320/878, Loss: 0.0300\n",
      "  Step 325/878, Loss: 0.0300\n",
      "  Step 330/878, Loss: 0.0300\n",
      "  Step 335/878, Loss: 0.0300\n",
      "  Step 340/878, Loss: 0.0300\n",
      "  Step 345/878, Loss: 0.0300\n",
      "  Step 350/878, Loss: 0.0300\n",
      "  Step 355/878, Loss: 0.0300\n",
      "  Step 360/878, Loss: 0.0300\n",
      "  Step 365/878, Loss: 0.0300\n",
      "  Step 370/878, Loss: 0.0300\n",
      "  Step 375/878, Loss: 0.0300\n",
      "  Step 380/878, Loss: 0.0300\n",
      "  Step 385/878, Loss: 0.0300\n",
      "  Step 390/878, Loss: 0.0300\n",
      "  Step 395/878, Loss: 0.0300\n",
      "  Step 400/878, Loss: 0.0300\n",
      "  Step 405/878, Loss: 0.0300\n",
      "  Step 410/878, Loss: 0.0300\n",
      "  Step 415/878, Loss: 0.0300\n",
      "  Step 420/878, Loss: 0.0300\n",
      "  Step 425/878, Loss: 0.0300\n",
      "  Step 430/878, Loss: 0.0300\n",
      "  Step 435/878, Loss: 0.0300\n",
      "  Step 440/878, Loss: 0.0300\n",
      "  Step 445/878, Loss: 0.0300\n",
      "  Step 450/878, Loss: 0.0300\n",
      "  Step 455/878, Loss: 0.0300\n",
      "  Step 460/878, Loss: 0.0300\n",
      "  Step 465/878, Loss: 0.0300\n",
      "  Step 470/878, Loss: 0.0299\n",
      "  Step 475/878, Loss: 0.0299\n",
      "  Step 480/878, Loss: 0.0299\n",
      "  Step 485/878, Loss: 0.0299\n",
      "  Step 490/878, Loss: 0.0299\n",
      "  Step 495/878, Loss: 0.0299\n",
      "  Step 500/878, Loss: 0.0299\n",
      "  Step 505/878, Loss: 0.0299\n",
      "  Step 510/878, Loss: 0.0299\n",
      "  Step 515/878, Loss: 0.0299\n",
      "  Step 520/878, Loss: 0.0299\n",
      "  Step 525/878, Loss: 0.0299\n",
      "  Step 530/878, Loss: 0.0299\n",
      "  Step 535/878, Loss: 0.0299\n",
      "  Step 540/878, Loss: 0.0299\n",
      "  Step 545/878, Loss: 0.0299\n",
      "  Step 550/878, Loss: 0.0299\n",
      "  Step 555/878, Loss: 0.0299\n",
      "  Step 560/878, Loss: 0.0299\n",
      "  Step 565/878, Loss: 0.0299\n",
      "  Step 570/878, Loss: 0.0299\n",
      "  Step 575/878, Loss: 0.0299\n",
      "  Step 580/878, Loss: 0.0299\n",
      "  Step 585/878, Loss: 0.0299\n",
      "  Step 590/878, Loss: 0.0299\n",
      "  Step 595/878, Loss: 0.0299\n",
      "  Step 600/878, Loss: 0.0299\n",
      "  Step 605/878, Loss: 0.0299\n",
      "  Step 610/878, Loss: 0.0299\n",
      "  Step 615/878, Loss: 0.0299\n",
      "  Step 620/878, Loss: 0.0299\n",
      "  Step 625/878, Loss: 0.0299\n",
      "  Step 630/878, Loss: 0.0299\n",
      "  Step 635/878, Loss: 0.0299\n",
      "  Step 640/878, Loss: 0.0299\n",
      "  Step 645/878, Loss: 0.0299\n",
      "  Step 650/878, Loss: 0.0299\n",
      "  Step 655/878, Loss: 0.0299\n",
      "  Step 660/878, Loss: 0.0299\n",
      "  Step 665/878, Loss: 0.0299\n",
      "  Step 670/878, Loss: 0.0299\n",
      "  Step 675/878, Loss: 0.0299\n",
      "  Step 680/878, Loss: 0.0299\n",
      "  Step 685/878, Loss: 0.0299\n",
      "  Step 690/878, Loss: 0.0299\n",
      "  Step 695/878, Loss: 0.0299\n",
      "  Step 700/878, Loss: 0.0299\n",
      "  Step 705/878, Loss: 0.0299\n",
      "  Step 710/878, Loss: 0.0298\n",
      "  Step 715/878, Loss: 0.0298\n",
      "  Step 720/878, Loss: 0.0298\n",
      "  Step 725/878, Loss: 0.0298\n",
      "  Step 730/878, Loss: 0.0298\n",
      "  Step 735/878, Loss: 0.0298\n",
      "  Step 740/878, Loss: 0.0298\n",
      "  Step 745/878, Loss: 0.0298\n",
      "  Step 750/878, Loss: 0.0298\n",
      "  Step 755/878, Loss: 0.0298\n",
      "  Step 760/878, Loss: 0.0298\n",
      "  Step 765/878, Loss: 0.0298\n",
      "  Step 770/878, Loss: 0.0298\n",
      "  Step 775/878, Loss: 0.0298\n",
      "  Step 780/878, Loss: 0.0298\n",
      "  Step 785/878, Loss: 0.0298\n",
      "  Step 790/878, Loss: 0.0298\n",
      "  Step 795/878, Loss: 0.0298\n",
      "  Step 800/878, Loss: 0.0298\n",
      "  Step 805/878, Loss: 0.0298\n",
      "  Step 810/878, Loss: 0.0298\n",
      "  Step 815/878, Loss: 0.0298\n",
      "  Step 820/878, Loss: 0.0298\n",
      "  Step 825/878, Loss: 0.0298\n",
      "  Step 830/878, Loss: 0.0298\n",
      "  Step 835/878, Loss: 0.0298\n",
      "  Step 840/878, Loss: 0.0298\n",
      "  Step 845/878, Loss: 0.0298\n",
      "  Step 850/878, Loss: 0.0298\n",
      "  Step 855/878, Loss: 0.0298\n",
      "  Step 860/878, Loss: 0.0298\n",
      "  Step 865/878, Loss: 0.0298\n",
      "  Step 870/878, Loss: 0.0298\n",
      "  Step 875/878, Loss: 0.0298\n",
      "  Epoch 18 Loss: 0.0298\n",
      "\n",
      "Epoch 19/20\n",
      "  Step 5/878, Loss: 0.0294\n",
      "  Step 10/878, Loss: 0.0294\n",
      "  Step 15/878, Loss: 0.0294\n",
      "  Step 20/878, Loss: 0.0294\n",
      "  Step 25/878, Loss: 0.0294\n",
      "  Step 30/878, Loss: 0.0294\n",
      "  Step 35/878, Loss: 0.0294\n",
      "  Step 40/878, Loss: 0.0294\n",
      "  Step 45/878, Loss: 0.0294\n",
      "  Step 50/878, Loss: 0.0294\n",
      "  Step 55/878, Loss: 0.0294\n",
      "  Step 60/878, Loss: 0.0294\n",
      "  Step 65/878, Loss: 0.0294\n",
      "  Step 70/878, Loss: 0.0294\n",
      "  Step 75/878, Loss: 0.0294\n",
      "  Step 80/878, Loss: 0.0294\n",
      "  Step 85/878, Loss: 0.0294\n",
      "  Step 90/878, Loss: 0.0294\n",
      "  Step 95/878, Loss: 0.0294\n",
      "  Step 100/878, Loss: 0.0294\n",
      "  Step 105/878, Loss: 0.0294\n",
      "  Step 110/878, Loss: 0.0294\n",
      "  Step 115/878, Loss: 0.0294\n",
      "  Step 120/878, Loss: 0.0294\n",
      "  Step 125/878, Loss: 0.0294\n",
      "  Step 130/878, Loss: 0.0294\n",
      "  Step 135/878, Loss: 0.0294\n",
      "  Step 140/878, Loss: 0.0294\n",
      "  Step 145/878, Loss: 0.0294\n",
      "  Step 150/878, Loss: 0.0294\n",
      "  Step 155/878, Loss: 0.0294\n",
      "  Step 160/878, Loss: 0.0294\n",
      "  Step 165/878, Loss: 0.0294\n",
      "  Step 170/878, Loss: 0.0294\n",
      "  Step 175/878, Loss: 0.0294\n",
      "  Step 180/878, Loss: 0.0294\n",
      "  Step 185/878, Loss: 0.0294\n",
      "  Step 190/878, Loss: 0.0294\n",
      "  Step 195/878, Loss: 0.0294\n",
      "  Step 200/878, Loss: 0.0294\n",
      "  Step 205/878, Loss: 0.0294\n",
      "  Step 210/878, Loss: 0.0294\n",
      "  Step 215/878, Loss: 0.0294\n",
      "  Step 220/878, Loss: 0.0294\n",
      "  Step 225/878, Loss: 0.0294\n",
      "  Step 230/878, Loss: 0.0293\n",
      "  Step 235/878, Loss: 0.0293\n",
      "  Step 240/878, Loss: 0.0293\n",
      "  Step 245/878, Loss: 0.0293\n",
      "  Step 250/878, Loss: 0.0293\n",
      "  Step 255/878, Loss: 0.0293\n",
      "  Step 260/878, Loss: 0.0293\n",
      "  Step 265/878, Loss: 0.0293\n",
      "  Step 270/878, Loss: 0.0293\n",
      "  Step 275/878, Loss: 0.0293\n",
      "  Step 280/878, Loss: 0.0293\n",
      "  Step 285/878, Loss: 0.0293\n",
      "  Step 290/878, Loss: 0.0293\n",
      "  Step 295/878, Loss: 0.0293\n",
      "  Step 300/878, Loss: 0.0293\n",
      "  Step 305/878, Loss: 0.0293\n",
      "  Step 310/878, Loss: 0.0293\n",
      "  Step 315/878, Loss: 0.0293\n",
      "  Step 320/878, Loss: 0.0293\n",
      "  Step 325/878, Loss: 0.0293\n",
      "  Step 330/878, Loss: 0.0293\n",
      "  Step 335/878, Loss: 0.0293\n",
      "  Step 340/878, Loss: 0.0293\n",
      "  Step 345/878, Loss: 0.0293\n",
      "  Step 350/878, Loss: 0.0293\n",
      "  Step 355/878, Loss: 0.0293\n",
      "  Step 360/878, Loss: 0.0293\n",
      "  Step 365/878, Loss: 0.0293\n",
      "  Step 370/878, Loss: 0.0293\n",
      "  Step 375/878, Loss: 0.0293\n",
      "  Step 380/878, Loss: 0.0293\n",
      "  Step 385/878, Loss: 0.0293\n",
      "  Step 390/878, Loss: 0.0293\n",
      "  Step 395/878, Loss: 0.0293\n",
      "  Step 400/878, Loss: 0.0293\n",
      "  Step 405/878, Loss: 0.0293\n",
      "  Step 410/878, Loss: 0.0293\n",
      "  Step 415/878, Loss: 0.0293\n",
      "  Step 420/878, Loss: 0.0293\n",
      "  Step 425/878, Loss: 0.0293\n",
      "  Step 430/878, Loss: 0.0293\n",
      "  Step 435/878, Loss: 0.0293\n",
      "  Step 440/878, Loss: 0.0293\n",
      "  Step 445/878, Loss: 0.0293\n",
      "  Step 450/878, Loss: 0.0293\n",
      "  Step 455/878, Loss: 0.0293\n",
      "  Step 460/878, Loss: 0.0293\n",
      "  Step 465/878, Loss: 0.0293\n",
      "  Step 470/878, Loss: 0.0293\n",
      "  Step 475/878, Loss: 0.0293\n",
      "  Step 480/878, Loss: 0.0293\n",
      "  Step 485/878, Loss: 0.0293\n",
      "  Step 490/878, Loss: 0.0292\n",
      "  Step 495/878, Loss: 0.0292\n",
      "  Step 500/878, Loss: 0.0292\n",
      "  Step 505/878, Loss: 0.0292\n",
      "  Step 510/878, Loss: 0.0292\n",
      "  Step 515/878, Loss: 0.0292\n",
      "  Step 520/878, Loss: 0.0292\n",
      "  Step 525/878, Loss: 0.0292\n",
      "  Step 530/878, Loss: 0.0292\n",
      "  Step 535/878, Loss: 0.0292\n",
      "  Step 540/878, Loss: 0.0292\n",
      "  Step 545/878, Loss: 0.0292\n",
      "  Step 550/878, Loss: 0.0292\n",
      "  Step 555/878, Loss: 0.0292\n",
      "  Step 560/878, Loss: 0.0292\n",
      "  Step 565/878, Loss: 0.0292\n",
      "  Step 570/878, Loss: 0.0292\n",
      "  Step 575/878, Loss: 0.0292\n",
      "  Step 580/878, Loss: 0.0292\n",
      "  Step 585/878, Loss: 0.0292\n",
      "  Step 590/878, Loss: 0.0292\n",
      "  Step 595/878, Loss: 0.0292\n",
      "  Step 600/878, Loss: 0.0292\n",
      "  Step 605/878, Loss: 0.0292\n",
      "  Step 610/878, Loss: 0.0292\n",
      "  Step 615/878, Loss: 0.0292\n",
      "  Step 620/878, Loss: 0.0292\n",
      "  Step 625/878, Loss: 0.0292\n",
      "  Step 630/878, Loss: 0.0292\n",
      "  Step 635/878, Loss: 0.0292\n",
      "  Step 640/878, Loss: 0.0292\n",
      "  Step 645/878, Loss: 0.0292\n",
      "  Step 650/878, Loss: 0.0292\n",
      "  Step 655/878, Loss: 0.0292\n",
      "  Step 660/878, Loss: 0.0292\n",
      "  Step 665/878, Loss: 0.0292\n",
      "  Step 670/878, Loss: 0.0292\n",
      "  Step 675/878, Loss: 0.0292\n",
      "  Step 680/878, Loss: 0.0292\n",
      "  Step 685/878, Loss: 0.0292\n",
      "  Step 690/878, Loss: 0.0292\n",
      "  Step 695/878, Loss: 0.0292\n",
      "  Step 700/878, Loss: 0.0292\n",
      "  Step 705/878, Loss: 0.0292\n",
      "  Step 710/878, Loss: 0.0292\n",
      "  Step 715/878, Loss: 0.0292\n",
      "  Step 720/878, Loss: 0.0292\n",
      "  Step 725/878, Loss: 0.0292\n",
      "  Step 730/878, Loss: 0.0292\n",
      "  Step 735/878, Loss: 0.0291\n",
      "  Step 740/878, Loss: 0.0291\n",
      "  Step 745/878, Loss: 0.0291\n",
      "  Step 750/878, Loss: 0.0291\n",
      "  Step 755/878, Loss: 0.0291\n",
      "  Step 760/878, Loss: 0.0291\n",
      "  Step 765/878, Loss: 0.0291\n",
      "  Step 770/878, Loss: 0.0291\n",
      "  Step 775/878, Loss: 0.0291\n",
      "  Step 780/878, Loss: 0.0291\n",
      "  Step 785/878, Loss: 0.0291\n",
      "  Step 790/878, Loss: 0.0291\n",
      "  Step 795/878, Loss: 0.0291\n",
      "  Step 800/878, Loss: 0.0291\n",
      "  Step 805/878, Loss: 0.0291\n",
      "  Step 810/878, Loss: 0.0291\n",
      "  Step 815/878, Loss: 0.0291\n",
      "  Step 820/878, Loss: 0.0291\n",
      "  Step 825/878, Loss: 0.0291\n",
      "  Step 830/878, Loss: 0.0291\n",
      "  Step 835/878, Loss: 0.0291\n",
      "  Step 840/878, Loss: 0.0291\n",
      "  Step 845/878, Loss: 0.0291\n",
      "  Step 850/878, Loss: 0.0291\n",
      "  Step 855/878, Loss: 0.0291\n",
      "  Step 860/878, Loss: 0.0291\n",
      "  Step 865/878, Loss: 0.0291\n",
      "  Step 870/878, Loss: 0.0291\n",
      "  Step 875/878, Loss: 0.0291\n",
      "  Epoch 19 Loss: 0.0291\n",
      "\n",
      "Epoch 20/20\n",
      "  Step 5/878, Loss: 0.0287\n",
      "  Step 10/878, Loss: 0.0287\n",
      "  Step 15/878, Loss: 0.0287\n",
      "  Step 20/878, Loss: 0.0287\n",
      "  Step 25/878, Loss: 0.0287\n",
      "  Step 30/878, Loss: 0.0287\n",
      "  Step 35/878, Loss: 0.0287\n",
      "  Step 40/878, Loss: 0.0287\n",
      "  Step 45/878, Loss: 0.0287\n",
      "  Step 50/878, Loss: 0.0287\n",
      "  Step 55/878, Loss: 0.0287\n",
      "  Step 60/878, Loss: 0.0287\n",
      "  Step 65/878, Loss: 0.0287\n",
      "  Step 70/878, Loss: 0.0287\n",
      "  Step 75/878, Loss: 0.0287\n",
      "  Step 80/878, Loss: 0.0287\n",
      "  Step 85/878, Loss: 0.0287\n",
      "  Step 90/878, Loss: 0.0287\n",
      "  Step 95/878, Loss: 0.0287\n",
      "  Step 100/878, Loss: 0.0287\n",
      "  Step 105/878, Loss: 0.0287\n",
      "  Step 110/878, Loss: 0.0287\n",
      "  Step 115/878, Loss: 0.0287\n",
      "  Step 120/878, Loss: 0.0287\n",
      "  Step 125/878, Loss: 0.0287\n",
      "  Step 130/878, Loss: 0.0287\n",
      "  Step 135/878, Loss: 0.0287\n",
      "  Step 140/878, Loss: 0.0287\n",
      "  Step 145/878, Loss: 0.0287\n",
      "  Step 150/878, Loss: 0.0287\n",
      "  Step 155/878, Loss: 0.0287\n",
      "  Step 160/878, Loss: 0.0287\n",
      "  Step 165/878, Loss: 0.0287\n",
      "  Step 170/878, Loss: 0.0287\n",
      "  Step 175/878, Loss: 0.0287\n",
      "  Step 180/878, Loss: 0.0287\n",
      "  Step 185/878, Loss: 0.0287\n",
      "  Step 190/878, Loss: 0.0287\n",
      "  Step 195/878, Loss: 0.0287\n",
      "  Step 200/878, Loss: 0.0286\n",
      "  Step 205/878, Loss: 0.0286\n",
      "  Step 210/878, Loss: 0.0286\n",
      "  Step 215/878, Loss: 0.0286\n",
      "  Step 220/878, Loss: 0.0286\n",
      "  Step 225/878, Loss: 0.0286\n",
      "  Step 230/878, Loss: 0.0286\n",
      "  Step 235/878, Loss: 0.0286\n",
      "  Step 240/878, Loss: 0.0286\n",
      "  Step 245/878, Loss: 0.0286\n",
      "  Step 250/878, Loss: 0.0286\n",
      "  Step 255/878, Loss: 0.0286\n",
      "  Step 260/878, Loss: 0.0286\n",
      "  Step 265/878, Loss: 0.0286\n",
      "  Step 270/878, Loss: 0.0286\n",
      "  Step 275/878, Loss: 0.0286\n",
      "  Step 280/878, Loss: 0.0286\n",
      "  Step 285/878, Loss: 0.0286\n",
      "  Step 290/878, Loss: 0.0286\n",
      "  Step 295/878, Loss: 0.0286\n",
      "  Step 300/878, Loss: 0.0286\n",
      "  Step 305/878, Loss: 0.0286\n",
      "  Step 310/878, Loss: 0.0286\n",
      "  Step 315/878, Loss: 0.0286\n",
      "  Step 320/878, Loss: 0.0286\n",
      "  Step 325/878, Loss: 0.0286\n",
      "  Step 330/878, Loss: 0.0286\n",
      "  Step 335/878, Loss: 0.0286\n",
      "  Step 340/878, Loss: 0.0286\n",
      "  Step 345/878, Loss: 0.0286\n",
      "  Step 350/878, Loss: 0.0286\n",
      "  Step 355/878, Loss: 0.0286\n",
      "  Step 360/878, Loss: 0.0286\n",
      "  Step 365/878, Loss: 0.0286\n",
      "  Step 370/878, Loss: 0.0286\n",
      "  Step 375/878, Loss: 0.0286\n",
      "  Step 380/878, Loss: 0.0286\n",
      "  Step 385/878, Loss: 0.0286\n",
      "  Step 390/878, Loss: 0.0286\n",
      "  Step 395/878, Loss: 0.0286\n",
      "  Step 400/878, Loss: 0.0286\n",
      "  Step 405/878, Loss: 0.0286\n",
      "  Step 410/878, Loss: 0.0286\n",
      "  Step 415/878, Loss: 0.0286\n",
      "  Step 420/878, Loss: 0.0286\n",
      "  Step 425/878, Loss: 0.0286\n",
      "  Step 430/878, Loss: 0.0286\n",
      "  Step 435/878, Loss: 0.0286\n",
      "  Step 440/878, Loss: 0.0286\n",
      "  Step 445/878, Loss: 0.0286\n",
      "  Step 450/878, Loss: 0.0285\n",
      "  Step 455/878, Loss: 0.0285\n",
      "  Step 460/878, Loss: 0.0285\n",
      "  Step 465/878, Loss: 0.0285\n",
      "  Step 470/878, Loss: 0.0285\n",
      "  Step 475/878, Loss: 0.0285\n",
      "  Step 480/878, Loss: 0.0285\n",
      "  Step 485/878, Loss: 0.0285\n",
      "  Step 490/878, Loss: 0.0285\n",
      "  Step 495/878, Loss: 0.0285\n",
      "  Step 500/878, Loss: 0.0285\n",
      "  Step 505/878, Loss: 0.0285\n",
      "  Step 510/878, Loss: 0.0285\n",
      "  Step 515/878, Loss: 0.0285\n",
      "  Step 520/878, Loss: 0.0285\n",
      "  Step 525/878, Loss: 0.0285\n",
      "  Step 530/878, Loss: 0.0285\n",
      "  Step 535/878, Loss: 0.0285\n",
      "  Step 540/878, Loss: 0.0285\n",
      "  Step 545/878, Loss: 0.0285\n",
      "  Step 550/878, Loss: 0.0285\n",
      "  Step 555/878, Loss: 0.0285\n",
      "  Step 560/878, Loss: 0.0285\n",
      "  Step 565/878, Loss: 0.0285\n",
      "  Step 570/878, Loss: 0.0285\n",
      "  Step 575/878, Loss: 0.0285\n",
      "  Step 580/878, Loss: 0.0285\n",
      "  Step 585/878, Loss: 0.0285\n",
      "  Step 590/878, Loss: 0.0285\n",
      "  Step 595/878, Loss: 0.0285\n",
      "  Step 600/878, Loss: 0.0285\n",
      "  Step 605/878, Loss: 0.0285\n",
      "  Step 610/878, Loss: 0.0285\n",
      "  Step 615/878, Loss: 0.0285\n",
      "  Step 620/878, Loss: 0.0285\n",
      "  Step 625/878, Loss: 0.0285\n",
      "  Step 630/878, Loss: 0.0285\n",
      "  Step 635/878, Loss: 0.0285\n",
      "  Step 640/878, Loss: 0.0285\n",
      "  Step 645/878, Loss: 0.0285\n",
      "  Step 650/878, Loss: 0.0285\n",
      "  Step 655/878, Loss: 0.0285\n",
      "  Step 660/878, Loss: 0.0285\n",
      "  Step 665/878, Loss: 0.0285\n",
      "  Step 670/878, Loss: 0.0285\n",
      "  Step 675/878, Loss: 0.0285\n",
      "  Step 680/878, Loss: 0.0285\n",
      "  Step 685/878, Loss: 0.0285\n",
      "  Step 690/878, Loss: 0.0285\n",
      "  Step 695/878, Loss: 0.0285\n",
      "  Step 700/878, Loss: 0.0285\n",
      "  Step 705/878, Loss: 0.0285\n",
      "  Step 710/878, Loss: 0.0285\n",
      "  Step 715/878, Loss: 0.0285\n",
      "  Step 720/878, Loss: 0.0284\n",
      "  Step 725/878, Loss: 0.0284\n",
      "  Step 730/878, Loss: 0.0284\n",
      "  Step 735/878, Loss: 0.0284\n",
      "  Step 740/878, Loss: 0.0284\n",
      "  Step 745/878, Loss: 0.0284\n",
      "  Step 750/878, Loss: 0.0284\n",
      "  Step 755/878, Loss: 0.0284\n",
      "  Step 760/878, Loss: 0.0284\n",
      "  Step 765/878, Loss: 0.0284\n",
      "  Step 770/878, Loss: 0.0284\n",
      "  Step 775/878, Loss: 0.0284\n",
      "  Step 780/878, Loss: 0.0284\n",
      "  Step 785/878, Loss: 0.0284\n",
      "  Step 790/878, Loss: 0.0284\n",
      "  Step 795/878, Loss: 0.0284\n",
      "  Step 800/878, Loss: 0.0284\n",
      "  Step 805/878, Loss: 0.0284\n",
      "  Step 810/878, Loss: 0.0284\n",
      "  Step 815/878, Loss: 0.0284\n",
      "  Step 820/878, Loss: 0.0284\n",
      "  Step 825/878, Loss: 0.0284\n",
      "  Step 830/878, Loss: 0.0284\n",
      "  Step 835/878, Loss: 0.0284\n",
      "  Step 840/878, Loss: 0.0284\n",
      "  Step 845/878, Loss: 0.0284\n",
      "  Step 850/878, Loss: 0.0284\n",
      "  Step 855/878, Loss: 0.0284\n",
      "  Step 860/878, Loss: 0.0284\n",
      "  Step 865/878, Loss: 0.0284\n",
      "  Step 870/878, Loss: 0.0284\n",
      "  Step 875/878, Loss: 0.0284\n",
      "  Epoch 20 Loss: 0.0284\n",
      "\n",
      "✓ Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Custom training loop (FIXED VERSION)\n",
    "print(\"\\nStarting training...\\n\")\n",
    "\n",
    "# Track history\n",
    "history_loss = []\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n",
    "    epoch_losses = []\n",
    "    \n",
    "    # Generate pairs for this epoch\n",
    "    pairs = generate_pairs(PAIRS_PER_EPOCH)\n",
    "    \n",
    "    # Mini-batch training\n",
    "    for step in range(STEPS_PER_EPOCH):\n",
    "        # Get batch\n",
    "        start_idx = step * BATCH_SIZE\n",
    "        end_idx = start_idx + BATCH_SIZE\n",
    "        batch_pairs = pairs[start_idx:end_idx]\n",
    "        \n",
    "        # Load batch data\n",
    "        img1_batch = []\n",
    "        img2_batch = []\n",
    "        labels_batch = []\n",
    "        \n",
    "        for anchor_idx, pair_idx, label in batch_pairs:\n",
    "            img1_path = os.path.join('images', train_df.iloc[anchor_idx]['filename'])\n",
    "            img2_path = os.path.join('images', train_df.iloc[pair_idx]['filename'])\n",
    "            \n",
    "            img1 = preprocess_image(img1_path)\n",
    "            img2 = preprocess_image(img2_path)\n",
    "            \n",
    "            img1_batch.append(img1)\n",
    "            img2_batch.append(img2)\n",
    "            labels_batch.append(label)\n",
    "        \n",
    "        img1_batch = np.array(img1_batch)\n",
    "        img2_batch = np.array(img2_batch)\n",
    "        labels_batch = np.array(labels_batch)\n",
    "        \n",
    "        # Train on batch\n",
    "        loss = siamese_model.train_on_batch(\n",
    "            [img1_batch, img2_batch],\n",
    "            labels_batch\n",
    "        )\n",
    "        \n",
    "        epoch_losses.append(loss)\n",
    "        \n",
    "        # Progress\n",
    "        if (step + 1) % 5 == 0:\n",
    "            print(f\"  Step {step + 1}/{STEPS_PER_EPOCH}, Loss: {np.mean(epoch_losses):.4f}\")\n",
    "    \n",
    "    # Epoch summary\n",
    "    epoch_loss = np.mean(epoch_losses)\n",
    "    history_loss.append(epoch_loss)\n",
    "    print(f\"  Epoch {epoch + 1} Loss: {epoch_loss:.4f}\\n\")\n",
    "\n",
    "# Create history object for compatibility\n",
    "class History:\n",
    "    def __init__(self, loss):\n",
    "        self.history = {'loss': loss}\n",
    "\n",
    "history = History(history_loss)\n",
    "\n",
    "print(\"✓ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA2AAAAHUCAYAAABcVkvuAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAchJJREFUeJzt3XlYVnX+//HXDdyAIPuOAoIbuCu44JqVmLbYNjktZqWVX5sptX5luzZOVtOYOaW2aGqrU9luKaWpuS+gprgjKIsIyCIo3ML9+wO9JwI3hPtmeT6ui2vk3J9zzvu854S+OOd8jsFsNpsFAAAAAKhzdrYuAAAAAACaCgIYAAAAAFgJAQwAAAAArIQABgAAAABWQgADAAAAACshgAEAAACAlRDAAAAAAMBKCGAAAAAAYCUEMAAAAACwEgIYAMBqDAbDJX39+uuvV7SfKVOmyGAw1GjdX3/9tVZquJJ9f/HFF1bfNwDAOhxsXQAAoOlYv359pe//8Y9/aOXKlVqxYkWl5R06dLii/YwdO1bXXXddjdbt0aOH1q9ff8U1AABQHQIYAMBq+vTpU+l7Pz8/2dnZVVn+Z8XFxXJxcbnk/bRs2VItW7asUY3u7u4XrQcAgJriFkQAQL1y1VVXqVOnTlq9erX69u0rFxcXPfDAA5KkxYsXKy4uTkFBQWrWrJmioqI0efJkFRUVVdpGdbcgtmrVSjfccIN++ukn9ejRQ82aNVNkZKTmz59faVx1tyDed999at68uQ4cOKDhw4erefPmCgkJ0eOPP66SkpJK6x89elS333673Nzc5OnpqbvvvlubN2+WwWDQggULaqVHv//+u0aMGCEvLy85OzurW7duWrhwYaUx5eXlmjZtmtq3b69mzZrJ09NTXbp00ZtvvmkZc/z4cT300EMKCQmRk5OT/Pz81K9fP/3888+1UicAoCqugAEA6p2MjAzdc889evLJJ/Xyyy/Lzq7i94X79+/X8OHDNWHCBLm6umrPnj169dVXtWnTpiq3MVZn+/btevzxxzV58mQFBATo/fff15gxY9SmTRsNHDjwguuaTCbddNNNGjNmjB5//HGtXr1a//jHP+Th4aEXXnhBklRUVKTBgwcrNzdXr776qtq0aaOffvpJI0eOvPKmnLV371717dtX/v7+mjVrlnx8fPTRRx/pvvvu07Fjx/Tkk09Kkl577TVNmTJFzz33nAYOHCiTyaQ9e/YoLy/Psq1Ro0Zp27Zt+uc//6l27dopLy9P27ZtU05OTq3VCwCojAAGAKh3cnNz9fnnn+vqq6+utPy5556z/NlsNqtfv36KiorSoEGDtGPHDnXp0uWC283OztbatWsVGhoqSRo4cKB++eUXffLJJxcNYKWlpZo6dar+8pe/SJKuueYabdmyRZ988oklgC1cuFAHDhzQjz/+aHkGLS4uTsXFxXrnnXcurwnnMWXKFJWWlmrlypUKCQmRJA0fPlx5eXmaOnWqHn74YXl4eGjt2rXq3LmzpkyZYll36NChlba1du1ajR07Vg8++KBl2YgRI2qlTgBA9bgFEQBQ73h5eVUJX5J06NAh3XXXXQoMDJS9vb2MRqMGDRokSUpKSrrodrt162YJX5Lk7Oysdu3aKSUl5aLrGgwG3XjjjZWWdenSpdK6q1atkpubW5UJQO68886Lbv9SrVixQtdcc40lfJ1z3333qbi42DLRSa9evbR9+3aNHz9ey5YtU0FBQZVt9erVSwsWLNC0adO0YcMGmUymWqsTAFA9AhgAoN4JCgqqsuzkyZMaMGCANm7cqGnTpunXX3/V5s2btWTJEknSqVOnLrpdHx+fKsucnJwuaV0XFxc5OztXWff06dOW73NychQQEFBl3eqW1VROTk61/QkODrZ8LklPP/20Xn/9dW3YsEHDhg2Tj4+P5ardOYsXL9bo0aP1/vvvKzY2Vt7e3rr33nuVmZlZa/UCACojgAEA6p3q3uG1YsUKpaena/78+Ro7dqwGDhyomJgYubm52aDC6vn4+OjYsWNVltdmoPHx8VFGRkaV5enp6ZIkX19fSZKDg4MmTZqkbdu2KTc3V59++qmOHDmioUOHqri42DJ25syZOnz4sFJSUjR9+nQtWbJE9913X63VCwCojAAGAGgQzoUyJyenSstr69mq2jBo0CAVFhbqxx9/rLT8s88+q7V9XHPNNZYw+keLFi2Si4tLtVPoe3p66vbbb9cjjzyi3NxcHT58uMqY0NBQ/e1vf9OQIUO0bdu2WqsXAFAZk3AAABqEvn37ysvLS+PGjdOLL74oo9Gojz/+WNu3b7d1aRajR4/WG2+8oXvuuUfTpk1TmzZt9OOPP2rZsmWSZJnN8WI2bNhQ7fJBgwbpxRdf1Pfff6/BgwfrhRdekLe3tz7++GP98MMPeu211+Th4SFJuvHGG9WpUyfFxMTIz89PKSkpmjlzpsLCwtS2bVvl5+dr8ODBuuuuuxQZGSk3Nzdt3rxZP/30k2699dbaaQgAoAoCGACgQfDx8dEPP/ygxx9/XPfcc49cXV01YsQILV68WD169LB1eZIkV1dXrVixQhMmTNCTTz4pg8GguLg4zZ49W8OHD5enp+clbeff//53tctXrlypq666SuvWrdMzzzyjRx55RKdOnVJUVJQ++OCDSrcODh48WF9++aXef/99FRQUKDAwUEOGDNHzzz8vo9EoZ2dn9e7dWx9++KEOHz4sk8mk0NBQPfXUU5ap7AEAtc9gNpvNti4CAIDG7OWXX9Zzzz2n1NRUtWzZ0tblAABsiCtgAADUorfeekuSFBkZKZPJpBUrVmjWrFm65557CF8AAAIYAAC1ycXFRW+88YYOHz6skpISy219f3yJNACg6eIWRAAAAACwEqahBwAAAAArIYABAAAAgJUQwAAAAADASpiEo4bKy8uVnp4uNzc3GQwGW5cDAAAAwEbMZrMKCwsVHBwsO7sLX+MigNVQenq6QkJCbF0GAAAAgHriyJEjF33lCAGshtzc3CRVNNnd3d2mtZhMJi1fvlxxcXEyGo02raWpoOfWR8+ti35bHz23PnpuffTcuui39RQUFCgkJMSSES6EAFZD5247dHd3rxcBzMXFRe7u7vzHZSX03ProuXXRb+uj59ZHz62PnlsX/ba+S3k0iUk4AAAAAMBKCGAAAAAAYCUEMAAAAACwEgIYAAAAAFgJAQwAAAAArIQABgAAAABWQgADAAAAACshgAEAAACAlRDAAAAAAMBKCGAAAAAAYCUEMAAAAACwEgIYAAAAAFgJAayRMJXbugIAAAAAF0MAawTe+PmAXthir51p+bYuBQAAAMAFEMAagaMnTqm4zKAPN6TauhQAAAAAF0AAawRG9QmRJH2/M1PZJ0tsXA0AAACA8yGANQLdQjwV6mqWqcyszzZxFQwAAACorwhgjcTAoIpZOD7akCpTGTNyAAAAAPURAayR6O5jlo+rozILTmv5rmO2LgcAAABANQhgjYSDnTQypqUkaeG6w7YtBgAAAEC1CGCNyJ29WsrBzqBNh3O1O73A1uUAAAAA+BMCWCMS6O6soZ0CJXEVDAAAAKiPCGCNzH19W0mSvk5M04miUtsWAwAAAKASAlgjExPmpQ5B7io5U67FW47YuhwAAAAAf0AAa2QMBoPlKtiH61NUVm62bUEAAAAALAhgjdBN3YLl5WJUWt4p/ZzElPQAAABAfUEAa4ScjfYa2TNUEpNxAAAAAPUJAayRuqdPqOwM0rqDOdp3rNDW5QAAAAAQAazRaunloiEdAiRxFQwAAACoLwhgjdjos5NxLNmWpvxTJtsWAwAAAIAA1pjFRviofYCbTpnK9DlT0gMAAAA2RwBrxAwGg+7tGyZJ+nBDisqZkh4AAACwKQJYI3dL9xZyd3ZQSk6xft2XZetyAAAAgCaNANbIuTg66I6YEEnSgnUpNq4GAAAAaNoIYE3AvbGtZDBIq/cd18HjJ21dDgAAANBkEcCagFAfF13d3l+S9OF6roIBAAAAtkIAayLOTUn/xdajOllyxrbFAAAAAE0UAayJ6N/GVxF+rjpZckZfbj1q63IAAACAJokA1kTY2Rk0OraVJGnh+sNMSQ8AAADYAAGsCbktuqWaOzno0PEi/XYg29blAAAAAE0OAawJae7koNujW0qSFq47bNtiAAAAgCbI5gFs9uzZCg8Pl7Ozs6Kjo7VmzZoLjl+1apWio6Pl7OysiIgIzZ07t8qYvLw8PfLIIwoKCpKzs7OioqK0dOnSK9pvY3FvbJgkacXeLKXmFNu4GgAAAKBpsWkAW7x4sSZMmKBnn31WCQkJGjBggIYNG6bU1NRqxycnJ2v48OEaMGCAEhIS9Mwzz+jRRx/Vl19+aRlTWlqqIUOG6PDhw/riiy+0d+9evffee2rRokWN99uYRPg118B2fjKbpUXrD9u6HAAAAKBJsWkAmzFjhsaMGaOxY8cqKipKM2fOVEhIiObMmVPt+Llz5yo0NFQzZ85UVFSUxo4dqwceeECvv/66Zcz8+fOVm5urr7/+Wv369VNYWJj69++vrl271ni/jc19fSuugv13yxEVlzIlPQAAAGAtDrbacWlpqbZu3arJkydXWh4XF6d169ZVu8769esVFxdXadnQoUM1b948mUwmGY1Gffvtt4qNjdUjjzyib775Rn5+frrrrrv01FNPyd7evkb7laSSkhKVlJRYvi8oKJAkmUwmmUymyzr22nZu/5daR79wL4V6N1Nq7il9sSVVd/YMqcvyGqXL7TmuHD23LvptffTc+ui59dFz66Lf1nM5PbZZAMvOzlZZWZkCAgIqLQ8ICFBmZma162RmZlY7/syZM8rOzlZQUJAOHTqkFStW6O6779bSpUu1f/9+PfLIIzpz5oxeeOGFGu1XkqZPn66pU6dWWb58+XK5uLhc6mHXqfj4+Ese28PNoNRce82O3y33rJ0yGOqwsEbscnqO2kHPrYt+Wx89tz56bn303Lrod90rLr70uRVsFsDOMfzpX/5ms7nKsouN/+Py8vJy+fv7691335W9vb2io6OVnp6uf/3rX3rhhRdqvN+nn35akyZNsnxfUFCgkJAQxcXFyd3d/SJHWbdMJpPi4+M1ZMgQGY3GS1qn/ymTlv1rlTJPlcsnqo/6RHjXcZWNS016jitDz62LflsfPbc+em599Ny66Lf1nLs77lLYLID5+vrK3t6+ylWnrKysKlenzgkMDKx2vIODg3x8fCRJQUFBMhqNsre3t4yJiopSZmamSktLa7RfSXJycpKTk1OV5Uajsd6c0JdTi4/RqFt7tNTHG1P10aYjGtD+/MeO86tP//83FfTcuui39dFz66Pn1kfPrYt+173L6a/NJuFwdHRUdHR0lUui8fHx6tu3b7XrxMbGVhm/fPlyxcTEWA66X79+OnDggMrLyy1j9u3bp6CgIDk6OtZov43V6L6tJEnxu4/p6AmmpAcAAADqmk1nQZw0aZLef/99zZ8/X0lJSZo4caJSU1M1btw4SRW3/d17772W8ePGjVNKSoomTZqkpKQkzZ8/X/PmzdMTTzxhGfN///d/ysnJ0WOPPaZ9+/bphx9+0Msvv6xHHnnkkvfbVLQLcFPf1j4qN0sfbWj8U/ADAAAAtmbTZ8BGjhypnJwcvfTSS8rIyFCnTp20dOlShYVVTJOekZFR6d1c4eHhWrp0qSZOnKi3335bwcHBmjVrlm677TbLmJCQEC1fvlwTJ05Uly5d1KJFCz322GN66qmnLnm/Tcnovq207mCOPtucqgnXtpWz0f7iKwEAAACoEZtPwjF+/HiNHz++2s8WLFhQZdmgQYO0bdu2C24zNjZWGzZsqPF+m5JrowLUwrOZ0vJO6dvEdN3BlPQAAABAnbHpLYiwPXs7g0bFVlz5W7DusGVWSQAAAAC1jwAGjYwJkZODnXZnFGhLyglblwMAAAA0WgQwyMvVUTd3ayGp4ioYAAAAgLpBAIOk/01J/9PvmcrMP23bYgAAAIBGigAGSVKHYHf1auWtsnKzPt6YYutyAAAAgEaJAAaLc1fBPt2UqpIzZbYtBgAAAGiECGCwiOsYoCAPZ2WfLNUPOzJsXQ4AAADQ6BDAYGG0t9M9fSqmpF/IZBwAAABArSOAoZK/9gyRo4Odth/NV0IqU9IDAAAAtYkAhkp8mjvpxi7BkrgKBgAAANQ2AhiquO/sZBw/7MxQViFT0gMAAAC1hQCGKjq39FCPUE+Zysz6dOMRW5cDAAAANBoEMFTr3JT0H29MUemZctsWAwAAADQSBDBUa1inIPm5OSmrsEQ/7cq0dTkAAABAo0AAQ7UcHex0d+9QSUzGAQAAANQWAhjO667eoTLaG7Q15YR+T8u3dTkAAABAg0cAw3n5uzlreOcgSdICroIBAAAAV4wAhgs6NxnHt9vTlXOyxLbFAAAAAA0cAQwX1D3EU11aeqj0TLk+28yU9AAAAMCVIIDhggwGg0bHtpIkfbwhRWfKmJIeAAAAqCkCGC7qhq5B8nF1VHr+acXvPmbrcgAAAIAGiwCGi3JysNedvSqmpGcyDgAAAKDmCGC4JHf3CZW9nUEbk3OVlFFg63IAAACABokAhksS5NFM13UMlCQtWn/YtsUAAAAADRQBDJfs3JT0XyWkKa+41LbFAAAAAA0QAQyXrGcrL0UFueu0qVz/3cKU9AAAAMDlIoDhkhkMBt3XN0yStGh9isrKzTauCAAAAGhYCGC4LCO6tZCni1FHT5zSij1Zti4HAAAAaFAIYLgszkZ7jewZIklayJT0AAAAwGUhgOGyjeoTJjuD9NuBbB3IKrR1OQAAAECDQQDDZWvp5aJrowIkSQvXpdi4GgAAAKDhIIChRu47OyX9l9uOquC0ybbFAAAAAA0EAQw1EtvaR+0Cmqu4tExfbDlq63IAAACABoEAhhoxGAy6N7aVJGnR+sMqZ0p6AAAA4KIIYKixW7q3kJuzgw7nFGvV/uO2LgcAAACo9whgqDFXJwfdEcOU9AAAAMClIoDhitwbGyaDQfp173ElZxfZuhwAAACgXiOA4YqE+bhqcHt/SRXPggEAAAA4PwIYrtjos1PSf7HlqIpKzti2GAAAAKAeI4Dhig1o46sIX1cVlpzRkm1MSQ8AAACcj80D2OzZsxUeHi5nZ2dFR0drzZo1Fxy/atUqRUdHy9nZWREREZo7d26lzxcsWCCDwVDl6/Tp05YxU6ZMqfJ5YGBgnRxfU2BnZ9C9sWGSpIXrU2Q2MyU9AAAAUB2bBrDFixdrwoQJevbZZ5WQkKABAwZo2LBhSk1NrXZ8cnKyhg8frgEDBighIUHPPPOMHn30UX355ZeVxrm7uysjI6PSl7Ozc6UxHTt2rPT5zp076+w4m4LbolvK1dFeB7JOau2BHFuXAwAAANRLNg1gM2bM0JgxYzR27FhFRUVp5syZCgkJ0Zw5c6odP3fuXIWGhmrmzJmKiorS2LFj9cADD+j111+vNO7cFa0/fv2Zg4NDpc/9/Pzq5BibCjdno26PbilJWsCU9AAAAEC1HGy149LSUm3dulWTJ0+utDwuLk7r1q2rdp3169crLi6u0rKhQ4dq3rx5MplMMhqNkqSTJ08qLCxMZWVl6tatm/7xj3+oe/fuldbbv3+/goOD5eTkpN69e+vll19WRETEeestKSlRSUmJ5fuCggJJkslkkslkuvQDrwPn9m/rOu7q2VIL16folz3HdCgrXyFeLjatpy7Vl543JfTcuui39dFz66Pn1kfPrYt+W8/l9NhmASw7O1tlZWUKCAiotDwgIECZmZnVrpOZmVnt+DNnzig7O1tBQUGKjIzUggUL1LlzZxUUFOjNN99Uv379tH37drVt21aS1Lt3by1atEjt2rXTsWPHNG3aNPXt21e7du2Sj49PtfuePn26pk6dWmX58uXL5eJSP4JGfHy8rUtQpIed9uTbadqnqzWiVbmty6lz9aHnTQ09ty76bX303ProufXRc+ui33WvuLj4ksfaLICdYzAYKn1vNpurLLvY+D8u79Onj/r06WP5vF+/furRo4f+85//aNasWZKkYcOGWT7v3LmzYmNj1bp1ay1cuFCTJk2qdr9PP/10pc8KCgoUEhKiuLg4ubu7X8qh1hmTyaT4+HgNGTLEchXQVpxbH9fDHyVoa56jZl47SM0c7W1aT12pTz1vKui5ddFv66Pn1kfPrY+eWxf9tp5zd8ddCpsFMF9fX9nb21e52pWVlVXlKtc5gYGB1Y53cHA475UrOzs79ezZU/v37z9vLa6ururcufMFxzg5OcnJyanKcqPRWG9O6PpQy7UdghTqvVepucX6YVeW7uwVatN66lp96HlTQ8+ti35bHz23PnpuffTcuuh33buc/tpsEg5HR0dFR0dXuSQaHx+vvn37VrtObGxslfHLly9XTEzMeQ/abDYrMTFRQUFB562lpKRESUlJFxyDS2P/xynp1x1mSnoAAADgD2w6C+KkSZP0/vvva/78+UpKStLEiROVmpqqcePGSaq47e/ee++1jB83bpxSUlI0adIkJSUlaf78+Zo3b56eeOIJy5ipU6dq2bJlOnTokBITEzVmzBglJiZatilJTzzxhFatWqXk5GRt3LhRt99+uwoKCjR69GjrHXwj9peYEDUz2mtPZqHW7M+2dTkAAABAvWHTZ8BGjhypnJwcvfTSS8rIyFCnTp20dOlShYVVXEHJyMio9E6w8PBwLV26VBMnTtTbb7+t4OBgzZo1S7fddptlTF5enh566CFlZmbKw8ND3bt31+rVq9WrVy/LmKNHj+rOO+9Udna2/Pz81KdPH23YsMGyX1wZj2YVU9J/uCFFj3y8TfPv76merbxtXRYAAABgczafhGP8+PEaP358tZ8tWLCgyrJBgwZp27Zt593eG2+8oTfeeOOC+/zss88uq0ZcvqeGRWrvsUJtSs7VqHkb9d69MRrQlnetAQAAoGmz6S2IaLyaOzlo4f29NKidn06byjVmwRYt31X96wUAAACApoIAhjrTzNFe794brWGdAlVaVq7/+3ibvklMs3VZAAAAgM0QwFCnnBzs9Z87u+vWHi1UVm7WhMWJ+mRj6sVXBAAAABohAhjqnIO9nV6/vatG9QmT2Sw989VOvbf6kK3LAgAAAKyOAAarsLMz6KURHTVuUGtJ0j+XJumN+H28JwwAAABNCgEMVmMwGDR5WKT+39D2kqQ3f9mvf/6QRAgDAABAk0EAg9U9MriNXryxgyTp/d+S9cxXO1VWTggDAABA40cAg03c3y9cr93WRXYG6dNNRzTpv4kylZXbuiwAAACgThHAYDN39AzRrDu7y8HOoG8S0zX+4206bSqzdVkAAABAnSGAwaZu6BKsd++NlqODneJ3H9PYhVtUXHrG1mUBAAAAdYIABpu7OjJAC+7vKRdHe/12IFuj5m1S/imTrcsCAAAAah0BDPVC39a++mhsb7k7O2hrygnd9d4G5ZwssXVZAAAAQK0igKHe6BHqpc8eipWPq6N2pRdo5LsbdKzgtK3LAgAAAGoNAQz1Sodgd/13XKyCPJx1IOuk/jJ3vY7kFtu6LAAAAKBWEMBQ77T2a67/PhyrUG8XpeYW6y9z1+tA1klblwUAAABcMQIY6qUQbxd9Pi5Wbf2bK7PgtEa+s1670vNtXRYAAABwRQhgqLcC3J21+OFYdWrhrpyiUt357gZtTTlh67IAAACAGiOAoV7zdnXUJw/2UUyYlwpOn9GoeRu17kC2rcsCAAAAaoQAhnrP3dmoRWN6aUBbXxWXlum+BZv1S9IxW5cFAAAAXDYCGBoEF0cHvT86RnEdAlR6plwPf7hV3+9It3VZAAAAwGUhgKHBcHKw19t399DN3YJ1ptysRz9N0H83H7F1WQAAAMAlI4ChQTHa22nGHd10Z69QlZulJ7/coQ/WJtu6LAAAAOCSEMDQ4NjZGfTyLZ304IBwSdLU73brrRX7ZTabbVwZAAAAcGEEMDRIBoNBzwyP0oRr20qSXl++T6/+tJcQBgAAgHqNAIYGy2AwaMK17fTc9VGSpLmrDuqFb3apvJwQBgAAgPqJAIYGb+yACL18S2cZDNKHG1L0xBfbdaas3NZlAQAAAFUQwNAo3NU7VDNHdpO9nUFLtqXpb58kqORMma3LAgAAACohgKHRGNGthebc3UOO9nb6aVemHlq0VadKCWEAAACoPwhgaFTiOgZq3n0xama016p9xzX6g00qPG2ydVkAAACAJAIYGqEBbf304ZhecnNy0KbkXN3z/kadKCq1dVkAAAAAAQyNU0wrb336UB95uRi1/Wi+/vruBmUVnrZ1WQAAAGjiCGBotDq18NB/H46Vv5uT9h4r1B1z1+voiWJblwUAAIAmjACGRq1tgJu+GNdXLb2a6XBOse6Yu14Hj5+0dVkAAABooghgaPRCfVz0+bhYRfi5Kj3/tG55e61+259t67IAAADQBBHA0CQEeTTTfx+OVY9QTxWcPqPRH2zSh+sP27osAAAANDEEMDQZvs2d9MmDfXRr9xYqKzfr+W926YVvfteZsnJblwYAAIAmggCGJsXZaK9/39FVT10XKYNBWrQ+Rfd9sFn5xbwrDAAAAHWPAIYmx2Aw6P+uaq137omWi6O9fjuQrVtmr9UhJucAAABAHSOAocmK6xioL8b1VQvPZjqUXaSbmZwDAAAAdczmAWz27NkKDw+Xs7OzoqOjtWbNmguOX7VqlaKjo+Xs7KyIiAjNnTu30ucLFiyQwWCo8nX6dOWX8F7uftE4dQh219eP9GNyDgAAAFiFTQPY4sWLNWHCBD377LNKSEjQgAEDNGzYMKWmplY7Pjk5WcOHD9eAAQOUkJCgZ555Ro8++qi+/PLLSuPc3d2VkZFR6cvZ2bnG+0Xj5ufG5BwAAACwDpsGsBkzZmjMmDEaO3asoqKiNHPmTIWEhGjOnDnVjp87d65CQ0M1c+ZMRUVFaezYsXrggQf0+uuvVxpnMBgUGBhY6etK9ovG79zkHE9e157JOQAAAFBnHGy149LSUm3dulWTJ0+utDwuLk7r1q2rdp3169crLi6u0rKhQ4dq3rx5MplMMhqNkqSTJ08qLCxMZWVl6tatm/7xj3+oe/fuNd6vJJWUlKikpMTyfUFBgSTJZDLJZLLtP9LP7d/WdTQGD/YLU7h3Mz3+xU79diBbN7/9m965p7vCfV0rjaPn1kfPrYt+Wx89tz56bn303Lrot/VcTo9tFsCys7NVVlamgICASssDAgKUmZlZ7TqZmZnVjj9z5oyys7MVFBSkyMhILViwQJ07d1ZBQYHefPNN9evXT9u3b1fbtm1rtF9Jmj59uqZOnVpl+fLly+Xi4nKph12n4uPjbV1Co/G3SOm9PfZKzinWiLd+0/3tytXe01xlHD23PnpuXfTb+ui59dFz66Pn1kW/615xcfElj7VZADvHYDBU+t5sNldZdrHxf1zep08f9enTx/J5v3791KNHD/3nP//RrFmzarzfp59+WpMmTbJ8X1BQoJCQEMXFxcnd3f2861mDyWRSfHy8hgwZYrkKiCt3y8kSjf8kUQlH8vXOXgc9P7y97u4dKome2wI9ty76bX303ProufXRc+ui39Zz7u64S2GzAObr6yt7e/sqV52ysrKqXJ06JzAwsNrxDg4O8vHxqXYdOzs79ezZU/v376/xfiXJyclJTk5OVZYbjcZ6c0LXp1oagyAvoz59KFbPLNmpJQlpmvL9Hh3KOaUXbuigc22m59ZHz62LflsfPbc+em599Ny66Hfdu5z+2mwSDkdHR0VHR1e5JBofH6++fftWu05sbGyV8cuXL1dMTMx5D9psNisxMVFBQUE13i+arvNOznGKe6kBAABw+Ww6C+KkSZP0/vvva/78+UpKStLEiROVmpqqcePGSaq47e/ee++1jB83bpxSUlI0adIkJSUlaf78+Zo3b56eeOIJy5ipU6dq2bJlOnTokBITEzVmzBglJiZatnkp+wX+yGAwaPxVbTT3nmi5ONrrtwPZ+ss7G5V1ytaVAQAAoKGx6TNgI0eOVE5Ojl566SVlZGSoU6dOWrp0qcLCwiRJGRkZld7NFR4erqVLl2rixIl6++23FRwcrFmzZum2226zjMnLy9NDDz2kzMxMeXh4qHv37lq9erV69ep1yfsFqjO0Y6C+GNdXYxduVnJOsWbk2atdtxwNigy8+MoAAACA6sEkHOPHj9f48eOr/WzBggVVlg0aNEjbtm077/beeOMNvfHGG1e0X+B8OgS765u/9ddDizYr4Ui+Hli0TVNu7KBRsa1sXRoAAAAaAJveggg0RH5uTvrw/hj19C1XWblZz3+zSy9887vOlJXbujQAAADUcwQwoAacjPa6u025nhjStvLkHMVMzgEAAIDzI4ABNWQwSA8PDK80Occts9fq0PGTti4NAAAA9RQBDLhC5ybnCPZw1qHsIt389lqtPZBt67IAAABQDxHAgFpwbnKOHqGeKjh9RvfO36QPN6TYuiwAAADUMwQwoJb4uTnpkwf76NbuLSom5/j6dybnAAAAQCUEMKAWORvt9e87uurJ69ozOQcAAACqIIABtcxgMGj8VW2YnAMAAABVEMCAOsLkHAAAAPgzAhhQhzoEu+vrv/VTdybnAAAAgAhgQJ3zd3PWpw/20S1MzgEAANDkEcAAK3A22msGk3MAAAA0eQQwwEqYnAMAAAAEMMDKqpuc45ekY7YuCwAAAFZAAANs4M+Tc4xZuEWPfZagnJMlti4NAAAAdYgABtjIuck5HhwQLjuD9E1iuoa8sVrfJKbJbDbbujwAAADUAQIYYEPORns9e30HfTW+nyID3ZRbVKrHPkvUmIVblJ53ytblAQAAoJYRwIB6oGuIp779W389PqSdHO3ttGJPluLeWK0PN6SovJyrYQAAAI0FAQyoJxwd7PT3a9pq6WP91SPUUydLzuj5r3/XX9/dwEyJAAAAjQQBDKhn2vi76fNxfTXlxg5ycbTXpsO5uu7NNZr96wGZeHkzAABAg0YAA+ohezuD7usXruUTB2pgOz+VninXaz/t1Yi31ur3tHxblwcAAIAaIoAB9VhLLxctvL+n/v2XrvJ0MWp3RoFGvL1Wr/60R6dNZbYuDwAAAJeJAAbUcwaDQbdFt1T8xEG6vkuQysrNmvPrQQ17c402HsqxdXkAAAC4DAQwoIHwc3PS23f10LujohXg7qTk7CKNfHeDnvt6pwpPm2xdHgAAAC4BAQxoYOI6Bmr5xEG6s1eIJOmjDamKe2O1Vuw5ZuPKAAAAcDE1CmBHjhzR0aNHLd9v2rRJEyZM0LvvvltrhQE4P49mRk2/tYs+ebC3wnxclJF/Wg8s2KJHP01QzskSW5cHAACA86hRALvrrru0cuVKSVJmZqaGDBmiTZs26ZlnntFLL71UqwUCOL++rX3102MD9dDACNkZpG+3p+vaGav0TWKazGZe4AwAAFDf1CiA/f777+rVq5ck6b///a86deqkdevW6ZNPPtGCBQtqsz4AF9HM0V7PDI/S14/0U2Sgm04Um/TYZ4kas3CL0vNO2bo8AAAA/EGNApjJZJKTk5Mk6eeff9ZNN90kSYqMjFRGRkbtVQfgknVp6alv/9Zfjw9pJ0d7O63Yk6W4N1brww0pKi/nahgAAEB9UKMA1rFjR82dO1dr1qxRfHy8rrvuOklSenq6fHx8arVAAJfO0cFOf7+mrZY+1l/RYV46WXJGz3/9u/767gYdPH7S1uUBAAA0eTUKYK+++qreeecdXXXVVbrzzjvVtWtXSdK3335ruTURgO208XfT5w/HaupNHeXiaK9Nh3M17M01mv3rAZnKym1dHgAAQJPlUJOVrrrqKmVnZ6ugoEBeXl6W5Q899JBcXFxqrTgANWdnZ9Dovq10TZS/nvnqd63ed1yv/bRX32/P0Gu3d1GnFh62LhEAAKDJqdEVsFOnTqmkpMQSvlJSUjRz5kzt3btX/v7+tVoggCvT0stFC+/vqRl3dJWni1G7Mwo04u21euXHPTptKrN1eQAAAE1KjQLYiBEjtGjRIklSXl6eevfurX//+9+6+eabNWfOnFotEMCVMxgMurVHS8VPHKTruwSprNysuasOatiba7TxUI6tywMAAGgyahTAtm3bpgEDBkiSvvjiCwUEBCglJUWLFi3SrFmzarVAALXHz81Jb9/VQ++OilaAu5OSs4s08t0NevarnSo8bbJ1eQAAAI1ejQJYcXGx3NzcJEnLly/XrbfeKjs7O/Xp00cpKSm1WiCA2hfXMVDLJw7Snb1CJEkfb0xV3Bur9UvSMRtXBgAA0LjVKIC1adNGX3/9tY4cOaJly5YpLi5OkpSVlSV3d/daLRBA3fBoZtT0W7vokwd7K8zHRRn5pzVm4RY9+mmCck6W2Lo8AACARqlGAeyFF17QE088oVatWqlXr16KjY2VVHE1rHv37rVaIIC61be1r356bKAeGhghO4P07fZ0XTtjlT7dlKozTFkPAABQq2oUwG6//XalpqZqy5YtWrZsmWX5NddcozfeeKPWigNgHc0c7fXM8Ch9/Ug/RQa66USxSU8v2alrZ6zSN4lpKi8327pEAACARqFGAUySAgMD1b17d6WnpystLU2S1KtXL0VGRtZacQCsq0tLT3339/567vooebs66nBOsR77LFHDZ63R8l2ZMpsJYgAAAFeiRgGsvLxcL730kjw8PBQWFqbQ0FB5enrqH//4h8rLL++WpdmzZys8PFzOzs6Kjo7WmjVrLjh+1apVio6OlrOzsyIiIjR37tzzjv3ss89kMBh08803V1o+ZcoUGQyGSl+BgYGXVTfQWBnt7TR2QIRWPzlYjw9pJzdnB+3JLNRDH27VzbPX6bf92QQxAACAGqpRAHv22Wf11ltv6ZVXXlFCQoK2bduml19+Wf/5z3/0/PPPX/J2Fi9erAkTJujZZ59VQkKCBgwYoGHDhik1NbXa8cnJyRo+fLgGDBighIQEPfPMM3r00Uf15ZdfVhmbkpKiJ554wjJd/p917NhRGRkZlq+dO3dect1AU9DcyUF/v6at1jw5WP93VWs1M9pr+5E83TNvo+58b4O2puTaukQAAIAGx6EmKy1cuFDvv/++brrpJsuyrl27qkWLFho/frz++c9/XtJ2ZsyYoTFjxmjs2LGSpJkzZ2rZsmWaM2eOpk+fXmX83LlzFRoaqpkzZ0qSoqKitGXLFr3++uu67bbbLOPKysp09913a+rUqVqzZo3y8vKqbMvBwYGrXsAl8HRx1FPXRer+fq00e+VBfbIxVRsO5eq2Oet1daS/Ho9rp47BHrYuEwAAoEGoUQDLzc2t9lmvyMhI5eZe2m/FS0tLtXXrVk2ePLnS8ri4OK1bt67addavX2+Z8v6coUOHat68eTKZTDIajZKkl156SX5+fhozZsx5b2ncv3+/goOD5eTkpN69e+vll19WRETEeestKSlRScn/puYuKCiQJJlMJplMtn2B7bn927qOpqQp9tzL2V7PDmun+2ND9Pavh/RlQrpW7MnSij1ZGtYxQI9d00at/VzrbP9Nsee2RL+tj55bHz23PnpuXfTbei6nxzUKYF27dtVbb72lWbNmVVr+1ltvqUuXLpe0jezsbJWVlSkgIKDS8oCAAGVmZla7TmZmZrXjz5w5o+zsbAUFBWnt2rWaN2+eEhMTz7vv3r17a9GiRWrXrp2OHTumadOmqW/fvtq1a5d8fHyqXWf69OmaOnVqleXLly+Xi4vLRY7WOuLj421dQpPTVHvez1Fq20X68YidEnIM+nHXMf20K1M9/cy6rmW5fJzrbt9Ntee2Qr+tj55bHz23PnpuXfS77hUXF1/y2BoFsNdee03XX3+9fv75Z8XGxspgMGjdunU6cuSIli5delnbMhgMlb43m81Vll1s/LnlhYWFuueee/Tee+/J19f3vNsYNmyY5c+dO3dWbGysWrdurYULF2rSpEnVrvP0009X+qygoEAhISGKi4uz+cunTSaT4uPjNWTIEMtVQNQtel7hPkl7Mws185cD+nnPcW06blBCrr3uiG6p/xsUrgD32kti9Ny66Lf10XPro+fWR8+ti35bz7m74y5FjQLYoEGDtG/fPr399tvas2ePzGazbr31Vj300EOaMmXKeSe++CNfX1/Z29tXudqVlZVV5SrXOYGBgdWOd3BwkI+Pj3bt2qXDhw/rxhtvtHx+blZGBwcH7d27V61bt66yXVdXV3Xu3Fn79+8/b71OTk5ycnKqstxoNNabE7o+1dJU0HOpU4i33r+vlxJST+jfy/fptwPZ+njTEX2xLU2j+7bSuEGt5e3qWGv7o+fWRb+tj55bHz23PnpuXfS77l1Of2sUwCQpODi4ymQb27dv18KFCzV//vyLru/o6Kjo6GjFx8frlltusSyPj4/XiBEjql0nNjZW3333XaVly5cvV0xMjIxGoyIjI6vMZvjcc8+psLBQb775pkJCQqrdbklJiZKSki4pOAKoXvdQL300trfWH8zR68v3amvKCb27+pA+2ZiqMf3DNXZAuNyc+eEPAACathoHsNowadIkjRo1SjExMYqNjdW7776r1NRUjRs3TlLFbX9paWlatGiRJGncuHF66623NGnSJD344INav3695s2bp08//VSS5OzsrE6dOlXah6enpyRVWv7EE0/oxhtvVGhoqLKysjRt2jQVFBRo9OjRVjhqoHGLbe2jL8bF6te9x/WvZXu1O6NAb/6yXwvXH9b/DWqte2NbqZmjva3LBAAAsAmbBrCRI0cqJydHL730kjIyMtSpUyctXbpUYWFhkqSMjIxK7wQLDw/X0qVLNXHiRL399tsKDg7WrFmzKk1BfymOHj2qO++8U9nZ2fLz81OfPn20YcMGy34BXBmDwaDBkf4a1M5PP/6eqRnxe3XweJGm/7hH7/+WrL9f3UZ/7RkqR4cavYoQAACgwbJpAJOk8ePHa/z48dV+tmDBgirLBg0apG3btl3y9qvbxmeffXbJ6wOoOTs7g67vEqShHQP0dWK6Zv68T0dPnNIL3+zSO6sO6bFr2+rW7i3kYE8QAwAATcNlBbBbb731gp9X98JjAHCwt9Pt0S11U9dgLd6cqv+sOKC0vFN68osdmrvqoCYNaafhnYJkZ3f+GVABAAAag8sKYB4eHhf9/N57772iggA0Xo4OdhoV20q3R4foww2HNefXgzp0vEh/+yRBUUEH9URcO10d6X/BV1EAAAA0ZJcVwD744IO6qgNAE9LM0V4PDWytO3uFav5vh/XemkNKyijQmIVb1CPUU08Mba++rc//Lj8AAICGigcvANiMm7NRj13bVmueHKyHB0XI2Winbal5uuu9jbr7/Q1KSD1h6xIBAABqFQEMgM15uTrq6WFRWv3/Bmt0bJiM9gatPZCjW2av09iFm5WUcelvlwcAAKjPbD4LIgCc4+/urKkjOunBgRGa9ct+fbH1qH5OytIve7I0vFOguvL6MAAA0MBxBQxAvdPSy0Wv3d5VP08apBu7Bstsln7YmanpifYa/0mitqZwayIAAGiYCGAA6q0Iv+b6z53dtfTRARoS5S+zDIpPytJtc9bpL3PX6ZekYyovN9u6TAAAgEtGAANQ73UIdtfsu7rpmW5ndEd0Czna22nz4RMas3CLhs5crc+3HFHpmXJblwkAAHBRBDAADUZAM+mfN3fUmqcGa9yg1nJzctD+rJP6f1/s0MDXVurd1QdVeNpk6zIBAADOiwAGoMEJcHfW5GGRWvf01Xp6WKQC3J2UWXBaLy/do76vrNCrP+1RVsFpW5cJAABQBQEMQIPl5mzUw4Naa/WTg/Xa7V3Uxr+5Ck+f0ZxfD6r/qys1+csdOnj8pK3LBAAAsCCAAWjwnBzsdUdMiJZPGKj3741RTJiXSsvK9dnmI7p2xio9/OEWbeOlzgAAoB7gPWAAGg07O4Ou7RCgazsEaMvhXL2z+pDidx/Tsl0VX71aeevhQREa3N5fdnYGW5cLAACaIAIYgEYpppW3Ylp560BWod5bnawlCUe16XCuNh3OVbuA5npoYGvd1DVYjg7cCAAAAKyHf3kAaNTa+Lvp1du76LenrtbDgyLk5uSgfcdO6onPt2vgayv13upDOllyxtZlAgCAJoIABqBJCHB31tPDorT27MyJ/m4VMyf+c2mSYqf/otd+2qOsQmZOBAAAdYsABqBJcT87c+Kapwbrtdu6qLWfqwpPn9HsXw+q/ysr9fSSHTrEzIkAAKCOEMAANElODva6o2eI4icO0nv3xij67MyJn246omtmrNK4D7cqgZkTAQBALWMSDgBNmp2dQUM6BGjI2ZkT5646pJ+TjumnXZn6aVemeoV7a9ygCF3VjpkTAQDAlSOAAcBZMa289f7ZmRPfXX1IXyWkaVNyrjYlV8yc+PDA1rqRmRMBAMAV4F8RAPAnbfzd9NrtXbXmyav18MAINT87c+Ljn2/XoH+t1PtrmDkRAADUDAEMAM4j0MNZTw+P0rqnr9bkszMnZuSf1rQfmDkRAADUDAEMAC7C3dmocWdnTnz1ts6K+NPMif/v8+3ak1lg6zIBAEADwDNgAHCJnBzsNbJnqP4SHaKfk47pndWHtDXlhD7felSfbz2qAW19NaZ/uAa185PBwIQdAACgKgIYAFwmOzuD4joGKq5joLalntC835L1484MrdmfrTX7s9XWv7nG9A/Xzd1byNlob+tyAQBAPUIAA4Ar0CPUSz3u8tKR3GItXHdYn20+ov1ZJzV5yU79a9le3dMnTPf0CZOfm5OtSwUAAPUAz4ABQC0I8XbRczd00Pqnr9Zz10ephWcz5RSV6s1f9qvfqyv01Bc7tO9Yoa3LBAAANkYAA4Ba5OZs1NgBEVr1/67S23f1UPdQT5WeKdfiLUcU98Zq3Tt/k1bvOy6z2WzrUgEAgA1wCyIA1AEHeztd3yVI13cJ0taUE5r/W7J+/D1Dq/cd1+p9x9UuoLnG9o/QTd2CeU4MAIAmhAAGAHUsOsxL0WEVz4l9sPawFm9O1b5jJ/Xklzv02rI9lufEfJvznBgAAI0dtyACgJWEeLvohRs7aP0z1+jZ4VEK9nBW9slSzfx5v/q+skKTv9yh/TwnBgBAo0YAAwArc3c26sGBEVr95GD9587u6trSQ6VnyvXZ5iMa8sZqjZ6/SWv285wYAACNEbcgAoCNONjb6cauwbrh7HNi769J1rLdmVq177hW7TuuyEA3PdA/XCO6BcvJgefEAABoDAhgAGBjBoNBMa28FdPKWyk5Rfpg7WH9d8sR7cks1JNf7NBrP+3VvbFhurt3qHx4TgwAgAaNWxABoB4J83HVlJs6av3T1+jpYZEK8nBW9skSzYjfp76vrNDTS3bqQBbPiQEA0FARwACgHvJoZtTDg1pr9ZOD9eZfu6lLSw+VnCnXp5tSde2M1br/g01aeyCb58QAAGhguAURAOoxo72dRnRroZu6Bmvz4ROa99shLd99TCv3HtfKvRXPiY0dEKEbuwbxnBgAAA0AAQwAGgCDwaBe4d7qFe6tw9lFWrDuf8+JPfH5dr360x6Njg3TXb3D5O3qaOtyAQDAedj8FsTZs2crPDxczs7Oio6O1po1ay44ftWqVYqOjpazs7MiIiI0d+7c84797LPPZDAYdPPNN1/xfgGgvmjle/Y5scnXaPKwSAW6O+t4YYleX75PfV/5RU8v2and6QW2LhMAAFTDpgFs8eLFmjBhgp599lklJCRowIABGjZsmFJTU6sdn5ycrOHDh2vAgAFKSEjQM888o0cffVRffvlllbEpKSl64oknNGDAgCveLwDURx4uRo0b1Fprnqp4TqxTC3edNlU8JzZ81hrdNmedvko4qtOmMluXCgAAzrJpAJsxY4bGjBmjsWPHKioqSjNnzlRISIjmzJlT7fi5c+cqNDRUM2fOVFRUlMaOHasHHnhAr7/+eqVxZWVluvvuuzV16lRFRERc8X4BoD4795zYd3/rr8UP9dENXYLkYGfQ1pQTmrh4u/q+skLTf0xSak6xrUsFAKDJs9kzYKWlpdq6dasmT55caXlcXJzWrVtX7Trr169XXFxcpWVDhw7VvHnzZDKZZDQaJUkvvfSS/Pz8NGbMmCq3FtZkv5JUUlKikpISy/cFBRW395hMJplMposcbd06t39b19GU0HPro+eXpkeIu3qEdNYz17XTf7emafGWo8rIP613Vh3Su6sPaWBbX93VK0SD2vrK3s5w3u3Qb+uj59ZHz62PnlsX/baey+mxzQJYdna2ysrKFBAQUGl5QECAMjMzq10nMzOz2vFnzpxRdna2goKCtHbtWs2bN0+JiYm1tl9Jmj59uqZOnVpl+fLly+Xi4nLe9awpPj7e1iU0OfTc+uj5pQuX9P+ipN0nDPot06A9+XZatS9bq/Zly9vJrL4B5erjb5ab8fzboN/WR8+tj55bHz23Lvpd94qLL/0uE5vPgmgwVP4NrNlsrrLsYuPPLS8sLNQ999yj9957T76+vrW636efflqTJk2yfF9QUKCQkBDFxcXJ3d39gvuqayaTSfHx8RoyZIjlKiDqFj23PnpeczdKekpSSk6xPt18RF9uS1fuKZO+T7XXsjSDhnYI0N29QxQd6mn5OUi/rY+eWx89tz56bl3023rO3R13KWwWwHx9fWVvb1/lqlNWVlaVq1PnBAYGVjvewcFBPj4+2rVrlw4fPqwbb7zR8nl5ebkkycHBQXv37lVISMhl71eSnJyc5OTkVGW50WisNyd0faqlqaDn1kfPa65NoIeev9FD/++6KH2/I0MfbUhR4pE8fb8zU9/vzFRkoJvu7hOmW7q3kNPZHtNv66Pn1kfPrY+eWxf9rnuX01+bTcLh6Oio6OjoKpdE4+Pj1bdv32rXiY2NrTJ++fLliomJkdFoVGRkpHbu3KnExETL10033aTBgwcrMTFRISEhNdovADQmzkZ73R7dUl8/0k/f/a2/RsaEyNlopz2ZhXr+69/V+58/68Xvdiu9yNaVAgDQ+Nj0FsRJkyZp1KhRiomJUWxsrN59912lpqZq3Lhxkipu+0tLS9OiRYskSePGjdNbb72lSZMm6cEHH9T69es1b948ffrpp5IkZ2dnderUqdI+PD09JanS8ovtFwCais4tPfTq7V30zPAofbntqD7amKJDx4v0yaajkhz0S/4mjYptpWGdguToYPNXRwIA0ODZNICNHDlSOTk5eumll5SRkaFOnTpp6dKlCgsLkyRlZGRUejdXeHi4li5dqokTJ+rtt99WcHCwZs2apdtuu61W9wsATY2Hi1EP9A/X/f1aaf3BHC1cl6z43ce0JSVPW1IS9Y/muzWyZ4ju7BWqll71Y+IhAAAaIptPwjF+/HiNHz++2s8WLFhQZdmgQYO0bdu2S95+ddu42H4BoKkyGAzq28ZXPcM89MlXS5Xj0V6Ltx7VsYISvb3yoOb8elBXR/rr7j5hGtTWT3YXmMoeAABUZfMABgConzydpLuubq2/X9tOvyQd04cbUrT2QI5+TsrSz0lZCvV20d29Q/WXmBB5uzraulwAABoEAhgA4IKM9na6rlOQrusUpIPHT+rjDan6fOsRpeYWa/qPe/Tv+H26oXOQ7u4Tph5/mMoeAABURQADAFyy1n7N9cKNHfT/hrbXd9vT9eGGFO1My9eShDQtSUhThyB33dMnTCO6BcvVib9iAAD4M6a0AgBctmaO9rqjZ4i++3t/ffNIP90e3VJODnbanVGgZ77aqT4v/6Ip3+7SgaxCW5cKAEC9wq8nAQBXpGuIp7qGeOq566P0xdaj+mhDig7nFGvBusNasO6w+kR4667eYRoSFaBmjva2LhcAAJsigAEAaoWni6PGDojQA/3CtfZgtj5cn6Kfk45pw6FcbTiUK1dHew3tFKhburdQ39a+smcGRQBAE0QAAwDUKjs7gwa09dOAtn5KzzulzzalaklCmo6eOKUl29K0ZFua/NycdGOXYN3cPVidW3gwcQcAoMkggAEA6kywZzNNimuviUPaaWvKCX2VkKYfdmboeGGJ5q9N1vy1yYrwc9XN3VpoRLdghfm42rpkAADqFAEMAFDnDAaDYlp5K6aVt168saNW7zuurxPTFL/7mA4dL9KM+H2aEb9P3UM9dUv3Frq+c5B8mjvZumwAAGodAQwAYFWODna6tkOAru0QoMLTJi3bdUzfJKZp7YFsJaTmKSE1T1O/262BbX11c/cWGtIhQC6O/HUFAGgc+BsNAGAzbs5G3R7dUrdHt1RWwWl9tyND3ySmacfRfK3ce1wr9x6Xi6O9hnYM1IhuwerfxlcO9rxBBQDQcBHAAAD1gr+7s8b0D9eY/uE6ePykvklI09eJ6UrNLdZXCWn6KiFNvs0ddUOXYN3cvYW6tmTyDgBAw0MAAwDUO639mlsm79iWmqdvEtP0/Y4MZZ8stbxfrJWPi0Z0a6Gbu7dQuC+TdwAAGgYCGACg3jIYDIoO81J0mJeev6GDftufra8T07R81zEdzinWm7/s15u/7FfXlh66uXsL3dAlWH5uTN4BAKi/CGAAgAbBaG+nwZH+Ghzpr6KSM1q+O1NfJ6TrtwPZ2n40X9uP5mvaD0nq18ZXt3QPVlyHQLk68dccAKB+4W8mAECD4+rkoFu6t9Qt3VvqeGGJftiRrq8S07X9SJ5W7zuu1fuOq5nxdw3pEKCbuwdrQFs/GZm8AwBQDxDAAAANmp+bk+7rF677+oUrObtI3ySm6ZvEdCVnF+nb7en6dnu6vF0ddUOXII3o1kI9Qj2ZvAMAYDMEMABAoxHu66oJ17bTY9e01faj+fo6IU3f70hX9slSLVqfokXrUxTq7aIR3YI1olsLtfFvbuuSAQBNDAEMANDoGAwGdQvxVLcQTz13fZTWHszRNwlp+mlXplJzi/WfFQf0nxUHFBnopus7B+n6LkGK8COMAQDqHgEMANCoOdjbaVA7Pw1q56dppWcUv/uYvklM1+p9x7Uns1B7Mgv17/h9igpy1/WdAzW8M2EMAFB3CGAAgCbDxdFBI7q10IhuLZRXXKrlu4/phx0ZWnsgW0kZBUrKKNDry/epQ5C7ru8SpOs7B6kV7xgDANQiAhgAoEnydHHUHTEhuiMmpCKM7Tqm73dmaN2BbO3OKNDujAL9a9ledQz+XxgL8yGMAQCuDAEMANDkebo46o6eIbqjZ4hOFJVq+e5Mfb8jQ+sO5mhXeoF2pRfotZ/2qlMLd13fOVjXdw5SqI+LrcsGADRABDAAAP7Ay9VRI3uGamTPUOUWlWr5rkz9sLMijP2eVqDf0wr06k971LmFh+XKWIg3YQwAcGkIYAAAnIe3q6P+2itUf+1VEcaW7crUDzsytO5gtnam5WtnWr5e+XGPurb00PDOQRpOGAMAXAQBDACAS+Dt6qg7e4Xqzl6hyj5ZomW7MrV0Z4bWH8zR9qP52n40X9N/3KOuIZ6W2RRbehHGAACVEcAAALhMvs2ddHfvMN3dO0zZJ0v00+8VV8Y2Judo+5E8bT+Sp5eX7lG3EE/d0CVIwzoHqYVnM1uXDQCoBwhgAABcAd/mTrqnT5ju6ROm44Ul+mlXpn7Yka6NyblKPJKnxCN5mvZDkrqHeur6s7cpBhPGAKDJIoABAFBL/NycNKpPmEb1CVNW4Wkt+71iNsVNh3OVkJqnhNSKMNYj1FPXdwnW8M6BCvIgjAFAU0IAAwCgDvi7OWtUbCuNim2lrILT+vH3itkUNx/O1bbUPG1LzdM/vt+tmDAvDe8cpCFRvrYuGQBgBQQwAADqmL+7s0b3baXRfVvpWMFp/bgzQ0t3ZmpzSq62pJzQlpQTeul7KdzNXkfdkjW0U5Ba+zWXwWCwdekAgFpGAAMAwIoC3J11X79w3dcvXJn5p/Xj7xn6YUeGtqScUHKhQf9avl//Wr5fYT4uuiYyQNd28FfPVt4y2tvZunQAQC0ggAEAYCOBHs66v1+47u8XrtTsQs36cqWyHPy14dAJpeQUa/7aZM1fmyw3Zwdd1d5f10b566p2/vJwMdq6dABADRHAAACoB4I8nDUg0Kzhw6NVUm7Qb/uP6+ekLK3ck6WcolJ9tz1d321Pl72dQTFhXhrSIUDXRAUo3NfV1qUDAC4DAQwAgHqmuZODrusUpOs6Bams3KzEI3n6JemYfk46pn3HTmpjcq42Judq2g9JivBz1bVRAbom0l/RYV5y4FZFAKjXCGAAANRj9nYGRYd5KTrMS09eF6kjucX6OemYfknK0sbkHB06XqR3jx/Su6sPydPFqMHt/XVNlL8GtvOTuzO3KgJAfUMAAwCgAQnxdrE8N1Zw2qTV+47rl6Qsrdybpbxik75KSNNXCWlysDOod4R3xUQeUQEK9XGxdekAABHAAABosNydjbqhS7Bu6BKsM2Xl2pb6v1sVDx4v0toDOVp7IEcvfb9b7QKa65qoAF0b5a9uIV6yt2OKewCwBQIYAACNgIO9nXqFe6tXuLeeHh6l5OwiSxjbfPiE9h07qX3HTmrOrwfl4+pomVVxQDs/NXfinwMAYC38xAUAoBEK93XV2AERGjsgQvnFJv26L8tyq2JOUam+3HZUX247Kkd7O/Vp7aNro/x1TVSAWng2s3XpANCo2XyqpNmzZys8PFzOzs6Kjo7WmjVrLjh+1apVio6OlrOzsyIiIjR37txKny9ZskQxMTHy9PSUq6urunXrpg8//LDSmClTpshgMFT6CgwMrPVjAwCgPvBwMWpEtxaadWd3bXt+iD55sLfG9A9XKx8XlZaVa/W+43rhm13q98oKXTdztf69fK8Sj+SpvNxs69IBoNGx6RWwxYsXa8KECZo9e7b69eund955R8OGDdPu3bsVGhpaZXxycrKGDx+uBx98UB999JHWrl2r8ePHy8/PT7fddpskydvbW88++6wiIyPl6Oio77//Xvfff7/8/f01dOhQy7Y6duyon3/+2fK9vb193R8wAAA2ZrS3U9/Wvurb2lfPXR+lg8eLzs6qeExbU05oT2ah9mQW6j8rDsjPzUlXt/fXoPZ+6tfGVx7NmFURAK6UTQPYjBkzNGbMGI0dO1aSNHPmTC1btkxz5szR9OnTq4yfO3euQkNDNXPmTElSVFSUtmzZotdff90SwK666qpK6zz22GNauHChfvvtt0oBzMHBgateAIAmzWAwqI1/c7Xxb65xg1ort6hUv+6tuFVx1b7jOl5YosVbjmjxliOytzOoW4inBrXz06B2furcwkN2TOQBAJfNZgGstLRUW7du1eTJkystj4uL07p166pdZ/369YqLi6u0bOjQoZo3b55MJpOMxsq/mTObzVqxYoX27t2rV199tdJn+/fvV3BwsJycnNS7d2+9/PLLioiIOG+9JSUlKikpsXxfUFAgSTKZTDKZTBc/4Dp0bv+2rqMpoefWR8+ti35bX33ouZujQTd2DtCNnQNUeqZcmw6f0Kp9x7XmQI4OHi/S1pQT2ppyQjPi98nLxah+rX00sK2v+rfxkZ+bk83qrqn60POmhp5bF/22nsvpsc0CWHZ2tsrKyhQQEFBpeUBAgDIzM6tdJzMzs9rxZ86cUXZ2toKCgiRJ+fn5atGihUpKSmRvb6/Zs2dryJAhlnV69+6tRYsWqV27djp27JimTZumvn37ateuXfLx8al239OnT9fUqVOrLF++fLlcXOrHu1Xi4+NtXUKTQ8+tj55bF/22vvrW8+6SureRckOkPXkGJeUZtC/foBPFJn2/M1Pf76z4O7uFi1mRnmZFeZoV7maWg82fMr909a3nTQE9ty76XfeKi4sveazNZ0E0GCrfvmA2m6ssu9j4Py93c3NTYmKiTp48qV9++UWTJk1SRESE5fbEYcOGWcZ27txZsbGxat26tRYuXKhJkyZVu9+nn3660mcFBQUKCQlRXFyc3N3dL+1g64jJZFJ8fLyGDBlS5Sog6gY9tz56bl302/oaUs9NZeXafjRfq/dna83+HP2eXqC0YoPSig36JV1ydbRXnwhvDWjrqwFtfBTqXT9+UflnDannjQU9ty76bT3n7o67FDYLYL6+vrK3t69ytSsrK6vKVa5zAgMDqx3v4OBQ6cqVnZ2d2rRpI0nq1q2bkpKSNH369CrPh53j6uqqzp07a//+/eet18nJSU5OVW+vMBqN9eaErk+1NBX03ProuXXRb+trCD03GqXYNv6KbeOvp4ZJ2SdL9Nv+7IrbFfcfV/bJUv2y57h+2XNcUsWU+IPa+WlgO1/1ifCRi6PNf/9bSUPoeWNDz62Lfte9y+mvzX4COjo6Kjo6WvHx8brlllssy+Pj4zVixIhq14mNjdV3331Xadny5csVExNzwYM2m82Vnt/6s5KSEiUlJWnAgAGXeRQAAMC3uZNu7t5CN3dvofJys3ZnFGjVvuNate+4tqWcUHJ2kZKzi7Rg3WE52tupZ7jX2ck8/NUuoPkF73wBgMbGpr+CmjRpkkaNGqWYmBjFxsbq3XffVWpqqsaNGyep4ra/tLQ0LVq0SJI0btw4vfXWW5o0aZIefPBBrV+/XvPmzdOnn35q2eb06dMVExOj1q1bq7S0VEuXLtWiRYs0Z84cy5gnnnhCN954o0JDQ5WVlaVp06apoKBAo0ePtm4DAABoZOzsDOrUwkOdWnjokcFtVHjapHUHcyoC2d7jSss7pbUHcrT2QI5eXrpHge7OGtjOV4Pa+at/G195uPBbegCNm00D2MiRI5WTk6OXXnpJGRkZ6tSpk5YuXaqwsDBJUkZGhlJTUy3jw8PDtXTpUk2cOFFvv/22goODNWvWLMsU9JJUVFSk8ePH6+jRo2rWrJkiIyP10UcfaeTIkZYxR48e1Z133qns7Gz5+fmpT58+2rBhg2W/AACgdrg5GzW0Y6CGdgyU2WzWoewirdp7XKv3H9f6gznKLDit/245qv9uOSo7g9QtxFMDz05136Wlp+yZ6h5AI2Pzm7DHjx+v8ePHV/vZggULqiwbNGiQtm3bdt7tTZs2TdOmTbvgPj/77LPLqhEAAFw5g8Gg1n7N1dqvuR7oH67TpjJtSs7V6rO3K+7POqltqXnalpqnmT/vl6eLUf3b+J59fsxPAe7Otj4EALhiNg9gAACgaXI22mvg2XD1nKT0vFNava/i6tia/dnKKzbp+x0Z+n5HhiQpMtDNEsaiw7zkbLS37QEAQA0QwAAAQL0Q7NlMf+0Vqr/2CtWZsnIlHsmzXB3bkZavPZmF2pNZqHdWH5KTg51iWnmpb2tf9Wvjq84tPLhdEUCDQAADAAD1joO9nWJaeSumlbcmxbVXblGp1uw/fnaq+2wdLyyxTObxr2V75ebsoD4RPurX2kf92viqjT+zKwKonwhgAACg3vN2ddSIbi00olsLmc1mHcg6qbUHsrXuYI7WH8pR4ekzit99TPG7j0mS/Nyc1Le1j/q19lXfNj5q6VU/XwYNoOkhgAEAgAbFYDCobYCb2ga46b5+4SorN+v3tHytPZitdQdytPlwro4XluibxHR9k5guSQrzcTl7u6KPYiN85NPcycZHAaCpIoABAIAGzd7OoK4hnuoa4qnxV7XRaVOZtqWe0LoDOVp3MFvbj+YrJadYKTmp+nRTxettooLcFRvuJYcTBg0sOSMvI+8fA2AdBDAAANCoOBvt1be1r/q29pXUXoWnTdqUnKu1ZwPZnsxCJWUUKCmjQJK95r+8Ul1DPNWvtY/6tvFV91BPOTkwwyKAukEAAwAAjZqbs1HXRAXomqgASVL2yRKtO5ij3/Zl6ZffjyqnRNqackJbU05o1ooDcjbaqWcrb/Vr46u+rX3UMZgZFgHUHgIYAABoUnybO+mmrsEa1sFP/RxT1CV2sDal5FmukGWfLNWa/dlasz9bkuTRzKg+EecCma9a+7kywyKAGiOAAQCAJq2lVzOF+7trZM9Qmc1m7Tt2bobFbG08lKv8UyYt23VMy3ZVzLAY4O50dnbFiitkwZ7NbHwEABoSAhgAAMBZBoNB7QPd1D7QTQ/0D9eZsnLtSMvX+oM5WnsgW1tSTuhYQYmWJKRpSUKaJCnc11V9W/uoT4SPekd4y9/N2cZHAaA+I4ABAACch4O9nXqEeqlHqJceGVwxw+LWlBNaeyBbaw/maOfRPCVnFyk5u0gfb6yYYTHC11W9I7zVK9xbvcO5QgagMgIYAADAJXI22qtfG1/1a+MrSco/VTHD4rnbFZMyC3Qou0iHsov06aYjkqQQ72bqHe6j3uHe6hPho5ZezXiGDGjCCGAAAAA15NHMqCEdAjSkQ8UMi/nFJm0+nKuNyTnamJyr39PydST3lI7kHtUXW49KkoI9nNU7wufsFTJvhfsyqQfQlBDAAAAAaomHi1HXdgjQtWcDWeFpk7amnNDG5FxtPJSjHUfzlZ5/Wl8lpOmrs8+Q+bs5VYSxCB/1CfdWG//mBDKgESOAAQAA1BE3Z6Ouau+vq9r7S5KKS89oW0pexRWyQ7lKPJKnrMISfb8jQ9/vyJAk+bg6qlf4/54hiwx0kx3vIQMaDQIYAACAlbg4Oqh/W1/1b1vxDNlpU5kSUvO0KbnitsVtqSeUU1SqH3/P1I+/Z0qquM2xZytv9YmoCGQdgt15MTTQgBHAAAAAbMTZaK/Y1j6Kbe0jqa1Kz5Rrx9E8bUzO1YZDOdqackL5p0z6OemYfk6qeA+Zm5ODYlp5qXdExcQenVp4yGhvZ9sDAXDJCGAAAAD1hKODnWJaeSumlbceGdxGprJy/Z6Wb3mGbMvhEyosOaOVe49r5d7jkiQXR3tFh3mp99nnyLq09JCTg72NjwTA+RDAAAAA6imjvZ26h3qpe6iXxg1qrbJys5IyCrThUMUsi5uSc5V/yqQ1+7O1Zn+2JMnJoeLdZb0jvNWzlbe6hXjK1Yl/8gH1Bf81AgAANBD2dgZ1auGhTi08NHZAhMrLzdp7rFAbD+Vo0+FcbTyUq5yiUq0/lKP1h3Is63QMdld0mJd6tvJWTJiX/N2dbXwkQNNFAAMAAGig7OwMigpyV1SQu+7rFy6z2ayDx09qw6FcbUzO1dbDuUrPP60dR/O142i+Plh7WJIU6u2imDCvs7c7eqmNX3NmWgSshAAGAADQSBgMBrXxd1Mbfzfd0ydMkpSWd0pbDudqa8oJbT58QnsyC5SaW6zU3GItOfsuMo9mRkWHeSmmlZdiwrzVpaWHnI08RwbUBQIYAABAI9bCs5ladGuhEd1aSJIKTpuUkJqnLYdzteXwCSUcqZhpccWeLK3YkyVJcrS3U+eWHparZNFhXvJ2dbTlYQCNBgEMAACgCXF3NmpQOz8NaucnSTKVlWt3eoE2/+EqWfbJEm1NOaGtKSf0zupDkqTWfq6KCau4ZTGmlbda+bjIYOC2ReByEcAAAACaMKO9nbqGeKpriKfGDpDMZrNSc4u1+fAJbU3J1ebDJ3Qg66QOHi/SweNFWrzliCTJt7nj/yb2aOWtjsHuvI8MuAQEMAAAAFgYDAaF+bgqzMdVt0e3lCSdKCqtuDqWkquth09ox9F8ZZ8s1bJdx7RsV8ULop2NduoW4mm5StYjzEvuzkZbHgpQLxHAAAAAcEFero66tkOAru0QIEk6bSrT72n5lqtkW1JOKK/YpA2HcrXhUK4kyWCQ2ge4KaZVxVWy6DAv+bvyT0+A/woAAABwWZyN9mensPeW1Frl5RXT329JOWF5liwlp1h7Mgu1J7NQH21IlSQFujsp0GinDI/Dimnlo04tmG0RTQ8BDAAAAFfEzs6gtgFuahvgpjt7hUqSsgpPa+vhE5arZL+nFyizoESZslPiT/skSQ52BkUGual7iJe6hXiqe6inwn1dmdwDjRoBDAAAALXO381ZwzoHaVjnIElScekZbU3O0eJfNuq0S6ASjxYo+2SJfk8r0O9pBfpwQ4qkineSdQvxtASybiGe8nRhCnw0HgQwAAAA1DkXRwf1ifBW7h6zhg/vLgcHB6XlnVLikTwlpOYp8UiedqblK/+USav2Hdeqfcct64b7uqp7iKe6hXqqe4iXIoPcmHERDRYBDAAAAFZnMBjU0stFLb1cdEOXYElS6Zly7ckssASyhNQTOpxTrOTsIiVnF2lJQpokycnBTp1beJy9SualbqGeCvZw5tZFNAgEMAAAANQLjg526tLSU11aemr02WUnikqVePR/V8kSU0+o4PQZbUk5oS0pJyQlS5L83Zz+F8hCPNWlpYdcnfinLuofzkoAAADUW16ujhrc3l+D2/tLksrLzUrOKTobyE4oITVPezILlVVYouW7j2n57or3ktkZpHYBbup+9rbFbqGeauPXXHZ2XCWDbRHAAAAA0GDY2RnU2q+5Wvs1t7wo+lRpmXam5VsCWeKRPGXkn7ZMg//ppiOSJDcnB3UJ8bDMutgt1FO+zZ1seThogghgAAAAaNCaOdqrV7i3eoV7W5Zl5p+uCGRnJ/nYeTRfhSVntPZAjtYeyLGMC/Fupm7nAlmIpzoGu/NuMtQpAhgAAAAanUAPZ13nEaTrOlVMg3+mrFx7jxVWmnXxQNZJHck9pSO5p/Td9nRJFe8miwpytwSybqGeCvdx5dZF1BoCGAAAABo9B3s7dQz2UMdgD93dO0ySlH/KpB1H85R4boKPI3nKKSrVzrR87UzLt7ybzN3ZQV1DPC1T4Xdt6Skfbl1EDdn8BQqzZ89WeHi4nJ2dFR0drTVr1lxw/KpVqxQdHS1nZ2dFRERo7ty5lT5fsmSJYmJi5OnpKVdXV3Xr1k0ffvjhFe8XAAAAjYtHM6MGtPXT369pq3n39dSW567VmicH6z93dteY/uGKDvOSk4OdCk6f0Zr92Zq14oAeWLBF0dN+1oDXVujvnyZo3m/J2ppyQqdNZbY+HDQQNr0CtnjxYk2YMEGzZ89Wv3799M4772jYsGHavXu3QkNDq4xPTk7W8OHD9eCDD+qjjz7S2rVrNX78ePn5+em2226TJHl7e+vZZ59VZGSkHB0d9f333+v++++Xv7+/hg4dWqP9AgAAoPEzGAwK8XZRiLeLbuxa8W4yU1m59mQUWp4n234kTwePF1W5ddFo/79bF7u25NZFnJ9NA9iMGTM0ZswYjR07VpI0c+ZMLVu2THPmzNH06dOrjJ87d65CQ0M1c+ZMSVJUVJS2bNmi119/3RLArrrqqkrrPPbYY1q4cKF+++03SwC73P0CAACgaTLa26lzSw91bumhUbEVy8536+KOo/nacTRfErcu4vxsFsBKS0u1detWTZ48udLyuLg4rVu3rtp11q9fr7i4uErLhg4dqnnz5slkMsloNFb6zGw2a8WKFdq7d69effXVGu9XkkpKSlRSUmL5vqCgQJJkMplkMpkucrR169z+bV1HU0LPrY+eWxf9tj56bn303PoaS89dHKQ+rTzVp5WnpIp/cx7NO6UdRwu0/Wi+th/N1670Asuti2v2Z1vWDfFqpi4tPdS1pYe6tfRQhyA3OdXRrIuNpd8NweX02GYBLDs7W2VlZQoICKi0PCAgQJmZmdWuk5mZWe34M2fOKDs7W0FBFbPc5Ofnq0WLFiopKZG9vb1mz56tIUOG1Hi/kjR9+nRNnTq1yvLly5fLxcXl4gdsBfHx8bYuocmh59ZHz62LflsfPbc+em59jbXnBkndJHVrIZUFSenF0uGTBqWcNCj1pEHHThl05MQpHTlxSj/srPh3p73BrGAXqVVzs8LczAprbpavc8WLpGtLY+13fVJcXHzJY20+C6LBUPnsMpvNVZZdbPyfl7u5uSkxMVEnT57UL7/8okmTJikiIqLS7YmXu9+nn35akyZNsnxfUFCgkJAQxcXFyd3d/fwHaAUmk0nx8fEaMmRIlauAqBv03ProuXXRb+uj59ZHz62vqfe84JRJO9LOXSXL0/aj+cotMulIkXSkyKA1xyrGeTRzUMcgd0UFuSkq0E0dgtwV7usiB/vLmz+vqffbms7dHXcpbBbAfH19ZW9vX+WqU1ZWVpWrU+cEBgZWO97BwUE+Pj6WZXZ2dmrTpo0kqVu3bkpKStL06dN11VVX1Wi/kuTk5CQnp6r37BqNxnpzQtenWpoKem599Ny66Lf10XPro+fW11R77mM0arC7iwZHBUo6e+viiVOW58gSj+Tp97R85Z86o3WHcrXuUK5lXUcHO0WeDWMdgt3VIchdUUHucnW6+D/nm2q/rely+muzAObo6Kjo6GjFx8frlltusSyPj4/XiBEjql0nNjZW3333XaVly5cvV0xMzAUP2mw2W57fqsl+AQAAgNp2vlkX92YWald6vnanF2h3RoF2pxeoqLTsD5N8nFtfauXjWimUdQx2l5+b0wXv7IJt2fQWxEmTJmnUqFGKiYlRbGys3n33XaWmpmrcuHGSKm77S0tL06JFiyRJ48aN01tvvaVJkybpwQcf1Pr16zVv3jx9+umnlm1Onz5dMTExat26tUpLS7V06VItWrRIc+bMueT9AgAAALZgtLdTpxYe6tTCw7KsvNys1NxiSxg797+ZBaeVnF2k5Owi/bAzwzLet7mjooLcFRnQXCXZBrU/XqS2gR6yZ0r8esGmAWzkyJHKycnRSy+9pIyMDHXq1ElLly5VWFjF28kzMjKUmppqGR8eHq6lS5dq4sSJevvttxUcHKxZs2ZZpqCXpKKiIo0fP15Hjx5Vs2bNFBkZqY8++kgjR4685P0CAAAA9YWdnUGtfF3VytdVwzsHWZZnnyxR0p9C2cHjJ5V9svQPsy/aa9H+tXI22iky8H9XyjoEuysq0F3NHOtmBkacn80n4Rg/frzGjx9f7WcLFiyosmzQoEHatm3bebc3bdo0TZs27Yr2CwAAANR3vs2dNKCtnwa09bMsO1Vapr3HCrU7vUC/p53Qut1HdKzEXqdM5ZbnzM6xM0jhvq7qEOxR6TZGPzfeVVaXbB7AAAAAANSOZo726hbiqW4hnjKZgrTU/rCGXhentILSSlfKdmcU6HhhiQ4eL9LB40X6bnu6ZRv+bk6VrpR1CHJXKx9X2XELY60ggAEAAACNmL2dQa39mqu1X3PLZB+SlFV4ukooS84uUlZhibL2Hteve49bxro42isq6H8TfXQM9lDbgOZyrqOXSDdmBDAAAACgCfJ3c5Z/e2dd1d7fsqy49Iz2ZBZqV/r/QtmejAIVl5Zpa8oJbU05YRnrYGdQG//m6nA2kJ27YubRjCnvL4QABgAAAECS5OLooB6hXuoR6mVZdqasXMnZRdqdUaBd6QXalZ6vXekFyis2aU9mofZkFmrJtjTL+BDvZmevlHlYrpYFuDM1/jkEMAAAAADn5WBvp7YBbmob4KYR3VpIqnjPbkb+aUsg251eEc7S8k7pSG7F17Jdxyzb8HZ1VMfgP76vzEPhvq5Ncmp8AhgAAACAy2IwGBTs2UzBns00pEOAZXlecanlmbJz4ezg8SLlFv1xavwKzYz2igpys9zC2DHYXe0C3Br9c2UEMAAAAAC1wtPFUX1b+6pva1/LstOmMu0991xZRsXti3syCnXKVKZtqXnalppnGWtvZ1Abv+b/u1oW7K6OQR7ycGk8z5URwAAAAADUGWejvbqGeKpriKdlWVm5WcnZRZVuX9yVnq8TxSbtPVaovccKtSThf8+VtfSq/FxZh2B3BXk4N8jnyghgAAAAAKzK/uwMim38m1d6riyz4LR2pRWcnfCj4mrZ0ROnLF/Ld//vuTIvF6M6Bntozj095ObccK6QEcAAAAAA2JzBYFCQRzMFeTTTtX94riy/2GQJZOemxt+fdVInik3amZav5k4NK9I0rGoBAAAANCkeLkbFtvZRbGsfy7LTpjLtO1ao44UlDe42RAIYAAAAgAbF2WivLi09bV1GjdjZugAAAAAAaCoIYAAAAABgJQQwAAAAALASAhgAAAAAWAkBDAAAAACshAAGAAAAAFZCAAMAAAAAKyGAAQAAAICVEMAAAAAAwEoIYAAAAABgJQQwAAAAALASAhgAAAAAWAkBDAAAAACshAAGAAAAAFbiYOsCGiqz2SxJKigosHElkslkUnFxsQoKCmQ0Gm1dTpNAz62PnlsX/bY+em599Nz66Ll10W/rOZcJzmWECyGA1VBhYaEkKSQkxMaVAAAAAKgPCgsL5eHhccExBvOlxDRUUV5ervT0dLm5uclgMNi0loKCAoWEhOjIkSNyd3e3aS1NBT23PnpuXfTb+ui59dFz66Pn1kW/rcdsNquwsFDBwcGys7vwU15cAashOzs7tWzZ0tZlVOLu7s5/XFZGz62PnlsX/bY+em599Nz66Ll10W/ruNiVr3OYhAMAAAAArIQABgAAAABWQgBrBJycnPTiiy/KycnJ1qU0GfTc+ui5ddFv66Pn1kfPrY+eWxf9rp+YhAMAAAAArIQrYAAAAABgJQQwAAAAALASAhgAAAAAWAkBDAAAAACshADWQMyePVvh4eFydnZWdHS01qxZc8Hxq1atUnR0tJydnRUREaG5c+daqdKGb/r06erZs6fc3Nzk7++vm2++WXv37r3gOr/++qsMBkOVrz179lip6oZtypQpVXoXGBh4wXU4x69Mq1atqj1nH3nkkWrHc45fntWrV+vGG29UcHCwDAaDvv7660qfm81mTZkyRcHBwWrWrJmuuuoq7dq166Lb/fLLL9WhQwc5OTmpQ4cO+uqrr+roCBqeC/XcZDLpqaeeUufOneXq6qrg4GDde++9Sk9Pv+A2FyxYUO15f/r06To+mobhYuf5fffdV6V3ffr0ueh2Oc+rd7F+V3euGgwG/etf/zrvNjnHbYMA1gAsXrxYEyZM0LPPPquEhAQNGDBAw4YNU2pqarXjk5OTNXz4cA0YMEAJCQl65pln9Oijj+rLL7+0cuUN06pVq/TII49ow4YNio+P15kzZxQXF6eioqKLrrt3715lZGRYvtq2bWuFihuHjh07Vurdzp07zzuWc/zKbd68uVK/4+PjJUl/+ctfLrge5/ilKSoqUteuXfXWW29V+/lrr72mGTNm6K233tLmzZsVGBioIUOGqLCw8LzbXL9+vUaOHKlRo0Zp+/btGjVqlO644w5t3Lixrg6jQblQz4uLi7Vt2zY9//zz2rZtm5YsWaJ9+/bppptuuuh23d3dK53zGRkZcnZ2rotDaHAudp5L0nXXXVepd0uXLr3gNjnPz+9i/f7zeTp//nwZDAbddtttF9wu57gNmFHv9erVyzxu3LhKyyIjI82TJ0+udvyTTz5pjoyMrLTs4YcfNvfp06fOamzMsrKyzJLMq1atOu+YlStXmiWZT5w4Yb3CGpEXX3zR3LVr10sezzle+x577DFz69atzeXl5dV+zjlec5LMX331leX78vJyc2BgoPmVV16xLDt9+rTZw8PDPHfu3PNu54477jBfd911lZYNHTrU/Ne//rXWa27o/tzz6mzatMksyZySknLeMR988IHZw8OjdotrpKrr+ejRo80jRoy4rO1wnl+aSznHR4wYYb766qsvOIZz3Da4AlbPlZaWauvWrYqLi6u0PC4uTuvWrat2nfXr11cZP3ToUG3ZskUmk6nOam2s8vPzJUne3t4XHdu9e3cFBQXpmmuu0cqVK+u6tEZl//79Cg4OVnh4uP7617/q0KFD5x3LOV67SktL9dFHH+mBBx6QwWC44FjO8SuXnJyszMzMSuewk5OTBg0adN6f69L5z/sLrYPzy8/Pl8FgkKen5wXHnTx5UmFhYWrZsqVuuOEGJSQkWKfARuLXX3+Vv7+/2rVrpwcffFBZWVkXHM95XjuOHTumH374QWPGjLnoWM5x6yOA1XPZ2dkqKytTQEBApeUBAQHKzMysdp3MzMxqx585c0bZ2dl1VmtjZDabNWnSJPXv31+dOnU677igoCC9++67+vLLL7VkyRK1b99e11xzjVavXm3Fahuu3r17a9GiRVq2bJnee+89ZWZmqm/fvsrJyal2POd47fr666+Vl5en++6777xjOMdrz7mf3Zfzc/3cepe7Dqp3+vRpTZ48WXfddZfc3d3POy4yMlILFizQt99+q08//VTOzs7q16+f9u/fb8VqG65hw4bp448/1ooVK/Tvf/9bmzdv1tVXX62SkpLzrsN5XjsWLlwoNzc33XrrrRccxzluGw62LgCX5s+/lTabzRf8TXV146tbjgv729/+ph07dui333674Lj27durffv2lu9jY2N15MgRvf766xo4cGBdl9ngDRs2zPLnzp07KzY2Vq1bt9bChQs1adKkatfhHK898+bN07BhwxQcHHzeMZzjte9yf67XdB1UZjKZ9Ne//lXl5eWaPXv2Bcf26dOn0qQR/fr1U48ePfSf//xHs2bNqutSG7yRI0da/typUyfFxMQoLCxMP/zwwwWDAef5lZs/f77uvvvuiz7LxTluG1wBq+d8fX1lb29f5Tc/WVlZVX5DdE5gYGC14x0cHOTj41NntTY2f//73/Xtt99q5cqVatmy5WWv36dPH36DVEOurq7q3LnzefvHOV57UlJS9PPPP2vs2LGXvS7neM2cm+Hzcn6un1vvctdBZSaTSXfccYeSk5MVHx9/watf1bGzs1PPnj0572soKChIYWFhF+wf5/mVW7Nmjfbu3Vujn+uc49ZBAKvnHB0dFR0dbZmh7Jz4+Hj17du32nViY2OrjF++fLliYmJkNBrrrNbGwmw2629/+5uWLFmiFStWKDw8vEbbSUhIUFBQUC1X1zSUlJQoKSnpvP3jHK89H3zwgfz9/XX99ddf9rqc4zUTHh6uwMDASudwaWmpVq1add6f69L5z/sLrYP/ORe+9u/fr59//rlGv6wxm81KTEzkvK+hnJwcHTly5IL94zy/cvPmzVN0dLS6du162etyjluJrWb/wKX77LPPzEaj0Txv3jzz7t27zRMmTDC7urqaDx8+bDabzebJkyebR40aZRl/6NAhs4uLi3nixInm3bt3m+fNm2c2Go3mL774wlaH0KD83//9n9nDw8P866+/mjMyMixfxcXFljF/7vkbb7xh/uqrr8z79u0z//777+bJkyebJZm//PJLWxxCg/P444+bf/31V/OhQ4fMGzZsMN9www1mNzc3zvE6VlZWZg4NDTU/9dRTVT7jHL8yhYWF5oSEBHNCQoJZknnGjBnmhIQEy4x7r7zyitnDw8O8ZMkS886dO8133nmnOSgoyFxQUGDZxqhRoyrNdrt27Vqzvb29+ZVXXjEnJSWZX3nlFbODg4N5w4YNVj+++uhCPTeZTOabbrrJ3LJlS3NiYmKln+0lJSWWbfy551OmTDH/9NNP5oMHD5oTEhLM999/v9nBwcG8ceNGWxxivXOhnhcWFpoff/xx87p168zJycnmlStXmmNjY80tWrTgPK+hi/1cMZvN5vz8fLOLi4t5zpw51W6Dc7x+IIA1EG+//bY5LCzM7OjoaO7Ro0elKdFHjx5tHjRoUKXxv/76q7l79+5mR0dHc6tWrc77HyKqklTt1wcffGAZ8+eev/rqq+bWrVubnZ2dzV5eXub+/fubf/jhB+sX30CNHDnSHBQUZDYajebg4GDzrbfeat61a5flc87xurFs2TKzJPPevXurfMY5fmXOTdv/56/Ro0ebzeaKqehffPFFc2BgoNnJyck8cOBA886dOyttY9CgQZbx53z++efm9u3bm41GozkyMpIA/AcX6nlycvJ5f7avXLnSso0/93zChAnm0NBQs6Ojo9nPz88cFxdnXrdunfUPrp66UM+Li4vNcXFxZj8/P7PRaDSHhoaaR48ebU5NTa20Dc7zS3exnytms9n8zjvvmJs1a2bOy8urdhuc4/WDwWw+++Q6AAAAAKBO8QwYAAAAAFgJAQwAAAAArIQABgAAAABWQgADAAAAACshgAEAAACAlRDAAAAAAMBKCGAAAAAAYCUEMAAAAACwEgIYAAA2YDAY9PXXX9u6DACAlRHAAABNzn333SeDwVDl67rrrrN1aQCARs7B1gUAAGAL1113nT744INKy5ycnGxUDQCgqeAKGACgSXJyclJgYGClLy8vL0kVtwfOmTNHw4YNU7NmzRQeHq7PP/+80vo7d+7U1VdfrWbNmsnHx0cPPfSQTp48WWnM/Pnz1bFjRzk5OSkoKEh/+9vfKn2enZ2tW265RS4uLmrbtq2+/fbbuj1oAIDNEcAAAKjG888/r9tuu03bt2/XPffcozvvvFNJSUmSpOLiYl133XXy8vLS5s2b9fnnn+vnn3+uFLDmzJmjRx55RA899JB27typb7/9Vm3atKm0j6lTp+qOO+7Qjh07NHz4cN19993Kzc216nECAKzLYDabzbYuAgAAa7rvvvv00UcfydnZudLyp556Ss8//7wMBoPGjRunOXPmWD7r06ePevToodmzZ+u9997TU089pSNHjsjV1VWStHTpUt14441KT09XQECAWrRoofvvv1/Tpk2rtgaDwaDnnntO//jHPyRJRUVFcnNz09KlS3kWDQAaMZ4BAwA0SYMHD64UsCTJ29vb8ufY2NhKn8XGxioxMVGSlJSUpK5du1rClyT169dP5eXl2rt3rwwGg9LT03XNNddcsIYuXbpY/uzq6io3NzdlZWXV9JAAAA0AAQwA0CS5urpWuSXwYgwGgyTJbDZb/lzdmGbNml3S9oxGY5V1y8vLL6smAEDDwjNgAABUY8OGDVW+j4yMlCR16NBBiYmJKioqsny+du1a2dnZqV27dnJzc1OrVq30yy+/WLVmAED9xxUwAECTVFJSoszMzErLHBwc5OvrK0n6/PPPFRMTo/79++vjjz/Wpk2bNG/ePEnS3XffrRdffFGjR4/WlClTdPz4cf3973/XqFGjFBAQIEmaMmWKxo0bJ39/fw0bNkyFhYVau3at/v73v1v3QAEA9QoBDADQJP30008KCgqqtKx9+/bas2ePpIoZCj/77DONHz9egYGB+vjjj9WhQwdJkouLi5YtW6bHHntMPXv2lIuLi2677TbNmDHDsq3Ro0fr9OnTeuONN/TEE0/I19dXt99+u/UOEABQLzELIgAAf2IwGPTVV1/p5ptvtnUpAIBGhmfAAAAAAMBKCGAAAAAAYCU8AwYAwJ9wdz4AoK5wBQwAAAAArIQABgAAAABWQgADAAAAACshgAEAAACAlRDAAAAAAMBKCGAAAAAAYCUEMAAAAACwEgIYAAAAAFjJ/wcYhrW68TYDpgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: 0.0284\n"
     ]
    }
   ],
   "source": [
    "# Plot training history\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final loss: {history.history['loss'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BUILDING BINARY DESCRIPTOR DATABASE\n",
      "======================================================================\n",
      "\n",
      "Creating float model for re-ranking...\n",
      "\n",
      "Model: mobilenetv3small + NetVLAD(64)\n",
      "  Output dimension: 36864\n",
      "Trainable params: 73,792\n",
      "Copying trained weights from binary model...\n",
      "  ✓ Copied weights: MobileNetV3Small\n",
      "  ✓ Copied weights: netvlad\n",
      "\n",
      "Extracting binary codes for training images...\n",
      "  Processed 10/1757\n",
      "  Processed 20/1757\n",
      "  Processed 30/1757\n",
      "  Processed 40/1757\n",
      "  Processed 50/1757\n",
      "  Processed 60/1757\n",
      "  Processed 70/1757\n",
      "  Processed 80/1757\n",
      "  Processed 90/1757\n",
      "  Processed 100/1757\n",
      "  Processed 110/1757\n",
      "  Processed 120/1757\n",
      "  Processed 130/1757\n",
      "  Processed 140/1757\n",
      "  Processed 150/1757\n",
      "  Processed 160/1757\n",
      "  Processed 170/1757\n",
      "  Processed 180/1757\n",
      "  Processed 190/1757\n",
      "  Processed 200/1757\n",
      "  Processed 210/1757\n",
      "  Processed 220/1757\n",
      "  Processed 230/1757\n",
      "  Processed 240/1757\n",
      "  Processed 250/1757\n",
      "  Processed 260/1757\n",
      "  Processed 270/1757\n",
      "  Processed 280/1757\n",
      "  Processed 290/1757\n",
      "  Processed 300/1757\n",
      "  Processed 310/1757\n",
      "  Processed 320/1757\n",
      "  Processed 330/1757\n",
      "  Processed 340/1757\n",
      "  Processed 350/1757\n",
      "  Processed 360/1757\n",
      "  Processed 370/1757\n",
      "  Processed 380/1757\n",
      "  Processed 390/1757\n",
      "  Processed 400/1757\n",
      "  Processed 410/1757\n",
      "  Processed 420/1757\n",
      "  Processed 430/1757\n",
      "  Processed 440/1757\n",
      "  Processed 450/1757\n",
      "  Processed 460/1757\n",
      "  Processed 470/1757\n",
      "  Processed 480/1757\n",
      "  Processed 490/1757\n",
      "  Processed 500/1757\n",
      "  Processed 510/1757\n",
      "  Processed 520/1757\n",
      "  Processed 530/1757\n",
      "  Processed 540/1757\n",
      "  Processed 550/1757\n",
      "  Processed 560/1757\n",
      "  Processed 570/1757\n",
      "  Processed 580/1757\n",
      "  Processed 590/1757\n",
      "  Processed 600/1757\n",
      "  Processed 610/1757\n",
      "  Processed 620/1757\n",
      "  Processed 630/1757\n",
      "  Processed 640/1757\n",
      "  Processed 650/1757\n",
      "  Processed 660/1757\n",
      "  Processed 670/1757\n",
      "  Processed 680/1757\n",
      "  Processed 690/1757\n",
      "  Processed 700/1757\n",
      "  Processed 710/1757\n",
      "  Processed 720/1757\n",
      "  Processed 730/1757\n",
      "  Processed 740/1757\n",
      "  Processed 750/1757\n",
      "  Processed 760/1757\n",
      "  Processed 770/1757\n",
      "  Processed 780/1757\n",
      "  Processed 790/1757\n",
      "  Processed 800/1757\n",
      "  Processed 810/1757\n",
      "  Processed 820/1757\n",
      "  Processed 830/1757\n",
      "  Processed 840/1757\n",
      "  Processed 850/1757\n",
      "  Processed 860/1757\n",
      "  Processed 870/1757\n",
      "  Processed 880/1757\n",
      "  Processed 890/1757\n",
      "  Processed 900/1757\n",
      "  Processed 910/1757\n",
      "  Processed 920/1757\n",
      "  Processed 930/1757\n",
      "  Processed 940/1757\n",
      "  Processed 950/1757\n",
      "  Processed 960/1757\n",
      "  Processed 970/1757\n",
      "  Processed 980/1757\n",
      "  Processed 990/1757\n",
      "  Processed 1000/1757\n",
      "  Processed 1010/1757\n",
      "  Processed 1020/1757\n",
      "  Processed 1030/1757\n",
      "  Processed 1040/1757\n",
      "  Processed 1050/1757\n",
      "  Processed 1060/1757\n",
      "  Processed 1070/1757\n",
      "  Processed 1080/1757\n",
      "  Processed 1090/1757\n",
      "  Processed 1100/1757\n",
      "  Processed 1110/1757\n",
      "  Processed 1120/1757\n",
      "  Processed 1130/1757\n",
      "  Processed 1140/1757\n",
      "  Processed 1150/1757\n",
      "  Processed 1160/1757\n",
      "  Processed 1170/1757\n",
      "  Processed 1180/1757\n",
      "  Processed 1190/1757\n",
      "  Processed 1200/1757\n",
      "  Processed 1210/1757\n",
      "  Processed 1220/1757\n",
      "  Processed 1230/1757\n",
      "  Processed 1240/1757\n",
      "  Processed 1250/1757\n",
      "  Processed 1260/1757\n",
      "  Processed 1270/1757\n",
      "  Processed 1280/1757\n",
      "  Processed 1290/1757\n",
      "  Processed 1300/1757\n",
      "  Processed 1310/1757\n",
      "  Processed 1320/1757\n",
      "  Processed 1330/1757\n",
      "  Processed 1340/1757\n",
      "  Processed 1350/1757\n",
      "  Processed 1360/1757\n",
      "  Processed 1370/1757\n",
      "  Processed 1380/1757\n",
      "  Processed 1390/1757\n",
      "  Processed 1400/1757\n",
      "  Processed 1410/1757\n",
      "  Processed 1420/1757\n",
      "  Processed 1430/1757\n",
      "  Processed 1440/1757\n",
      "  Processed 1450/1757\n",
      "  Processed 1460/1757\n",
      "  Processed 1470/1757\n",
      "  Processed 1480/1757\n",
      "  Processed 1490/1757\n",
      "  Processed 1500/1757\n",
      "  Processed 1510/1757\n",
      "  Processed 1520/1757\n",
      "  Processed 1530/1757\n",
      "  Processed 1540/1757\n",
      "  Processed 1550/1757\n",
      "  Processed 1560/1757\n",
      "  Processed 1570/1757\n",
      "  Processed 1580/1757\n",
      "  Processed 1590/1757\n",
      "  Processed 1600/1757\n",
      "  Processed 1610/1757\n",
      "  Processed 1620/1757\n",
      "  Processed 1630/1757\n",
      "  Processed 1640/1757\n",
      "  Processed 1650/1757\n",
      "  Processed 1660/1757\n",
      "  Processed 1670/1757\n",
      "  Processed 1680/1757\n",
      "  Processed 1690/1757\n",
      "  Processed 1700/1757\n",
      "  Processed 1710/1757\n",
      "  Processed 1720/1757\n",
      "  Processed 1730/1757\n",
      "  Processed 1740/1757\n",
      "  Processed 1750/1757\n",
      "\n",
      "✓ Binary descriptor database built\n",
      "  Shape: (1757, 512)\n",
      "  Storage: 878.50 KB\n",
      "\n",
      "Extracting float descriptors for re-ranking...\n",
      "  Processed 10/1757\n",
      "  Processed 20/1757\n",
      "  Processed 30/1757\n",
      "  Processed 40/1757\n",
      "  Processed 50/1757\n",
      "  Processed 60/1757\n",
      "  Processed 70/1757\n",
      "  Processed 80/1757\n",
      "  Processed 90/1757\n",
      "  Processed 100/1757\n",
      "  Processed 110/1757\n",
      "  Processed 120/1757\n",
      "  Processed 130/1757\n",
      "  Processed 140/1757\n",
      "  Processed 150/1757\n",
      "  Processed 160/1757\n",
      "  Processed 170/1757\n",
      "  Processed 180/1757\n",
      "  Processed 190/1757\n",
      "  Processed 200/1757\n",
      "  Processed 210/1757\n",
      "  Processed 220/1757\n",
      "  Processed 230/1757\n",
      "  Processed 240/1757\n",
      "  Processed 250/1757\n",
      "  Processed 260/1757\n",
      "  Processed 270/1757\n",
      "  Processed 280/1757\n",
      "  Processed 290/1757\n",
      "  Processed 300/1757\n",
      "  Processed 310/1757\n",
      "  Processed 320/1757\n",
      "  Processed 330/1757\n",
      "  Processed 340/1757\n",
      "  Processed 350/1757\n",
      "  Processed 360/1757\n",
      "  Processed 370/1757\n",
      "  Processed 380/1757\n",
      "  Processed 390/1757\n",
      "  Processed 400/1757\n",
      "  Processed 410/1757\n",
      "  Processed 420/1757\n",
      "  Processed 430/1757\n",
      "  Processed 440/1757\n",
      "  Processed 450/1757\n",
      "  Processed 460/1757\n",
      "  Processed 470/1757\n",
      "  Processed 480/1757\n",
      "  Processed 490/1757\n",
      "  Processed 500/1757\n",
      "  Processed 510/1757\n",
      "  Processed 520/1757\n",
      "  Processed 530/1757\n",
      "  Processed 540/1757\n",
      "  Processed 550/1757\n",
      "  Processed 560/1757\n",
      "  Processed 570/1757\n",
      "  Processed 580/1757\n",
      "  Processed 590/1757\n",
      "  Processed 600/1757\n",
      "  Processed 610/1757\n",
      "  Processed 620/1757\n",
      "  Processed 630/1757\n",
      "  Processed 640/1757\n",
      "  Processed 650/1757\n",
      "  Processed 660/1757\n",
      "  Processed 670/1757\n",
      "  Processed 680/1757\n",
      "  Processed 690/1757\n",
      "  Processed 700/1757\n",
      "  Processed 710/1757\n",
      "  Processed 720/1757\n",
      "  Processed 730/1757\n",
      "  Processed 740/1757\n",
      "  Processed 750/1757\n",
      "  Processed 760/1757\n",
      "  Processed 770/1757\n",
      "  Processed 780/1757\n",
      "  Processed 790/1757\n",
      "  Processed 800/1757\n",
      "  Processed 810/1757\n",
      "  Processed 820/1757\n",
      "  Processed 830/1757\n",
      "  Processed 840/1757\n",
      "  Processed 850/1757\n",
      "  Processed 860/1757\n",
      "  Processed 870/1757\n",
      "  Processed 880/1757\n",
      "  Processed 890/1757\n",
      "  Processed 900/1757\n",
      "  Processed 910/1757\n",
      "  Processed 920/1757\n",
      "  Processed 930/1757\n",
      "  Processed 940/1757\n",
      "  Processed 950/1757\n",
      "  Processed 960/1757\n",
      "  Processed 970/1757\n",
      "  Processed 980/1757\n",
      "  Processed 990/1757\n",
      "  Processed 1000/1757\n",
      "  Processed 1010/1757\n",
      "  Processed 1020/1757\n",
      "  Processed 1030/1757\n",
      "  Processed 1040/1757\n",
      "  Processed 1050/1757\n",
      "  Processed 1060/1757\n",
      "  Processed 1070/1757\n",
      "  Processed 1080/1757\n",
      "  Processed 1090/1757\n",
      "  Processed 1100/1757\n",
      "  Processed 1110/1757\n",
      "  Processed 1120/1757\n",
      "  Processed 1130/1757\n",
      "  Processed 1140/1757\n",
      "  Processed 1150/1757\n",
      "  Processed 1160/1757\n",
      "  Processed 1170/1757\n",
      "  Processed 1180/1757\n",
      "  Processed 1190/1757\n",
      "  Processed 1200/1757\n",
      "  Processed 1210/1757\n",
      "  Processed 1220/1757\n",
      "  Processed 1230/1757\n",
      "  Processed 1240/1757\n",
      "  Processed 1250/1757\n",
      "  Processed 1260/1757\n",
      "  Processed 1270/1757\n",
      "  Processed 1280/1757\n",
      "  Processed 1290/1757\n",
      "  Processed 1300/1757\n",
      "  Processed 1310/1757\n",
      "  Processed 1320/1757\n",
      "  Processed 1330/1757\n",
      "  Processed 1340/1757\n",
      "  Processed 1350/1757\n",
      "  Processed 1360/1757\n",
      "  Processed 1370/1757\n",
      "  Processed 1380/1757\n",
      "  Processed 1390/1757\n",
      "  Processed 1400/1757\n",
      "  Processed 1410/1757\n",
      "  Processed 1420/1757\n",
      "  Processed 1430/1757\n",
      "  Processed 1440/1757\n",
      "  Processed 1450/1757\n",
      "  Processed 1460/1757\n",
      "  Processed 1470/1757\n",
      "  Processed 1480/1757\n",
      "  Processed 1490/1757\n",
      "  Processed 1500/1757\n",
      "  Processed 1510/1757\n",
      "  Processed 1520/1757\n",
      "  Processed 1530/1757\n",
      "  Processed 1540/1757\n",
      "  Processed 1550/1757\n",
      "  Processed 1560/1757\n",
      "  Processed 1570/1757\n",
      "  Processed 1580/1757\n",
      "  Processed 1590/1757\n",
      "  Processed 1600/1757\n",
      "  Processed 1610/1757\n",
      "  Processed 1620/1757\n",
      "  Processed 1630/1757\n",
      "  Processed 1640/1757\n",
      "  Processed 1650/1757\n",
      "  Processed 1660/1757\n",
      "  Processed 1670/1757\n",
      "  Processed 1680/1757\n",
      "  Processed 1690/1757\n",
      "  Processed 1700/1757\n",
      "  Processed 1710/1757\n",
      "  Processed 1720/1757\n",
      "  Processed 1730/1757\n",
      "  Processed 1740/1757\n",
      "  Processed 1750/1757\n",
      "\n",
      "✓ Float descriptors for re-ranking built\n",
      "  Shape: (1757, 36864)\n"
     ]
    }
   ],
   "source": [
    "# Build Binary Database (if hashing enabled)\n",
    "if USE_BINARY_HASHING:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"BUILDING BINARY DESCRIPTOR DATABASE\")\n",
    "    print(\"=\"*70 + \"\\n\")\n",
    "    \n",
    "    # Step 1: Build separate float model (without hashing)\n",
    "    print(\"Creating float model for re-ranking...\")\n",
    "    netvlad_float_model = build_netvlad_model_with_hashing(\n",
    "        BACKBONE, NUM_CLUSTERS, use_hashing=False\n",
    "    )\n",
    "    \n",
    "    # Step 2: Copy trained weights from binary model to float model\n",
    "    # (excluding the binary hashing layer)\n",
    "    print(\"Copying trained weights from binary model...\")\n",
    "    \n",
    "    for float_layer in netvlad_float_model.layers:\n",
    "        # Find matching layer in trained model\n",
    "        for trained_layer in netvlad_model.layers:\n",
    "            if float_layer.name == trained_layer.name:\n",
    "                try:\n",
    "                    float_layer.set_weights(trained_layer.get_weights())\n",
    "                    print(f\"  ✓ Copied weights: {float_layer.name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  ⚠ Skipped {float_layer.name}: {e}\")\n",
    "                break\n",
    "    \n",
    "    # Step 3: Extract binary codes from binary model\n",
    "    print(\"\\nExtracting binary codes for training images...\")\n",
    "    train_binary_codes = []\n",
    "    \n",
    "    for idx, row in train_df.iterrows():\n",
    "        img_path = os.path.join('images', row['filename'])\n",
    "        img = preprocess_image(img_path)\n",
    "        img = np.expand_dims(img, 0)\n",
    "        \n",
    "        # Extract from BINARY model (outputs hash codes)\n",
    "        binary_code = netvlad_model.predict(img, verbose=0)[0]\n",
    "        train_binary_codes.append(binary_code)\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(train_df)}\")\n",
    "    \n",
    "    train_binary_codes = np.array(train_binary_codes)\n",
    "    \n",
    "    # Convert to packed format: {-1, +1} → {0, 1} uint8\n",
    "    train_binary_codes_packed = ((train_binary_codes + 1) / 2).astype(np.uint8)\n",
    "    \n",
    "    print(f\"\\n✓ Binary descriptor database built\")\n",
    "    print(f\"  Shape: {train_binary_codes.shape}\")\n",
    "    print(f\"  Storage: {train_binary_codes_packed.nbytes / 1024:.2f} KB\")\n",
    "    \n",
    "    # Step 4: Extract float descriptors from float model for re-ranking\n",
    "    print(\"\\nExtracting float descriptors for re-ranking...\")\n",
    "    \n",
    "    train_float_descriptors = []\n",
    "    train_filenames = []\n",
    "    train_coordinates = []\n",
    "    \n",
    "    for idx, row in train_df.iterrows():\n",
    "        img_path = os.path.join('images', row['filename'])\n",
    "        img = preprocess_image(img_path)\n",
    "        img = np.expand_dims(img, 0)\n",
    "        \n",
    "        # Extract from FLOAT model (outputs NetVLAD descriptors)\n",
    "        descriptor = netvlad_float_model.predict(img, verbose=0)[0]\n",
    "        descriptor = descriptor / (np.linalg.norm(descriptor) + 1e-8)  # L2 normalize\n",
    "        \n",
    "        train_float_descriptors.append(descriptor)\n",
    "        train_filenames.append(row['filename'])\n",
    "        train_coordinates.append((row['latitude'], row['longitude']))\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(train_df)}\")\n",
    "    \n",
    "    train_float_descriptors = np.array(train_float_descriptors)\n",
    "    \n",
    "    print(f\"\\n✓ Float descriptors for re-ranking built\")\n",
    "    print(f\"  Shape: {train_float_descriptors.shape}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Binary hashing disabled, using float descriptors only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Localization function defined (with binary search support)\n"
     ]
    }
   ],
   "source": [
    "def localize_image(img_path, k=5, use_binary=USE_BINARY_HASHING, rerank_top_n=50):\n",
    "    img = preprocess_image(img_path)\n",
    "    img = np.expand_dims(img, 0)\n",
    "    \n",
    "    if use_binary and USE_BINARY_HASHING:\n",
    "        # Stage 1: Binary search\n",
    "        binary_code = netvlad_model.predict(img, verbose=0)[0]\n",
    "        \n",
    "        # Hamming distance (count differing bits)\n",
    "        hamming_distances = np.sum(train_binary_codes != binary_code, axis=1)\n",
    "        top_n_indices = np.argsort(hamming_distances)[:rerank_top_n]\n",
    "        \n",
    "        # Stage 2: Float re-ranking\n",
    "        query_float = netvlad_float_model.predict(img, verbose=0)[0]\n",
    "        query_float = query_float / (np.linalg.norm(query_float) + 1e-8)\n",
    "        \n",
    "        candidate_descriptors = train_float_descriptors[top_n_indices]\n",
    "        similarities = np.dot(candidate_descriptors, query_float)\n",
    "        \n",
    "        top_k_in_candidates = np.argsort(similarities)[::-1][:k]\n",
    "        top_k_indices = top_n_indices[top_k_in_candidates]\n",
    "        top_k_similarities = similarities[top_k_in_candidates]\n",
    "        \n",
    "    else:\n",
    "        # Standard float search\n",
    "        descriptor = netvlad_model.predict(img, verbose=0)[0]\n",
    "        descriptor = descriptor / (np.linalg.norm(descriptor) + 1e-8)\n",
    "        \n",
    "        similarities = np.dot(train_descriptors, descriptor)\n",
    "        top_k_indices = np.argsort(similarities)[::-1][:k]\n",
    "        top_k_similarities = similarities[top_k_indices]\n",
    "    \n",
    "    # Build results\n",
    "    results = []\n",
    "    for idx, sim in zip(top_k_indices, top_k_similarities):\n",
    "        results.append((\n",
    "            train_filenames[idx],\n",
    "            train_coordinates[idx],\n",
    "            sim\n",
    "        ))\n",
    "    \n",
    "    pred_coords = train_coordinates[top_k_indices[0]]\n",
    "    \n",
    "    return pred_coords, results\n",
    "\n",
    "print(\"✓ Localization function defined (with binary search support)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_BINARY_HASHING:\n",
    "    # Extract descriptors for training images\n",
    "    print(\"Extracting descriptors for training images...\")\n",
    "\n",
    "    train_descriptors = []\n",
    "    train_filenames = []\n",
    "    train_coordinates = []\n",
    "\n",
    "    for idx, row in train_df.iterrows():\n",
    "        img_path = os.path.join('images', row['filename'])\n",
    "        img = preprocess_image(img_path)\n",
    "        img = np.expand_dims(img, 0)\n",
    "        \n",
    "        descriptor = netvlad_model.predict(img, verbose=0)[0]\n",
    "        \n",
    "        train_descriptors.append(descriptor)\n",
    "        train_filenames.append(row['filename'])\n",
    "        train_coordinates.append((row['latitude'], row['longitude']))\n",
    "        \n",
    "        if (idx + 1) % 10 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(train_df)}\")\n",
    "\n",
    "    train_descriptors = np.array(train_descriptors)\n",
    "\n",
    "    # Normalize\n",
    "    train_descriptors = train_descriptors / (np.linalg.norm(train_descriptors, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "    print(f\"\\n✓ Descriptor database built\")\n",
    "    print(f\"  Shape: {train_descriptors.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on test set...\n",
      "\n",
      "✓ Evaluated 81 test images\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "print(\"Evaluating on test set...\\n\")\n",
    "\n",
    "def haversine_distance_km(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1, lon1, lat2, lon2 = map(np.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = np.sin(dlat/2)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2)**2\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1-a))\n",
    "    return R * c\n",
    "\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "errors_m = []\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    img_path = os.path.join('images', row['filename'])\n",
    "    gt_coords = (row['latitude'], row['longitude'])\n",
    "    \n",
    "    pred_coords, top_k = localize_image(img_path, k=10)\n",
    "    \n",
    "    predictions.append(top_k)\n",
    "    ground_truths.append(gt_coords)\n",
    "    \n",
    "    error_km = haversine_distance_km(\n",
    "        gt_coords[0], gt_coords[1],\n",
    "        pred_coords[0], pred_coords[1]\n",
    "    )\n",
    "    errors_m.append(error_km * 1000)\n",
    "\n",
    "errors_m = np.array(errors_m)\n",
    "\n",
    "print(f\"✓ Evaluated {len(test_df)} test images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize error distribution\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(errors_m, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(np.median(errors_m), color='r', linestyle='--', label=f'Median: {np.median(errors_m):.1f}m')\n",
    "plt.axvline(np.mean(errors_m), color='g', linestyle='--', label=f'Mean: {np.mean(errors_m):.1f}m')\n",
    "plt.xlabel('Error (meters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Localization Error Distribution')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sorted_errors = np.sort(errors_m)\n",
    "cumulative = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors) * 100\n",
    "plt.plot(sorted_errors, cumulative, linewidth=2)\n",
    "plt.axvline(25, color='r', linestyle='--', alpha=0.7, label='25m threshold')\n",
    "plt.axvline(50, color='orange', linestyle='--', alpha=0.7, label='50m threshold')\n",
    "plt.xlabel('Distance Threshold (meters)')\n",
    "plt.ylabel('Percentage Localized (%)')\n",
    "plt.title('Cumulative Localization Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_recall(predictions, ground_truths, k, distance_threshold_m):\n",
    "    correct = 0\n",
    "    for pred_list, gt_coords in zip(predictions, ground_truths):\n",
    "        top_k = pred_list[:k]\n",
    "        for _, pred_coords, _ in top_k:\n",
    "            dist_km = haversine_distance_km(\n",
    "                gt_coords[0], gt_coords[1],\n",
    "                pred_coords[0], pred_coords[1]\n",
    "            )\n",
    "            if dist_km * 1000 <= distance_threshold_m:\n",
    "                correct += 1\n",
    "                break\n",
    "    return correct / len(predictions) if len(predictions) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Model saved: netvlad_mobilenetv3small_binary.keras\n",
      "✓ Binary database saved: train_database_mobilenetv3small_binary.npz\n",
      "  Binary codes: 878.50 KB\n",
      "  Float descriptors: 253008.00 KB\n",
      "✓ Results saved: results_mobilenetv3small.json\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "model_filename = f'netvlad_{BACKBONE}{\"_binary\" if USE_BINARY_HASHING else \"\"}.keras'\n",
    "netvlad_model.save(model_filename)\n",
    "print(f\"✓ Model saved: {model_filename}\")\n",
    "\n",
    "# Save descriptor database\n",
    "database_filename = f'train_database_{BACKBONE}{\"_binary\" if USE_BINARY_HASHING else \"\"}.npz'\n",
    "\n",
    "if USE_BINARY_HASHING:\n",
    "    np.savez(\n",
    "        database_filename,\n",
    "        binary_codes=train_binary_codes_packed,\n",
    "        float_descriptors=train_float_descriptors,\n",
    "        filenames=np.array(train_filenames, dtype=object),\n",
    "        coordinates=np.array(train_coordinates, dtype=object),\n",
    "        hash_bits=HASH_BITS\n",
    "    )\n",
    "    print(f\"✓ Binary database saved: {database_filename}\")\n",
    "    print(f\"  Binary codes: {train_binary_codes_packed.nbytes / 1024:.2f} KB\")\n",
    "    print(f\"  Float descriptors: {train_float_descriptors.nbytes / 1024:.2f} KB\")\n",
    "else:\n",
    "    np.savez(\n",
    "        database_filename,\n",
    "        descriptors=train_descriptors,\n",
    "        filenames=np.array(train_filenames, dtype=object),\n",
    "        coordinates=np.array(train_coordinates, dtype=object)\n",
    "    )\n",
    "    print(f\"✓ Database saved: {database_filename}\")\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'backbone': BACKBONE,\n",
    "    'epochs': EPOCHS,\n",
    "    'binary_hashing': USE_BINARY_HASHING,\n",
    "    'hash_bits': HASH_BITS if USE_BINARY_HASHING else None,\n",
    "    'final_loss': float(history.history['loss'][-1]),\n",
    "    'recall_metrics': {\n",
    "        f'Recall@{k}_{t}m': float(compute_recall(predictions, ground_truths, k, t))\n",
    "        for k in [1, 5]\n",
    "        for t in [10, 25, 50]\n",
    "    },\n",
    "    'error_statistics': {\n",
    "        'mean_m': float(np.mean(errors_m)),\n",
    "        'median_m': float(np.median(errors_m)),\n",
    "        'min_m': float(np.min(errors_m)),\n",
    "        'max_m': float(np.max(errors_m)),\n",
    "        'std_m': float(np.std(errors_m))\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'results_{BACKBONE}.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"✓ Results saved: results_{BACKBONE}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ZONE PACKAGE EXPORT\n",
      "======================================================================\n",
      "\n",
      "Exporting float model (without binary hashing) for zone package...\n",
      "Using inferred input size: 224x224x3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1767359407.017386   15451 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\n",
      "2026-01-02 13:10:07.017551: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2026-01-02 13:10:07.029004: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 900 MB memory:  -> device: 0, name: NVIDIA H100 NVL, pci bus id: 0000:ca:00.0, compute capability: 9.0\n",
      "2026-01-02 13:10:07.030240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 92489 MB memory:  -> device: 1, name: NVIDIA H100 NVL, pci bus id: 0000:e1:00.0, compute capability: 9.0\n",
      "2026-01-02 13:10:10.049813: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 900 MB memory:  -> device: 0, name: NVIDIA H100 NVL, pci bus id: 0000:ca:00.0, compute capability: 9.0\n",
      "2026-01-02 13:10:10.051057: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 92489 MB memory:  -> device: 1, name: NVIDIA H100 NVL, pci bus id: 0000:e1:00.0, compute capability: 9.0\n",
      "I0000 00:00:1767359410.796272   15451 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 2\n",
      "2026-01-02 13:10:10.796451: I tensorflow/core/grappler/clusters/single_machine.cc:361] Starting new session\n",
      "2026-01-02 13:10:10.808898: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 900 MB memory:  -> device: 0, name: NVIDIA H100 NVL, pci bus id: 0000:ca:00.0, compute capability: 9.0\n",
      "2026-01-02 13:10:10.810132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 92489 MB memory:  -> device: 1, name: NVIDIA H100 NVL, pci bus id: 0000:e1:00.0, compute capability: 9.0\n",
      "2026-01-02 13:10:11.514396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 900 MB memory:  -> device: 0, name: NVIDIA H100 NVL, pci bus id: 0000:ca:00.0, compute capability: 9.0\n",
      "2026-01-02 13:10:11.515618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 92489 MB memory:  -> device: 1, name: NVIDIA H100 NVL, pci bus id: 0000:e1:00.0, compute capability: 9.0\n",
      "2026-01-02 13:10:11.516797: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:388] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ ONNX saved: weights_mobilenetv3small.onnx\n",
      "✓ DB binary saved: database_mobilenetv3small.bin shape=(1757, 36864) dtype=float32\n",
      "✓ DB index saved: database_mobilenetv3small_index.json\n",
      "✓ Metadata saved: metadata_mobilenetv3small.json\n",
      "\n",
      "✓ Zone package export complete!\n",
      "  Files: weights_mobilenetv3small.onnx, database_mobilenetv3small.bin, database_mobilenetv3small_index.json, metadata_mobilenetv3small.json\n"
     ]
    }
   ],
   "source": [
    "# Export for Zone Package (ONNX + Binary DB + Metadata)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ZONE PACKAGE EXPORT\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    import tf2onnx\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Determine which model to export (float version for zone package)\n",
    "    if USE_BINARY_HASHING:\n",
    "        # Export the float model (without hashing) for zone package\n",
    "        export_model = netvlad_float_model\n",
    "        descriptors_to_export = train_float_descriptors\n",
    "        print(\"Exporting float model (without binary hashing) for zone package...\")\n",
    "    else:\n",
    "        export_model = netvlad_model\n",
    "        descriptors_to_export = train_descriptors\n",
    "        print(\"Exporting model for zone package...\")\n",
    "    \n",
    "    # File paths\n",
    "    onnx_path = Path(f\"weights_{BACKBONE}.onnx\")\n",
    "    db_bin_path = Path(f\"database_{BACKBONE}.bin\")\n",
    "    db_index_path = Path(f\"database_{BACKBONE}_index.json\")\n",
    "    metadata_path = Path(f\"metadata_{BACKBONE}.json\")\n",
    "    \n",
    "    # 1) Export Keras -> ONNX\n",
    "    _, H, W, C = export_model.input_shape\n",
    "    assert C == 3, f\"Expected 3 channels, got {C}\"\n",
    "    INPUT_SIZE = H  # typically 224\n",
    "    print(f\"Using inferred input size: {INPUT_SIZE}x{INPUT_SIZE}x{C}\")\n",
    "    \n",
    "    onnx_model, _ = tf2onnx.convert.from_keras(\n",
    "        export_model,\n",
    "        input_signature=[tf.TensorSpec([None, INPUT_SIZE, INPUT_SIZE, 3], tf.float32, name=\"input\")],\n",
    "        output_path=str(onnx_path)\n",
    "    )\n",
    "    print(f\"✓ ONNX saved: {onnx_path}\")\n",
    "    \n",
    "    # 2) Write descriptors to binary + JSON index\n",
    "    descriptors = np.asarray(descriptors_to_export, dtype=np.float32)  # shape (N, D)\n",
    "    descriptors.tofile(db_bin_path)\n",
    "    print(f\"✓ DB binary saved: {db_bin_path} shape={descriptors.shape} dtype=float32\")\n",
    "    \n",
    "    index = []\n",
    "    for i, (fname, coord) in enumerate(zip(train_filenames, train_coordinates)):\n",
    "        # coord assumed (lat, lon)\n",
    "        lat, lon = float(coord[0]), float(coord[1])\n",
    "        index.append({\n",
    "            \"row\": int(i),\n",
    "            \"file\": str(fname),\n",
    "            \"lat\": lat,\n",
    "            \"lon\": lon\n",
    "        })\n",
    "    with open(db_index_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(index, f, indent=2)\n",
    "    print(f\"✓ DB index saved: {db_index_path}\")\n",
    "    \n",
    "    # 3) Metadata.json\n",
    "    descriptor_dim = descriptors.shape[1]\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    metadata = {\n",
    "        \"id\": f\"netvlad-{BACKBONE}\",\n",
    "        \"version\": \"1.0.0\",\n",
    "        \"arch\": \"netvlad\",\n",
    "        \"engine\": \"onnx\",\n",
    "        \"input_size\": INPUT_SIZE,\n",
    "        \"mean\": mean,\n",
    "        \"std\": std,\n",
    "        \"center_crop\": True,\n",
    "        \"descriptor_dim\": int(descriptor_dim),\n",
    "        \"database\": {\n",
    "            \"file\": db_bin_path.name,\n",
    "            \"index\": db_index_path.name,\n",
    "            \"dtype\": \"float32\",\n",
    "            \"metric\": \"cosine\"\n",
    "        }\n",
    "    }\n",
    "    with open(metadata_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"✓ Metadata saved: {metadata_path}\")\n",
    "    \n",
    "    print(\"\\n✓ Zone package export complete!\")\n",
    "    print(f\"  Files: {onnx_path.name}, {db_bin_path.name}, {db_index_path.name}, {metadata_path.name}\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"⚠ tf2onnx not installed. Skipping ONNX export.\")\n",
    "    print(\"  To enable: pip install tf2onnx\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠ Zone package export failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MOBILE DEPLOYMENT SUMMARY\n",
      "======================================================================\n",
      "\n",
      "Generated Files:\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "✓ train_database_mobilenetv3small_binary.npz               253961.30 KB\n",
      "\n",
      "======================================================================\n",
      "RECOMMENDED DEPLOYMENT OPTIONS\n",
      "======================================================================\n",
      "\n",
      "Option 1: Maximum Accuracy (Float32)\n",
      "  Model:    netvlad_mobilenetv3small_binary_float32.tflite\n",
      "  Database: train_database_mobilenetv3small_binary.npz\n",
      "  Use case: Highest accuracy, larger size\n",
      "\n",
      "Option 2: Balanced (INT8 Quantization) - RECOMMENDED\n",
      "  Model:    netvlad_mobilenetv3small_binary_int8.tflite\n",
      "  Database: train_database_mobilenetv3small_binary.npz\n",
      "  Use case: Best balance of size, speed, and accuracy\n",
      "\n",
      "Option 3: Maximum Speed (Binary Hashing)\n",
      "  Model:    netvlad_mobilenetv3small_binary.h5\n",
      "  Database: train_database_mobilenetv3small_binary.npz (binary codes)\n",
      "  Search:   Two-stage (binary + float re-rank)\n",
      "  Use case: Fastest search with minimal accuracy loss\n",
      "\n",
      "======================================================================\n",
      "Database Compression (Float → Binary): 288.00x\n",
      "  Binary codes: 878.50 KB (512.0 bytes/image)\n",
      "  Float descriptors: 253008.00 KB (for re-ranking)\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Mobile Deployment Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MOBILE DEPLOYMENT SUMMARY\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"Generated Files:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "# List all files\n",
    "suffix = \"_binary\" if USE_BINARY_HASHING else \"\"\n",
    "\n",
    "model_files = [\n",
    "    f'netvlad_{BACKBONE}{suffix}.h5',\n",
    "    f'netvlad_{BACKBONE}{suffix}_float32.tflite',\n",
    "    f'netvlad_{BACKBONE}{suffix}_int8.tflite',\n",
    "    f'netvlad_{BACKBONE}{suffix}_dynamic.tflite',\n",
    "]\n",
    "\n",
    "for fname in model_files:\n",
    "    if os.path.exists(fname):\n",
    "        size_kb = os.path.getsize(fname) / 1024\n",
    "        print(f\"✓ {fname:<55} {size_kb:>10.2f} KB\")\n",
    "\n",
    "db_files = [\n",
    "    f'train_database_{BACKBONE}{suffix}.npz',\n",
    "]\n",
    "\n",
    "print()\n",
    "for fname in db_files:\n",
    "    if os.path.exists(fname):\n",
    "        size_kb = os.path.getsize(fname) / 1024\n",
    "        print(f\"✓ {fname:<55} {size_kb:>10.2f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RECOMMENDED DEPLOYMENT OPTIONS\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "print(\"Option 1: Maximum Accuracy (Float32)\")\n",
    "print(f\"  Model:    netvlad_{BACKBONE}{suffix}_float32.tflite\")\n",
    "print(f\"  Database: train_database_{BACKBONE}{suffix}.npz\")\n",
    "print(f\"  Use case: Highest accuracy, larger size\")\n",
    "print()\n",
    "\n",
    "print(\"Option 2: Balanced (INT8 Quantization) - RECOMMENDED\")\n",
    "print(f\"  Model:    netvlad_{BACKBONE}{suffix}_int8.tflite\")\n",
    "print(f\"  Database: train_database_{BACKBONE}{suffix}.npz\")\n",
    "print(f\"  Use case: Best balance of size, speed, and accuracy\")\n",
    "print()\n",
    "\n",
    "if USE_BINARY_HASHING:\n",
    "    print(\"Option 3: Maximum Speed (Binary Hashing)\")\n",
    "    print(f\"  Model:    netvlad_{BACKBONE}{suffix}.h5\")\n",
    "    print(f\"  Database: train_database_{BACKBONE}{suffix}.npz (binary codes)\")\n",
    "    print(f\"  Search:   Two-stage (binary + float re-rank)\")\n",
    "    print(f\"  Use case: Fastest search with minimal accuracy loss\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Compression stats\n",
    "if os.path.exists(f'netvlad_{BACKBONE}{suffix}.h5'):\n",
    "    h5_size = os.path.getsize(f'netvlad_{BACKBONE}{suffix}.h5') / 1024\n",
    "    \n",
    "    if os.path.exists(f'netvlad_{BACKBONE}{suffix}_int8.tflite'):\n",
    "        int8_size = os.path.getsize(f'netvlad_{BACKBONE}{suffix}_int8.tflite') / 1024\n",
    "        print(f\"\\nModel Compression (H5 → INT8): {h5_size / int8_size:.2f}x\")\n",
    "    elif os.path.exists(f'netvlad_{BACKBONE}{suffix}_dynamic.tflite'):\n",
    "        dynamic_size = os.path.getsize(f'netvlad_{BACKBONE}{suffix}_dynamic.tflite') / 1024\n",
    "        print(f\"\\nModel Compression (H5 → Dynamic): {h5_size / dynamic_size:.2f}x\")\n",
    "\n",
    "if USE_BINARY_HASHING:\n",
    "    float_db_size = train_float_descriptors.nbytes / 1024\n",
    "    binary_db_size = train_binary_codes_packed.nbytes / 1024\n",
    "    print(f\"Database Compression (Float → Binary): {float_db_size / binary_db_size:.2f}x\")\n",
    "    print(f\"  Binary codes: {binary_db_size:.2f} KB ({train_binary_codes_packed.nbytes / len(train_df):.1f} bytes/image)\")\n",
    "    print(f\"  Float descriptors: {float_db_size:.2f} KB (for re-ranking)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TFLITE MODEL VALIDATION\n",
      "======================================================================\n",
      "\n",
      "⚠ netvlad_mobilenetv3small_binary_float32.tflite not found, skipping...\n",
      "\n",
      "✓ TFLite validation complete!\n"
     ]
    }
   ],
   "source": [
    "# TFLite Model Validation\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TFLITE MODEL VALIDATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "def test_tflite_model(tflite_path, model_name, num_test_images=10):\n",
    "    \"\"\"Test TFLite model accuracy and speed.\"\"\"\n",
    "    \n",
    "    if not os.path.exists(tflite_path):\n",
    "        print(f\"⚠ {tflite_path} not found, skipping...\")\n",
    "        return None\n",
    "    \n",
    "    # Load interpreter\n",
    "    interpreter = tf.lite.Interpreter(model_path=tflite_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    \n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    \n",
    "    print(f\"\\nTesting {model_name}:\")\n",
    "    print(f\"  Input: {input_details[0]['shape']} ({input_details[0]['dtype']})\")\n",
    "    print(f\"  Output: {output_details[0]['shape']} ({output_details[0]['dtype']})\")\n",
    "    \n",
    "    # Test on sample images\n",
    "    errors = []\n",
    "    inference_times = []\n",
    "    \n",
    "    for idx in range(min(num_test_images, len(test_df))):\n",
    "        img_path = os.path.join('images', test_df.iloc[idx]['filename'])\n",
    "        img = preprocess_image(img_path)\n",
    "        \n",
    "        # Keras inference\n",
    "        img_keras = np.expand_dims(img, 0)\n",
    "        keras_desc = netvlad_model.predict(img_keras, verbose=0)[0]\n",
    "        \n",
    "        # TFLite inference\n",
    "        img_tflite = np.expand_dims(img, 0).astype(input_details[0]['dtype'])\n",
    "        \n",
    "        # Handle uint8 quantization\n",
    "        if input_details[0]['dtype'] == np.uint8:\n",
    "            input_scale, input_zero_point = input_details[0]['quantization']\n",
    "            img_tflite = (img_tflite / input_scale + input_zero_point).astype(np.uint8)\n",
    "        \n",
    "        interpreter.set_tensor(input_details[0]['index'], img_tflite)\n",
    "        \n",
    "        import time\n",
    "        start = time.time()\n",
    "        interpreter.invoke()\n",
    "        inference_time = (time.time() - start) * 1000\n",
    "        inference_times.append(inference_time)\n",
    "        \n",
    "        tflite_desc = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "        \n",
    "        # Normalize (if float output)\n",
    "        if output_details[0]['dtype'] != np.int8:\n",
    "            tflite_desc = tflite_desc / (np.linalg.norm(tflite_desc) + 1e-8)\n",
    "        \n",
    "        # Compare\n",
    "        error = np.linalg.norm(keras_desc - tflite_desc)\n",
    "        errors.append(error)\n",
    "    \n",
    "    avg_error = np.mean(errors)\n",
    "    avg_inference_time = np.mean(inference_times)\n",
    "    \n",
    "    print(f\"  Descriptor error: {avg_error:.6f}\")\n",
    "    print(f\"  Inference time: {avg_inference_time:.2f} ms\")\n",
    "    \n",
    "    return {'error': avg_error, 'inference_time': avg_inference_time}\n",
    "\n",
    "# Test all models\n",
    "results_tflite = {}\n",
    "suffix = \"_binary\" if USE_BINARY_HASHING else \"\"\n",
    "\n",
    "results_tflite['float32'] = test_tflite_model(f'netvlad_{BACKBONE}{suffix}_float32.tflite', 'Float32')\n",
    "\n",
    "if os.path.exists(f'netvlad_{BACKBONE}{suffix}_int8.tflite'):\n",
    "    results_tflite['int8'] = test_tflite_model(f'netvlad_{BACKBONE}{suffix}_int8.tflite', 'INT8')\n",
    "elif os.path.exists(f'netvlad_{BACKBONE}{suffix}_dynamic.tflite'):\n",
    "    results_tflite['dynamic'] = test_tflite_model(f'netvlad_{BACKBONE}{suffix}_dynamic.tflite', 'Dynamic')\n",
    "\n",
    "print(\"\\n✓ TFLite validation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "TFLITE CONVERSION AND QUANTIZATION\n",
      "======================================================================\n",
      "\n",
      "Converting to TFLite (Float32)...\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp9721kzz5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp9721kzz5/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmp9721kzz5'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_175')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 512), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140723193573840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193575184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193573648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193574416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193574800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193573072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193572112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193574992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193574608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193575568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909230736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909232272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909231696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909233040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909231504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909233616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909233232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909233808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909234192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909235536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909235920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909236304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909236112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909232464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909237456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909237072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909236688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909237648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909235152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909239760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909240144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909240528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909240336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909234000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909238608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909242256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909242640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909242448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909238992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909241296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909244368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909244752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909244560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909241680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909245904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909241872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909246288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909246096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909243984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903184848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903186192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903186576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903186384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903185040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903187728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903188112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903188496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903188304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903185616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903189648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903190224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903189264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903190992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903190416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903191568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903191952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903191760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903187344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903193104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903193488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903193872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903193680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903185424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903195024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903195408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903195792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903195600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903191184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903196944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903197520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903196560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903198288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903197712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903198864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903199248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903199056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903194640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903200400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903192720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903823824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903200016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903198480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903824976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903825360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903825744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903825552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903824208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903826896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903827472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903826512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903828240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903827664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903828816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903829200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903829008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903823440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903830352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903830736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903831120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903830928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903824592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903832272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903832656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903833040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903832848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903828432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903834192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903834768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903833808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903835536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903834960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903836112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903836496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903836304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903831888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903837648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903838032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903838416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903838224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903824400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903839568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903835728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902266960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903839184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903837264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902268688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902269264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902268304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902270032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902269456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902270608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902270992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902270800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902267344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902272144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902272528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902272912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902272720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902267152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902274064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902274448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902274832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902274640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902270224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902275984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902276560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902275600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902277328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902276752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902277904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902278288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902278096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902273680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902279440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902279824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902280208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902280016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902267920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902281360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902281744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902282128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902281936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902277520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902279056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902280976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902282896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902923664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902923088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902924240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902924624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902924432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902922320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902925776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902926160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902926544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902926352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902922704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902927696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902928080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902928464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902928272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902922512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902929616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902930192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902929232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902930960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902930000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902931920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902932304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902932112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902925392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902933456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902933840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902934224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902934032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902931344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902935760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902936720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902932688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902938064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902937488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1767359460.211987   15451 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1767359460.212011   15451 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2026-01-02 13:11:00.212520: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp9721kzz5\n",
      "2026-01-02 13:11:00.221090: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2026-01-02 13:11:00.221111: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp9721kzz5\n",
      "2026-01-02 13:11:00.324252: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2026-01-02 13:11:00.924569: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp9721kzz5\n",
      "2026-01-02 13:11:01.085626: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 873109 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Float32 TFLite: netvlad_mobilenetv3small_binary_float32.tflite\n",
      "  Size: 19769.73 KB\n",
      "\n",
      "Converting to TFLite (INT8)...\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp4vmvzife/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4vmvzife/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmp4vmvzife'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor_175')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 512), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140723193573840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193575184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193573648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193574416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193574800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193573072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193572112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193574992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193574608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140723193575568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909230736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909232272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909231696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909233040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909231504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909233616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909233232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909233808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909234192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909235536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909235920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909236304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909236112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909232464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909237456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909237072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909236688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909237648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909235152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909239760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909240144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909240528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909240336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909234000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909238608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909242256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909242640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909242448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909238992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909241296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909244368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909244752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909244560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909241680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909245904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909241872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909246288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909246096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736909243984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903184848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903186192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903186576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903186384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903185040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903187728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903188112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903188496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903188304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903185616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903189648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903190224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903189264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903190992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903190416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903191568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903191952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903191760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903187344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903193104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903193488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903193872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903193680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903185424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903195024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903195408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903195792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903195600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903191184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903196944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903197520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903196560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903198288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903197712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903198864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903199248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903199056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903194640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903200400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903192720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903823824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903200016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903198480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903824976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903825360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903825744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903825552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903824208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903826896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903827472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903826512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903828240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903827664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903828816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903829200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903829008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903823440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903830352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903830736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903831120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903830928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903824592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903832272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903832656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903833040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903832848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903828432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903834192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903834768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903833808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903835536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903834960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903836112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903836496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903836304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903831888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903837648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903838032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903838416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903838224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903824400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903839568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903835728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902266960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903839184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736903837264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902268688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902269264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902268304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902270032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902269456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902270608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902270992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902270800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902267344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902272144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902272528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902272912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902272720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902267152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902274064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902274448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902274832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902274640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902270224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902275984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902276560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902275600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902277328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902276752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902277904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902278288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902278096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902273680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902279440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902279824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902280208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902280016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902267920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902281360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902281744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902282128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902281936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902277520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902279056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902280976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902282896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902923664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902923088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902924240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902924624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902924432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902922320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902925776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902926160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902926544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902926352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902922704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902927696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902928080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902928464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902928272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902922512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902929616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902930192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902929232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902930960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902930000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902931920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902932304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902932112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902925392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902933456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902933840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902934224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902934032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902931344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902935760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902936720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902932688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902938064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140736902937488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/tensorflow/lite/python/convert.py:983: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1767359480.739922   15451 tf_tfl_flatbuffer_helpers.cc:392] Ignored output_format.\n",
      "W0000 00:00:1767359480.739946   15451 tf_tfl_flatbuffer_helpers.cc:395] Ignored drop_control_dependency.\n",
      "2026-01-02 13:11:20.740161: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmp4vmvzife\n",
      "2026-01-02 13:11:20.747868: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2026-01-02 13:11:20.747890: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmp4vmvzife\n",
      "2026-01-02 13:11:20.850303: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2026-01-02 13:11:21.421108: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmp4vmvzife\n",
      "2026-01-02 13:11:21.582221: I tensorflow/cc/saved_model/loader.cc:462] SavedModel load for tags { serve }; Status: success: OK. Took 842063 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: UINT8, output_inference_type: FLOAT32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ INT8 TFLite: netvlad_mobilenetv3small_binary_int8.tflite\n",
      "  Size: 19788.20 KB\n",
      "  Compression: 1.00x\n",
      "\n",
      "✓ TFLite conversion complete!\n"
     ]
    }
   ],
   "source": [
    "# TFLite Conversion with INT8 Quantization\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TFLITE CONVERSION AND QUANTIZATION\")\n",
    "print(\"=\"*70 + \"\\n\")\n",
    "\n",
    "# Representative dataset for INT8 calibration\n",
    "def representative_dataset_gen():\n",
    "    \"\"\"Generate calibration samples for INT8 quantization.\"\"\"\n",
    "    num_calibration_samples = min(100, len(train_df))\n",
    "    \n",
    "    for idx in range(num_calibration_samples):\n",
    "        img_path = os.path.join('images', train_df.iloc[idx]['filename'])\n",
    "        img = preprocess_image(img_path)\n",
    "        img = np.expand_dims(img, 0).astype(np.float32)\n",
    "        yield [img]\n",
    "\n",
    "# 1. Convert to Float32 TFLite\n",
    "print(\"Converting to TFLite (Float32)...\")\n",
    "converter_float = tf.lite.TFLiteConverter.from_keras_model(netvlad_model)\n",
    "converter_float.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_float_model = converter_float.convert()\n",
    "\n",
    "float_model_path = f'netvlad_{BACKBONE}{\"_binary\" if USE_BINARY_HASHING else \"\"}_float32.tflite'\n",
    "with open(float_model_path, 'wb') as f:\n",
    "    f.write(tflite_float_model)\n",
    "\n",
    "print(f\"✓ Float32 TFLite: {float_model_path}\")\n",
    "print(f\"  Size: {len(tflite_float_model) / 1024:.2f} KB\")\n",
    "\n",
    "# 2. Convert to INT8 TFLite\n",
    "print(\"\\nConverting to TFLite (INT8)...\")\n",
    "converter_int8 = tf.lite.TFLiteConverter.from_keras_model(netvlad_model)\n",
    "converter_int8.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter_int8.representative_dataset = representative_dataset_gen\n",
    "\n",
    "# Full integer quantization\n",
    "converter_int8.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter_int8.inference_input_type = tf.uint8\n",
    "converter_int8.inference_output_type = tf.float32  # Keep descriptor as float\n",
    "\n",
    "try:\n",
    "    tflite_int8_model = converter_int8.convert()\n",
    "    \n",
    "    int8_model_path = f'netvlad_{BACKBONE}{\"_binary\" if USE_BINARY_HASHING else \"\"}_int8.tflite'\n",
    "    with open(int8_model_path, 'wb') as f:\n",
    "        f.write(tflite_int8_model)\n",
    "    \n",
    "    print(f\"✓ INT8 TFLite: {int8_model_path}\")\n",
    "    print(f\"  Size: {len(tflite_int8_model) / 1024:.2f} KB\")\n",
    "    print(f\"  Compression: {len(tflite_float_model) / len(tflite_int8_model):.2f}x\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"⚠ INT8 conversion failed: {e}\")\n",
    "    print(\"  Falling back to dynamic range quantization...\")\n",
    "    \n",
    "    # Fallback: Dynamic range (weights only)\n",
    "    converter_dr = tf.lite.TFLiteConverter.from_keras_model(netvlad_model)\n",
    "    converter_dr.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    tflite_dr_model = converter_dr.convert()\n",
    "    \n",
    "    dr_model_path = f'netvlad_{BACKBONE}{\"_binary\" if USE_BINARY_HASHING else \"\"}_dynamic.tflite'\n",
    "    with open(dr_model_path, 'wb') as f:\n",
    "        f.write(tflite_dr_model)\n",
    "    \n",
    "    print(f\"✓ Dynamic range TFLite: {dr_model_path}\")\n",
    "    print(f\"  Size: {len(tflite_dr_model) / 1024:.2f} KB\")\n",
    "\n",
    "print(\"\\n✓ TFLite conversion complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Mobile Deployment: TFLite Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Save and Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained model\n",
    "model_filename = f'netvlad_{BACKBONE}.h5'\n",
    "netvlad_model.save(model_filename)\n",
    "print(f\"✓ Model saved: {model_filename}\")\n",
    "\n",
    "# Save descriptor database\n",
    "database_filename = f'train_database_{BACKBONE}.npz'\n",
    "np.savez(\n",
    "    database_filename,\n",
    "    descriptors=train_descriptors,\n",
    "    filenames=np.array(train_filenames, dtype=object),\n",
    "    coordinates=np.array(train_coordinates, dtype=object)\n",
    ")\n",
    "print(f\"✓ Database saved: {database_filename}\")\n",
    "\n",
    "# Save results\n",
    "import json\n",
    "\n",
    "results = {\n",
    "    'backbone': BACKBONE,\n",
    "    'epochs': EPOCHS,\n",
    "    'final_loss': float(history.history['loss'][-1]),\n",
    "    'recall_metrics': {\n",
    "        f'Recall@{k}_{t}m': float(compute_recall(predictions, ground_truths, k, t))\n",
    "        for k in [1, 5]\n",
    "        for t in [10, 25, 50]\n",
    "    },\n",
    "    'error_statistics': {\n",
    "        'mean_m': float(np.mean(errors_m)),\n",
    "        'median_m': float(np.median(errors_m)),\n",
    "        'min_m': float(np.min(errors_m)),\n",
    "        'max_m': float(np.max(errors_m)),\n",
    "        'std_m': float(np.std(errors_m))\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f'results_{BACKBONE}.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"✓ Results saved: results_{BACKBONE}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download files (for Colab)\n",
    "if IN_COLAB:\n",
    "    print(\"Downloading files...\")\n",
    "    files.download(model_filename)\n",
    "    files.download(database_filename)\n",
    "    files.download(f'results_{BACKBONE}.json')\n",
    "    print(\"✓ Files downloaded\")\n",
    "else:\n",
    "    print(\"Files saved in current directory\")\n",
    "    print(f\"  Model: {model_filename}\")\n",
    "    print(f\"  Database: {database_filename}\")\n",
    "    print(f\"  Results: results_{BACKBONE}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Test Single Image (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query image: 100.jpg\n",
      "Ground truth: (54.906279, 23.956114)\n",
      "\n",
      "Predicted: (54.905261, 23.957172)\n",
      "Error: 131.91 meters\n",
      "\n",
      "Top-5 matches:\n",
      "1. 56.jpg: 0.9559 | 131.9m ✗\n",
      "2. 39.jpg: 0.9539 | 196.3m ✗\n",
      "3. 35.jpg: 0.9516 | 195.9m ✗\n",
      "4. 93.jpg: 0.9431 | 58.7m ✗\n",
      "5. 49.jpg: 0.9376 | 183.5m ✗\n"
     ]
    }
   ],
   "source": [
    "# Test on a single image\n",
    "test_image = test_df.iloc[0]\n",
    "test_img_path = os.path.join('images', test_image['filename'])\n",
    "gt_coords = (test_image['latitude'], test_image['longitude'])\n",
    "\n",
    "print(f\"Query image: {test_image['filename']}\")\n",
    "print(f\"Ground truth: ({gt_coords[0]:.6f}, {gt_coords[1]:.6f})\")\n",
    "print()\n",
    "\n",
    "pred_coords, top_5 = localize_image(test_img_path, k=5)\n",
    "\n",
    "print(f\"Predicted: ({pred_coords[0]:.6f}, {pred_coords[1]:.6f})\")\n",
    "error_km = haversine_distance_km(\n",
    "    gt_coords[0], gt_coords[1],\n",
    "    pred_coords[0], pred_coords[1]\n",
    ")\n",
    "print(f\"Error: {error_km * 1000:.2f} meters\")\n",
    "print()\n",
    "\n",
    "print(\"Top-5 matches:\")\n",
    "for i, (fname, coords, sim) in enumerate(top_5, 1):\n",
    "    dist_km = haversine_distance_km(gt_coords[0], gt_coords[1], coords[0], coords[1])\n",
    "    status = \"✓\" if dist_km * 1000 <= 25 else \"✗\"\n",
    "    print(f\"{i}. {fname}: {sim:.4f} | {dist_km*1000:.1f}m {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Training complete! You have:\n",
    "1. ✓ Trained NetVLAD model\n",
    "2. ✓ Built descriptor database\n",
    "3. ✓ Evaluated on test set\n",
    "4. ✓ Saved model and results\n",
    "\n",
    "**Files created:**\n",
    "- `netvlad_{backbone}.h5` - Trained model\n",
    "- `train_database_{backbone}.npz` - Descriptor database\n",
    "- `results_{backbone}.json` - Evaluation metrics\n",
    "\n",
    "**Next steps:**\n",
    "- Try training with ResNet50 for better accuracy\n",
    "- Experiment with different hyperparameters\n",
    "- Test on your own images"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
